{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.081053901850362,
  "eval_steps": 500,
  "global_step": 21500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.000502815768302494,
      "grad_norm": 17.25,
      "learning_rate": 1.9998994368463394e-05,
      "loss": 1.755,
      "step": 10
    },
    {
      "epoch": 0.001005631536604988,
      "grad_norm": 70.0,
      "learning_rate": 1.9997988736926793e-05,
      "loss": 2.3226,
      "step": 20
    },
    {
      "epoch": 0.0015084473049074819,
      "grad_norm": 196.0,
      "learning_rate": 1.9996983105390186e-05,
      "loss": 3.7719,
      "step": 30
    },
    {
      "epoch": 0.002011263073209976,
      "grad_norm": 40.0,
      "learning_rate": 1.9995977473853582e-05,
      "loss": 2.4239,
      "step": 40
    },
    {
      "epoch": 0.00251407884151247,
      "grad_norm": 31.125,
      "learning_rate": 1.9994971842316978e-05,
      "loss": 2.9258,
      "step": 50
    },
    {
      "epoch": 0.0030168946098149637,
      "grad_norm": 23.25,
      "learning_rate": 1.999396621078037e-05,
      "loss": 1.6977,
      "step": 60
    },
    {
      "epoch": 0.0035197103781174576,
      "grad_norm": 121.5,
      "learning_rate": 1.9992960579243766e-05,
      "loss": 1.8602,
      "step": 70
    },
    {
      "epoch": 0.004022526146419952,
      "grad_norm": 62.25,
      "learning_rate": 1.9991954947707162e-05,
      "loss": 1.9538,
      "step": 80
    },
    {
      "epoch": 0.004525341914722446,
      "grad_norm": 86.0,
      "learning_rate": 1.9990949316170554e-05,
      "loss": 1.708,
      "step": 90
    },
    {
      "epoch": 0.00502815768302494,
      "grad_norm": 20.5,
      "learning_rate": 1.9989943684633954e-05,
      "loss": 2.9422,
      "step": 100
    },
    {
      "epoch": 0.0055309734513274336,
      "grad_norm": 28.0,
      "learning_rate": 1.9988938053097346e-05,
      "loss": 2.1455,
      "step": 110
    },
    {
      "epoch": 0.006033789219629927,
      "grad_norm": 31.125,
      "learning_rate": 1.9987932421560742e-05,
      "loss": 2.0912,
      "step": 120
    },
    {
      "epoch": 0.006536604987932421,
      "grad_norm": 129.0,
      "learning_rate": 1.9986926790024138e-05,
      "loss": 2.2301,
      "step": 130
    },
    {
      "epoch": 0.007039420756234915,
      "grad_norm": 30.0,
      "learning_rate": 1.998592115848753e-05,
      "loss": 3.5905,
      "step": 140
    },
    {
      "epoch": 0.007542236524537409,
      "grad_norm": 48.5,
      "learning_rate": 1.9984915526950926e-05,
      "loss": 2.2544,
      "step": 150
    },
    {
      "epoch": 0.008045052292839904,
      "grad_norm": 26.375,
      "learning_rate": 1.9983909895414322e-05,
      "loss": 2.4787,
      "step": 160
    },
    {
      "epoch": 0.008547868061142397,
      "grad_norm": 44.5,
      "learning_rate": 1.9982904263877715e-05,
      "loss": 1.9704,
      "step": 170
    },
    {
      "epoch": 0.009050683829444892,
      "grad_norm": 92.5,
      "learning_rate": 1.9981898632341114e-05,
      "loss": 1.738,
      "step": 180
    },
    {
      "epoch": 0.009553499597747385,
      "grad_norm": 63.0,
      "learning_rate": 1.9980893000804506e-05,
      "loss": 2.3344,
      "step": 190
    },
    {
      "epoch": 0.01005631536604988,
      "grad_norm": 99.5,
      "learning_rate": 1.9979887369267902e-05,
      "loss": 1.3479,
      "step": 200
    },
    {
      "epoch": 0.010559131134352374,
      "grad_norm": 24.375,
      "learning_rate": 1.9978881737731298e-05,
      "loss": 1.9866,
      "step": 210
    },
    {
      "epoch": 0.011061946902654867,
      "grad_norm": 36.5,
      "learning_rate": 1.997787610619469e-05,
      "loss": 1.9526,
      "step": 220
    },
    {
      "epoch": 0.011564762670957362,
      "grad_norm": 68.0,
      "learning_rate": 1.9976870474658087e-05,
      "loss": 2.4033,
      "step": 230
    },
    {
      "epoch": 0.012067578439259855,
      "grad_norm": 96.0,
      "learning_rate": 1.9975864843121482e-05,
      "loss": 2.0509,
      "step": 240
    },
    {
      "epoch": 0.01257039420756235,
      "grad_norm": 1.796875,
      "learning_rate": 1.9974859211584875e-05,
      "loss": 2.1263,
      "step": 250
    },
    {
      "epoch": 0.013073209975864843,
      "grad_norm": 41.25,
      "learning_rate": 1.997385358004827e-05,
      "loss": 1.4645,
      "step": 260
    },
    {
      "epoch": 0.013576025744167337,
      "grad_norm": 4.25,
      "learning_rate": 1.9972847948511667e-05,
      "loss": 1.9332,
      "step": 270
    },
    {
      "epoch": 0.01407884151246983,
      "grad_norm": 53.0,
      "learning_rate": 1.9971842316975063e-05,
      "loss": 1.3064,
      "step": 280
    },
    {
      "epoch": 0.014581657280772325,
      "grad_norm": 25.0,
      "learning_rate": 1.997083668543846e-05,
      "loss": 1.4321,
      "step": 290
    },
    {
      "epoch": 0.015084473049074818,
      "grad_norm": 127.5,
      "learning_rate": 1.996983105390185e-05,
      "loss": 1.9563,
      "step": 300
    },
    {
      "epoch": 0.015587288817377313,
      "grad_norm": 12.625,
      "learning_rate": 1.9968825422365247e-05,
      "loss": 1.7906,
      "step": 310
    },
    {
      "epoch": 0.016090104585679808,
      "grad_norm": 150.0,
      "learning_rate": 1.9967819790828643e-05,
      "loss": 1.7745,
      "step": 320
    },
    {
      "epoch": 0.016592920353982302,
      "grad_norm": 86.5,
      "learning_rate": 1.9966814159292035e-05,
      "loss": 1.9699,
      "step": 330
    },
    {
      "epoch": 0.017095736122284794,
      "grad_norm": 17.5,
      "learning_rate": 1.996580852775543e-05,
      "loss": 1.6041,
      "step": 340
    },
    {
      "epoch": 0.01759855189058729,
      "grad_norm": 126.0,
      "learning_rate": 1.9964802896218827e-05,
      "loss": 1.3953,
      "step": 350
    },
    {
      "epoch": 0.018101367658889783,
      "grad_norm": 94.5,
      "learning_rate": 1.9963797264682223e-05,
      "loss": 1.5418,
      "step": 360
    },
    {
      "epoch": 0.018604183427192278,
      "grad_norm": 162.0,
      "learning_rate": 1.996279163314562e-05,
      "loss": 1.7346,
      "step": 370
    },
    {
      "epoch": 0.01910699919549477,
      "grad_norm": 11.625,
      "learning_rate": 1.996178600160901e-05,
      "loss": 1.8128,
      "step": 380
    },
    {
      "epoch": 0.019609814963797264,
      "grad_norm": 17.0,
      "learning_rate": 1.9960780370072407e-05,
      "loss": 1.4312,
      "step": 390
    },
    {
      "epoch": 0.02011263073209976,
      "grad_norm": 12.375,
      "learning_rate": 1.9959774738535803e-05,
      "loss": 1.3423,
      "step": 400
    },
    {
      "epoch": 0.020615446500402253,
      "grad_norm": 16.875,
      "learning_rate": 1.9958769106999195e-05,
      "loss": 1.0962,
      "step": 410
    },
    {
      "epoch": 0.021118262268704748,
      "grad_norm": 78.5,
      "learning_rate": 1.995776347546259e-05,
      "loss": 1.5727,
      "step": 420
    },
    {
      "epoch": 0.02162107803700724,
      "grad_norm": 21.375,
      "learning_rate": 1.9956757843925987e-05,
      "loss": 1.3668,
      "step": 430
    },
    {
      "epoch": 0.022123893805309734,
      "grad_norm": 25.875,
      "learning_rate": 1.9955752212389383e-05,
      "loss": 1.4074,
      "step": 440
    },
    {
      "epoch": 0.02262670957361223,
      "grad_norm": 16.875,
      "learning_rate": 1.995474658085278e-05,
      "loss": 1.1227,
      "step": 450
    },
    {
      "epoch": 0.023129525341914724,
      "grad_norm": 71.0,
      "learning_rate": 1.995374094931617e-05,
      "loss": 1.2776,
      "step": 460
    },
    {
      "epoch": 0.023632341110217215,
      "grad_norm": 24.875,
      "learning_rate": 1.9952735317779567e-05,
      "loss": 1.55,
      "step": 470
    },
    {
      "epoch": 0.02413515687851971,
      "grad_norm": 89.0,
      "learning_rate": 1.9951729686242963e-05,
      "loss": 1.364,
      "step": 480
    },
    {
      "epoch": 0.024637972646822204,
      "grad_norm": 17.625,
      "learning_rate": 1.9950724054706356e-05,
      "loss": 1.0853,
      "step": 490
    },
    {
      "epoch": 0.0251407884151247,
      "grad_norm": 39.25,
      "learning_rate": 1.994971842316975e-05,
      "loss": 1.1744,
      "step": 500
    },
    {
      "epoch": 0.0251407884151247,
      "eval_accuracy": 0.49266236985622214,
      "eval_loss": 2.2441346645355225,
      "eval_runtime": 463.561,
      "eval_samples_per_second": 87.022,
      "eval_steps_per_second": 87.022,
      "step": 500
    },
    {
      "epoch": 0.025643604183427194,
      "grad_norm": 32.75,
      "learning_rate": 1.9948712791633147e-05,
      "loss": 1.1669,
      "step": 510
    },
    {
      "epoch": 0.026146419951729685,
      "grad_norm": 27.375,
      "learning_rate": 1.9947707160096543e-05,
      "loss": 1.0588,
      "step": 520
    },
    {
      "epoch": 0.02664923572003218,
      "grad_norm": 6.875,
      "learning_rate": 1.9946701528559936e-05,
      "loss": 1.3489,
      "step": 530
    },
    {
      "epoch": 0.027152051488334675,
      "grad_norm": 18.5,
      "learning_rate": 1.994569589702333e-05,
      "loss": 1.0483,
      "step": 540
    },
    {
      "epoch": 0.02765486725663717,
      "grad_norm": 16.625,
      "learning_rate": 1.9944690265486728e-05,
      "loss": 1.063,
      "step": 550
    },
    {
      "epoch": 0.02815768302493966,
      "grad_norm": 138.0,
      "learning_rate": 1.9943684633950123e-05,
      "loss": 1.7311,
      "step": 560
    },
    {
      "epoch": 0.028660498793242156,
      "grad_norm": 26.5,
      "learning_rate": 1.9942679002413516e-05,
      "loss": 1.0805,
      "step": 570
    },
    {
      "epoch": 0.02916331456154465,
      "grad_norm": 50.0,
      "learning_rate": 1.9941673370876912e-05,
      "loss": 0.9571,
      "step": 580
    },
    {
      "epoch": 0.029666130329847145,
      "grad_norm": 41.25,
      "learning_rate": 1.9940667739340308e-05,
      "loss": 1.4236,
      "step": 590
    },
    {
      "epoch": 0.030168946098149636,
      "grad_norm": 65.0,
      "learning_rate": 1.9939662107803704e-05,
      "loss": 0.8414,
      "step": 600
    },
    {
      "epoch": 0.03067176186645213,
      "grad_norm": 7.1875,
      "learning_rate": 1.9938656476267096e-05,
      "loss": 0.7962,
      "step": 610
    },
    {
      "epoch": 0.031174577634754626,
      "grad_norm": 30.25,
      "learning_rate": 1.9937650844730492e-05,
      "loss": 1.4491,
      "step": 620
    },
    {
      "epoch": 0.03167739340305712,
      "grad_norm": 7.84375,
      "learning_rate": 1.9936645213193888e-05,
      "loss": 1.1464,
      "step": 630
    },
    {
      "epoch": 0.032180209171359615,
      "grad_norm": 9.875,
      "learning_rate": 1.9935639581657284e-05,
      "loss": 1.0437,
      "step": 640
    },
    {
      "epoch": 0.03268302493966211,
      "grad_norm": 21.0,
      "learning_rate": 1.9934633950120676e-05,
      "loss": 1.3236,
      "step": 650
    },
    {
      "epoch": 0.033185840707964605,
      "grad_norm": 15.8125,
      "learning_rate": 1.9933628318584072e-05,
      "loss": 1.1448,
      "step": 660
    },
    {
      "epoch": 0.03368865647626709,
      "grad_norm": 63.75,
      "learning_rate": 1.9932622687047468e-05,
      "loss": 1.1797,
      "step": 670
    },
    {
      "epoch": 0.03419147224456959,
      "grad_norm": 34.5,
      "learning_rate": 1.9931617055510864e-05,
      "loss": 1.8753,
      "step": 680
    },
    {
      "epoch": 0.03469428801287208,
      "grad_norm": 30.25,
      "learning_rate": 1.9930611423974256e-05,
      "loss": 1.5983,
      "step": 690
    },
    {
      "epoch": 0.03519710378117458,
      "grad_norm": 24.25,
      "learning_rate": 1.9929605792437652e-05,
      "loss": 1.2898,
      "step": 700
    },
    {
      "epoch": 0.03569991954947707,
      "grad_norm": 72.0,
      "learning_rate": 1.9928600160901048e-05,
      "loss": 1.5699,
      "step": 710
    },
    {
      "epoch": 0.036202735317779566,
      "grad_norm": 63.5,
      "learning_rate": 1.9927594529364444e-05,
      "loss": 1.2395,
      "step": 720
    },
    {
      "epoch": 0.03670555108608206,
      "grad_norm": 4.3125,
      "learning_rate": 1.9926588897827836e-05,
      "loss": 1.0974,
      "step": 730
    },
    {
      "epoch": 0.037208366854384556,
      "grad_norm": 6.46875,
      "learning_rate": 1.9925583266291232e-05,
      "loss": 1.0399,
      "step": 740
    },
    {
      "epoch": 0.03771118262268705,
      "grad_norm": 10.125,
      "learning_rate": 1.9924577634754628e-05,
      "loss": 1.2209,
      "step": 750
    },
    {
      "epoch": 0.03821399839098954,
      "grad_norm": 30.25,
      "learning_rate": 1.9923572003218024e-05,
      "loss": 1.2585,
      "step": 760
    },
    {
      "epoch": 0.03871681415929203,
      "grad_norm": 9.3125,
      "learning_rate": 1.9922566371681417e-05,
      "loss": 1.0356,
      "step": 770
    },
    {
      "epoch": 0.03921962992759453,
      "grad_norm": 87.0,
      "learning_rate": 1.9921560740144812e-05,
      "loss": 1.3323,
      "step": 780
    },
    {
      "epoch": 0.03972244569589702,
      "grad_norm": 5.21875,
      "learning_rate": 1.992055510860821e-05,
      "loss": 0.837,
      "step": 790
    },
    {
      "epoch": 0.04022526146419952,
      "grad_norm": 46.75,
      "learning_rate": 1.99195494770716e-05,
      "loss": 1.1189,
      "step": 800
    },
    {
      "epoch": 0.04072807723250201,
      "grad_norm": 19.625,
      "learning_rate": 1.9918543845534997e-05,
      "loss": 0.8339,
      "step": 810
    },
    {
      "epoch": 0.04123089300080451,
      "grad_norm": 22.875,
      "learning_rate": 1.9917538213998393e-05,
      "loss": 1.0547,
      "step": 820
    },
    {
      "epoch": 0.041733708769107,
      "grad_norm": 70.5,
      "learning_rate": 1.991653258246179e-05,
      "loss": 1.4902,
      "step": 830
    },
    {
      "epoch": 0.042236524537409496,
      "grad_norm": 27.375,
      "learning_rate": 1.9915526950925184e-05,
      "loss": 1.0908,
      "step": 840
    },
    {
      "epoch": 0.042739340305711984,
      "grad_norm": 41.75,
      "learning_rate": 1.9914521319388577e-05,
      "loss": 1.2615,
      "step": 850
    },
    {
      "epoch": 0.04324215607401448,
      "grad_norm": 10.0,
      "learning_rate": 1.9913515687851973e-05,
      "loss": 1.4291,
      "step": 860
    },
    {
      "epoch": 0.043744971842316974,
      "grad_norm": 33.5,
      "learning_rate": 1.991251005631537e-05,
      "loss": 1.3932,
      "step": 870
    },
    {
      "epoch": 0.04424778761061947,
      "grad_norm": 30.875,
      "learning_rate": 1.991150442477876e-05,
      "loss": 1.3297,
      "step": 880
    },
    {
      "epoch": 0.04475060337892196,
      "grad_norm": 27.75,
      "learning_rate": 1.9910498793242157e-05,
      "loss": 1.0875,
      "step": 890
    },
    {
      "epoch": 0.04525341914722446,
      "grad_norm": 43.75,
      "learning_rate": 1.9909493161705553e-05,
      "loss": 1.151,
      "step": 900
    },
    {
      "epoch": 0.04575623491552695,
      "grad_norm": 14.5,
      "learning_rate": 1.990848753016895e-05,
      "loss": 0.8733,
      "step": 910
    },
    {
      "epoch": 0.04625905068382945,
      "grad_norm": 43.5,
      "learning_rate": 1.9907481898632345e-05,
      "loss": 1.2638,
      "step": 920
    },
    {
      "epoch": 0.04676186645213194,
      "grad_norm": 52.75,
      "learning_rate": 1.9906476267095737e-05,
      "loss": 1.1483,
      "step": 930
    },
    {
      "epoch": 0.04726468222043443,
      "grad_norm": 52.75,
      "learning_rate": 1.9905470635559133e-05,
      "loss": 1.3098,
      "step": 940
    },
    {
      "epoch": 0.047767497988736925,
      "grad_norm": 40.5,
      "learning_rate": 1.990446500402253e-05,
      "loss": 1.3221,
      "step": 950
    },
    {
      "epoch": 0.04827031375703942,
      "grad_norm": 13.0,
      "learning_rate": 1.990345937248592e-05,
      "loss": 1.1198,
      "step": 960
    },
    {
      "epoch": 0.048773129525341914,
      "grad_norm": 58.0,
      "learning_rate": 1.9902453740949317e-05,
      "loss": 1.2282,
      "step": 970
    },
    {
      "epoch": 0.04927594529364441,
      "grad_norm": 61.5,
      "learning_rate": 1.9901448109412713e-05,
      "loss": 1.322,
      "step": 980
    },
    {
      "epoch": 0.049778761061946904,
      "grad_norm": 11.75,
      "learning_rate": 1.990044247787611e-05,
      "loss": 0.9185,
      "step": 990
    },
    {
      "epoch": 0.0502815768302494,
      "grad_norm": 13.25,
      "learning_rate": 1.9899436846339505e-05,
      "loss": 1.1724,
      "step": 1000
    },
    {
      "epoch": 0.0502815768302494,
      "eval_accuracy": 0.49848785324739714,
      "eval_loss": 1.5374782085418701,
      "eval_runtime": 463.8368,
      "eval_samples_per_second": 86.97,
      "eval_steps_per_second": 86.97,
      "step": 1000
    },
    {
      "epoch": 0.05078439259855189,
      "grad_norm": 40.0,
      "learning_rate": 1.9898431214802897e-05,
      "loss": 1.4855,
      "step": 1010
    },
    {
      "epoch": 0.05128720836685439,
      "grad_norm": 40.5,
      "learning_rate": 1.9897425583266293e-05,
      "loss": 1.0843,
      "step": 1020
    },
    {
      "epoch": 0.051790024135156876,
      "grad_norm": 8.0,
      "learning_rate": 1.989641995172969e-05,
      "loss": 1.3188,
      "step": 1030
    },
    {
      "epoch": 0.05229283990345937,
      "grad_norm": 64.5,
      "learning_rate": 1.989541432019308e-05,
      "loss": 1.2116,
      "step": 1040
    },
    {
      "epoch": 0.052795655671761865,
      "grad_norm": 8.125,
      "learning_rate": 1.9894408688656477e-05,
      "loss": 0.8206,
      "step": 1050
    },
    {
      "epoch": 0.05329847144006436,
      "grad_norm": 23.125,
      "learning_rate": 1.9893403057119873e-05,
      "loss": 0.896,
      "step": 1060
    },
    {
      "epoch": 0.053801287208366855,
      "grad_norm": 11.0625,
      "learning_rate": 1.9892397425583266e-05,
      "loss": 1.1027,
      "step": 1070
    },
    {
      "epoch": 0.05430410297666935,
      "grad_norm": 57.25,
      "learning_rate": 1.9891391794046665e-05,
      "loss": 1.4035,
      "step": 1080
    },
    {
      "epoch": 0.054806918744971844,
      "grad_norm": 10.3125,
      "learning_rate": 1.9890386162510058e-05,
      "loss": 1.3311,
      "step": 1090
    },
    {
      "epoch": 0.05530973451327434,
      "grad_norm": 34.25,
      "learning_rate": 1.9889380530973453e-05,
      "loss": 1.6882,
      "step": 1100
    },
    {
      "epoch": 0.05581255028157683,
      "grad_norm": 36.0,
      "learning_rate": 1.988837489943685e-05,
      "loss": 0.9455,
      "step": 1110
    },
    {
      "epoch": 0.05631536604987932,
      "grad_norm": 56.5,
      "learning_rate": 1.9887369267900242e-05,
      "loss": 1.5086,
      "step": 1120
    },
    {
      "epoch": 0.056818181818181816,
      "grad_norm": 9.6875,
      "learning_rate": 1.9886363636363638e-05,
      "loss": 1.2461,
      "step": 1130
    },
    {
      "epoch": 0.05732099758648431,
      "grad_norm": 61.5,
      "learning_rate": 1.9885358004827034e-05,
      "loss": 1.1075,
      "step": 1140
    },
    {
      "epoch": 0.057823813354786806,
      "grad_norm": 35.25,
      "learning_rate": 1.9884352373290426e-05,
      "loss": 1.0538,
      "step": 1150
    },
    {
      "epoch": 0.0583266291230893,
      "grad_norm": 7.8125,
      "learning_rate": 1.9883346741753825e-05,
      "loss": 1.0417,
      "step": 1160
    },
    {
      "epoch": 0.058829444891391795,
      "grad_norm": 10.5,
      "learning_rate": 1.9882341110217218e-05,
      "loss": 1.1981,
      "step": 1170
    },
    {
      "epoch": 0.05933226065969429,
      "grad_norm": 35.75,
      "learning_rate": 1.9881335478680614e-05,
      "loss": 1.2672,
      "step": 1180
    },
    {
      "epoch": 0.059835076427996785,
      "grad_norm": 39.5,
      "learning_rate": 1.988032984714401e-05,
      "loss": 1.1306,
      "step": 1190
    },
    {
      "epoch": 0.06033789219629927,
      "grad_norm": 65.5,
      "learning_rate": 1.9879324215607402e-05,
      "loss": 1.3169,
      "step": 1200
    },
    {
      "epoch": 0.06084070796460177,
      "grad_norm": 29.625,
      "learning_rate": 1.9878318584070798e-05,
      "loss": 0.8346,
      "step": 1210
    },
    {
      "epoch": 0.06134352373290426,
      "grad_norm": 14.625,
      "learning_rate": 1.9877312952534194e-05,
      "loss": 0.8522,
      "step": 1220
    },
    {
      "epoch": 0.06184633950120676,
      "grad_norm": 18.5,
      "learning_rate": 1.9876307320997586e-05,
      "loss": 1.443,
      "step": 1230
    },
    {
      "epoch": 0.06234915526950925,
      "grad_norm": 67.0,
      "learning_rate": 1.9875301689460986e-05,
      "loss": 1.1026,
      "step": 1240
    },
    {
      "epoch": 0.06285197103781175,
      "grad_norm": 18.75,
      "learning_rate": 1.9874296057924378e-05,
      "loss": 0.8279,
      "step": 1250
    },
    {
      "epoch": 0.06335478680611424,
      "grad_norm": 22.625,
      "learning_rate": 1.9873290426387774e-05,
      "loss": 1.4913,
      "step": 1260
    },
    {
      "epoch": 0.06385760257441674,
      "grad_norm": 12.4375,
      "learning_rate": 1.987228479485117e-05,
      "loss": 0.9585,
      "step": 1270
    },
    {
      "epoch": 0.06436041834271923,
      "grad_norm": 27.25,
      "learning_rate": 1.9871279163314562e-05,
      "loss": 0.8827,
      "step": 1280
    },
    {
      "epoch": 0.06486323411102173,
      "grad_norm": 29.125,
      "learning_rate": 1.9870273531777958e-05,
      "loss": 1.0718,
      "step": 1290
    },
    {
      "epoch": 0.06536604987932422,
      "grad_norm": 11.4375,
      "learning_rate": 1.9869267900241354e-05,
      "loss": 0.8838,
      "step": 1300
    },
    {
      "epoch": 0.06586886564762671,
      "grad_norm": 26.0,
      "learning_rate": 1.9868262268704747e-05,
      "loss": 1.2334,
      "step": 1310
    },
    {
      "epoch": 0.06637168141592921,
      "grad_norm": 33.0,
      "learning_rate": 1.9867256637168142e-05,
      "loss": 1.2418,
      "step": 1320
    },
    {
      "epoch": 0.0668744971842317,
      "grad_norm": 23.125,
      "learning_rate": 1.986625100563154e-05,
      "loss": 1.353,
      "step": 1330
    },
    {
      "epoch": 0.06737731295253419,
      "grad_norm": 32.75,
      "learning_rate": 1.986524537409493e-05,
      "loss": 1.3633,
      "step": 1340
    },
    {
      "epoch": 0.06788012872083668,
      "grad_norm": 28.25,
      "learning_rate": 1.986423974255833e-05,
      "loss": 1.2437,
      "step": 1350
    },
    {
      "epoch": 0.06838294448913917,
      "grad_norm": 12.4375,
      "learning_rate": 1.9863234111021723e-05,
      "loss": 0.8242,
      "step": 1360
    },
    {
      "epoch": 0.06888576025744167,
      "grad_norm": 29.625,
      "learning_rate": 1.986222847948512e-05,
      "loss": 1.03,
      "step": 1370
    },
    {
      "epoch": 0.06938857602574416,
      "grad_norm": 20.75,
      "learning_rate": 1.9861222847948514e-05,
      "loss": 1.0067,
      "step": 1380
    },
    {
      "epoch": 0.06989139179404666,
      "grad_norm": 22.625,
      "learning_rate": 1.9860217216411907e-05,
      "loss": 1.2769,
      "step": 1390
    },
    {
      "epoch": 0.07039420756234915,
      "grad_norm": 27.875,
      "learning_rate": 1.9859211584875303e-05,
      "loss": 1.0452,
      "step": 1400
    },
    {
      "epoch": 0.07089702333065165,
      "grad_norm": 32.75,
      "learning_rate": 1.98582059533387e-05,
      "loss": 1.1272,
      "step": 1410
    },
    {
      "epoch": 0.07139983909895414,
      "grad_norm": 40.0,
      "learning_rate": 1.985720032180209e-05,
      "loss": 1.0697,
      "step": 1420
    },
    {
      "epoch": 0.07190265486725664,
      "grad_norm": 21.125,
      "learning_rate": 1.985619469026549e-05,
      "loss": 0.9728,
      "step": 1430
    },
    {
      "epoch": 0.07240547063555913,
      "grad_norm": 38.25,
      "learning_rate": 1.9855189058728883e-05,
      "loss": 1.0221,
      "step": 1440
    },
    {
      "epoch": 0.07290828640386163,
      "grad_norm": 13.5,
      "learning_rate": 1.985418342719228e-05,
      "loss": 0.9772,
      "step": 1450
    },
    {
      "epoch": 0.07341110217216412,
      "grad_norm": 66.0,
      "learning_rate": 1.9853177795655675e-05,
      "loss": 0.8205,
      "step": 1460
    },
    {
      "epoch": 0.07391391794046662,
      "grad_norm": 25.875,
      "learning_rate": 1.9852172164119067e-05,
      "loss": 0.9795,
      "step": 1470
    },
    {
      "epoch": 0.07441673370876911,
      "grad_norm": 15.4375,
      "learning_rate": 1.9851166532582463e-05,
      "loss": 1.1292,
      "step": 1480
    },
    {
      "epoch": 0.0749195494770716,
      "grad_norm": 11.8125,
      "learning_rate": 1.985016090104586e-05,
      "loss": 1.1356,
      "step": 1490
    },
    {
      "epoch": 0.0754223652453741,
      "grad_norm": 23.5,
      "learning_rate": 1.984915526950925e-05,
      "loss": 1.1094,
      "step": 1500
    },
    {
      "epoch": 0.0754223652453741,
      "eval_accuracy": 0.5020575111551809,
      "eval_loss": 1.2778980731964111,
      "eval_runtime": 464.4728,
      "eval_samples_per_second": 86.851,
      "eval_steps_per_second": 86.851,
      "step": 1500
    },
    {
      "epoch": 0.07592518101367658,
      "grad_norm": 20.625,
      "learning_rate": 1.984814963797265e-05,
      "loss": 1.2984,
      "step": 1510
    },
    {
      "epoch": 0.07642799678197908,
      "grad_norm": 16.5,
      "learning_rate": 1.9847144006436043e-05,
      "loss": 0.9923,
      "step": 1520
    },
    {
      "epoch": 0.07693081255028157,
      "grad_norm": 54.25,
      "learning_rate": 1.984613837489944e-05,
      "loss": 1.1829,
      "step": 1530
    },
    {
      "epoch": 0.07743362831858407,
      "grad_norm": 12.5625,
      "learning_rate": 1.9845132743362835e-05,
      "loss": 0.9829,
      "step": 1540
    },
    {
      "epoch": 0.07793644408688656,
      "grad_norm": 15.125,
      "learning_rate": 1.9844127111826227e-05,
      "loss": 0.9298,
      "step": 1550
    },
    {
      "epoch": 0.07843925985518906,
      "grad_norm": 14.3125,
      "learning_rate": 1.9843121480289623e-05,
      "loss": 0.948,
      "step": 1560
    },
    {
      "epoch": 0.07894207562349155,
      "grad_norm": 23.5,
      "learning_rate": 1.984211584875302e-05,
      "loss": 1.2381,
      "step": 1570
    },
    {
      "epoch": 0.07944489139179405,
      "grad_norm": 21.25,
      "learning_rate": 1.984111021721641e-05,
      "loss": 1.0238,
      "step": 1580
    },
    {
      "epoch": 0.07994770716009654,
      "grad_norm": 18.875,
      "learning_rate": 1.9840104585679807e-05,
      "loss": 0.7515,
      "step": 1590
    },
    {
      "epoch": 0.08045052292839903,
      "grad_norm": 45.5,
      "learning_rate": 1.9839098954143203e-05,
      "loss": 0.9784,
      "step": 1600
    },
    {
      "epoch": 0.08095333869670153,
      "grad_norm": 15.6875,
      "learning_rate": 1.9838093322606596e-05,
      "loss": 0.9643,
      "step": 1610
    },
    {
      "epoch": 0.08145615446500402,
      "grad_norm": 14.375,
      "learning_rate": 1.9837087691069995e-05,
      "loss": 0.6581,
      "step": 1620
    },
    {
      "epoch": 0.08195897023330652,
      "grad_norm": 8.6875,
      "learning_rate": 1.9836082059533388e-05,
      "loss": 1.0244,
      "step": 1630
    },
    {
      "epoch": 0.08246178600160901,
      "grad_norm": 20.0,
      "learning_rate": 1.9835076427996783e-05,
      "loss": 1.1502,
      "step": 1640
    },
    {
      "epoch": 0.08296460176991151,
      "grad_norm": 45.0,
      "learning_rate": 1.983407079646018e-05,
      "loss": 1.0626,
      "step": 1650
    },
    {
      "epoch": 0.083467417538214,
      "grad_norm": 45.75,
      "learning_rate": 1.9833065164923572e-05,
      "loss": 1.0124,
      "step": 1660
    },
    {
      "epoch": 0.0839702333065165,
      "grad_norm": 27.125,
      "learning_rate": 1.9832059533386968e-05,
      "loss": 1.2806,
      "step": 1670
    },
    {
      "epoch": 0.08447304907481899,
      "grad_norm": 58.5,
      "learning_rate": 1.9831053901850364e-05,
      "loss": 1.5004,
      "step": 1680
    },
    {
      "epoch": 0.08497586484312147,
      "grad_norm": 33.75,
      "learning_rate": 1.9830048270313756e-05,
      "loss": 1.586,
      "step": 1690
    },
    {
      "epoch": 0.08547868061142397,
      "grad_norm": 14.375,
      "learning_rate": 1.9829042638777155e-05,
      "loss": 0.9594,
      "step": 1700
    },
    {
      "epoch": 0.08598149637972646,
      "grad_norm": 28.875,
      "learning_rate": 1.9828037007240548e-05,
      "loss": 1.0504,
      "step": 1710
    },
    {
      "epoch": 0.08648431214802896,
      "grad_norm": 32.5,
      "learning_rate": 1.9827031375703944e-05,
      "loss": 0.8961,
      "step": 1720
    },
    {
      "epoch": 0.08698712791633145,
      "grad_norm": 30.75,
      "learning_rate": 1.982602574416734e-05,
      "loss": 1.0909,
      "step": 1730
    },
    {
      "epoch": 0.08748994368463395,
      "grad_norm": 18.125,
      "learning_rate": 1.9825020112630732e-05,
      "loss": 1.0049,
      "step": 1740
    },
    {
      "epoch": 0.08799275945293644,
      "grad_norm": 8.0,
      "learning_rate": 1.9824014481094128e-05,
      "loss": 0.9059,
      "step": 1750
    },
    {
      "epoch": 0.08849557522123894,
      "grad_norm": 44.5,
      "learning_rate": 1.9823008849557524e-05,
      "loss": 0.8509,
      "step": 1760
    },
    {
      "epoch": 0.08899839098954143,
      "grad_norm": 7.8125,
      "learning_rate": 1.9822003218020916e-05,
      "loss": 0.9924,
      "step": 1770
    },
    {
      "epoch": 0.08950120675784393,
      "grad_norm": 15.9375,
      "learning_rate": 1.9820997586484316e-05,
      "loss": 1.0462,
      "step": 1780
    },
    {
      "epoch": 0.09000402252614642,
      "grad_norm": 8.375,
      "learning_rate": 1.9819991954947708e-05,
      "loss": 0.863,
      "step": 1790
    },
    {
      "epoch": 0.09050683829444892,
      "grad_norm": 17.875,
      "learning_rate": 1.9818986323411104e-05,
      "loss": 1.0664,
      "step": 1800
    },
    {
      "epoch": 0.09100965406275141,
      "grad_norm": 21.625,
      "learning_rate": 1.98179806918745e-05,
      "loss": 1.023,
      "step": 1810
    },
    {
      "epoch": 0.0915124698310539,
      "grad_norm": 8.4375,
      "learning_rate": 1.9816975060337892e-05,
      "loss": 1.2834,
      "step": 1820
    },
    {
      "epoch": 0.0920152855993564,
      "grad_norm": 69.5,
      "learning_rate": 1.9815969428801288e-05,
      "loss": 1.231,
      "step": 1830
    },
    {
      "epoch": 0.0925181013676589,
      "grad_norm": 22.375,
      "learning_rate": 1.9814963797264684e-05,
      "loss": 1.0259,
      "step": 1840
    },
    {
      "epoch": 0.09302091713596139,
      "grad_norm": 7.0625,
      "learning_rate": 1.981395816572808e-05,
      "loss": 1.2927,
      "step": 1850
    },
    {
      "epoch": 0.09352373290426388,
      "grad_norm": 53.0,
      "learning_rate": 1.9812952534191472e-05,
      "loss": 0.8671,
      "step": 1860
    },
    {
      "epoch": 0.09402654867256637,
      "grad_norm": 13.8125,
      "learning_rate": 1.981194690265487e-05,
      "loss": 1.1904,
      "step": 1870
    },
    {
      "epoch": 0.09452936444086886,
      "grad_norm": 24.375,
      "learning_rate": 1.9810941271118264e-05,
      "loss": 1.4408,
      "step": 1880
    },
    {
      "epoch": 0.09503218020917135,
      "grad_norm": 9.25,
      "learning_rate": 1.980993563958166e-05,
      "loss": 1.2783,
      "step": 1890
    },
    {
      "epoch": 0.09553499597747385,
      "grad_norm": 47.5,
      "learning_rate": 1.9808930008045053e-05,
      "loss": 1.1093,
      "step": 1900
    },
    {
      "epoch": 0.09603781174577634,
      "grad_norm": 58.5,
      "learning_rate": 1.980792437650845e-05,
      "loss": 1.2751,
      "step": 1910
    },
    {
      "epoch": 0.09654062751407884,
      "grad_norm": 3.03125,
      "learning_rate": 1.9806918744971844e-05,
      "loss": 0.8366,
      "step": 1920
    },
    {
      "epoch": 0.09704344328238133,
      "grad_norm": 27.125,
      "learning_rate": 1.980591311343524e-05,
      "loss": 1.0107,
      "step": 1930
    },
    {
      "epoch": 0.09754625905068383,
      "grad_norm": 20.0,
      "learning_rate": 1.9804907481898633e-05,
      "loss": 0.9459,
      "step": 1940
    },
    {
      "epoch": 0.09804907481898632,
      "grad_norm": 13.0,
      "learning_rate": 1.980390185036203e-05,
      "loss": 0.9708,
      "step": 1950
    },
    {
      "epoch": 0.09855189058728882,
      "grad_norm": 8.5,
      "learning_rate": 1.9802896218825425e-05,
      "loss": 0.7031,
      "step": 1960
    },
    {
      "epoch": 0.09905470635559131,
      "grad_norm": 6.75,
      "learning_rate": 1.980189058728882e-05,
      "loss": 1.0424,
      "step": 1970
    },
    {
      "epoch": 0.09955752212389381,
      "grad_norm": 20.875,
      "learning_rate": 1.9800884955752213e-05,
      "loss": 1.4547,
      "step": 1980
    },
    {
      "epoch": 0.1000603378921963,
      "grad_norm": 19.125,
      "learning_rate": 1.979987932421561e-05,
      "loss": 1.0723,
      "step": 1990
    },
    {
      "epoch": 0.1005631536604988,
      "grad_norm": 25.625,
      "learning_rate": 1.9798873692679005e-05,
      "loss": 0.7404,
      "step": 2000
    },
    {
      "epoch": 0.1005631536604988,
      "eval_accuracy": 0.5057759048091225,
      "eval_loss": 1.196290373802185,
      "eval_runtime": 463.4626,
      "eval_samples_per_second": 87.04,
      "eval_steps_per_second": 87.04,
      "step": 2000
    },
    {
      "epoch": 0.10106596942880129,
      "grad_norm": 6.09375,
      "learning_rate": 1.97978680611424e-05,
      "loss": 0.7338,
      "step": 2010
    },
    {
      "epoch": 0.10156878519710379,
      "grad_norm": 11.4375,
      "learning_rate": 1.9796862429605793e-05,
      "loss": 1.273,
      "step": 2020
    },
    {
      "epoch": 0.10207160096540628,
      "grad_norm": 17.75,
      "learning_rate": 1.979585679806919e-05,
      "loss": 0.9983,
      "step": 2030
    },
    {
      "epoch": 0.10257441673370878,
      "grad_norm": 20.375,
      "learning_rate": 1.9794851166532585e-05,
      "loss": 1.306,
      "step": 2040
    },
    {
      "epoch": 0.10307723250201126,
      "grad_norm": 5.21875,
      "learning_rate": 1.979384553499598e-05,
      "loss": 0.8712,
      "step": 2050
    },
    {
      "epoch": 0.10358004827031375,
      "grad_norm": 14.125,
      "learning_rate": 1.9792839903459373e-05,
      "loss": 0.9737,
      "step": 2060
    },
    {
      "epoch": 0.10408286403861625,
      "grad_norm": 10.6875,
      "learning_rate": 1.979183427192277e-05,
      "loss": 0.835,
      "step": 2070
    },
    {
      "epoch": 0.10458567980691874,
      "grad_norm": 15.5,
      "learning_rate": 1.9790828640386165e-05,
      "loss": 1.0255,
      "step": 2080
    },
    {
      "epoch": 0.10508849557522124,
      "grad_norm": 12.125,
      "learning_rate": 1.978982300884956e-05,
      "loss": 0.9322,
      "step": 2090
    },
    {
      "epoch": 0.10559131134352373,
      "grad_norm": 66.0,
      "learning_rate": 1.9788817377312953e-05,
      "loss": 1.3371,
      "step": 2100
    },
    {
      "epoch": 0.10609412711182623,
      "grad_norm": 15.3125,
      "learning_rate": 1.978781174577635e-05,
      "loss": 1.0585,
      "step": 2110
    },
    {
      "epoch": 0.10659694288012872,
      "grad_norm": 35.0,
      "learning_rate": 1.9786806114239745e-05,
      "loss": 1.0711,
      "step": 2120
    },
    {
      "epoch": 0.10709975864843121,
      "grad_norm": 24.75,
      "learning_rate": 1.9785800482703138e-05,
      "loss": 1.3138,
      "step": 2130
    },
    {
      "epoch": 0.10760257441673371,
      "grad_norm": 41.75,
      "learning_rate": 1.9784794851166533e-05,
      "loss": 1.34,
      "step": 2140
    },
    {
      "epoch": 0.1081053901850362,
      "grad_norm": 7.34375,
      "learning_rate": 1.978378921962993e-05,
      "loss": 0.9177,
      "step": 2150
    },
    {
      "epoch": 0.1086082059533387,
      "grad_norm": 37.75,
      "learning_rate": 1.9782783588093325e-05,
      "loss": 1.1133,
      "step": 2160
    },
    {
      "epoch": 0.1091110217216412,
      "grad_norm": 63.75,
      "learning_rate": 1.978177795655672e-05,
      "loss": 1.0031,
      "step": 2170
    },
    {
      "epoch": 0.10961383748994369,
      "grad_norm": 5.875,
      "learning_rate": 1.9780772325020114e-05,
      "loss": 0.898,
      "step": 2180
    },
    {
      "epoch": 0.11011665325824618,
      "grad_norm": 12.5,
      "learning_rate": 1.977976669348351e-05,
      "loss": 1.0838,
      "step": 2190
    },
    {
      "epoch": 0.11061946902654868,
      "grad_norm": 20.25,
      "learning_rate": 1.9778761061946905e-05,
      "loss": 1.614,
      "step": 2200
    },
    {
      "epoch": 0.11112228479485117,
      "grad_norm": 29.375,
      "learning_rate": 1.9777755430410298e-05,
      "loss": 1.0045,
      "step": 2210
    },
    {
      "epoch": 0.11162510056315365,
      "grad_norm": 29.625,
      "learning_rate": 1.9776749798873694e-05,
      "loss": 1.08,
      "step": 2220
    },
    {
      "epoch": 0.11212791633145615,
      "grad_norm": 8.9375,
      "learning_rate": 1.977574416733709e-05,
      "loss": 0.9126,
      "step": 2230
    },
    {
      "epoch": 0.11263073209975864,
      "grad_norm": 20.5,
      "learning_rate": 1.9774738535800485e-05,
      "loss": 0.868,
      "step": 2240
    },
    {
      "epoch": 0.11313354786806114,
      "grad_norm": 25.375,
      "learning_rate": 1.977373290426388e-05,
      "loss": 0.7441,
      "step": 2250
    },
    {
      "epoch": 0.11363636363636363,
      "grad_norm": 30.25,
      "learning_rate": 1.9772727272727274e-05,
      "loss": 1.2848,
      "step": 2260
    },
    {
      "epoch": 0.11413917940466613,
      "grad_norm": 5.625,
      "learning_rate": 1.977172164119067e-05,
      "loss": 1.0578,
      "step": 2270
    },
    {
      "epoch": 0.11464199517296862,
      "grad_norm": 22.625,
      "learning_rate": 1.9770716009654066e-05,
      "loss": 1.399,
      "step": 2280
    },
    {
      "epoch": 0.11514481094127112,
      "grad_norm": 7.96875,
      "learning_rate": 1.9769710378117458e-05,
      "loss": 0.9063,
      "step": 2290
    },
    {
      "epoch": 0.11564762670957361,
      "grad_norm": 6.5625,
      "learning_rate": 1.9768704746580854e-05,
      "loss": 1.0792,
      "step": 2300
    },
    {
      "epoch": 0.1161504424778761,
      "grad_norm": 9.9375,
      "learning_rate": 1.976769911504425e-05,
      "loss": 0.8213,
      "step": 2310
    },
    {
      "epoch": 0.1166532582461786,
      "grad_norm": 17.0,
      "learning_rate": 1.9766693483507646e-05,
      "loss": 1.2637,
      "step": 2320
    },
    {
      "epoch": 0.1171560740144811,
      "grad_norm": 23.875,
      "learning_rate": 1.976568785197104e-05,
      "loss": 0.8917,
      "step": 2330
    },
    {
      "epoch": 0.11765888978278359,
      "grad_norm": 18.0,
      "learning_rate": 1.9764682220434434e-05,
      "loss": 0.849,
      "step": 2340
    },
    {
      "epoch": 0.11816170555108609,
      "grad_norm": 9.5625,
      "learning_rate": 1.976367658889783e-05,
      "loss": 0.9853,
      "step": 2350
    },
    {
      "epoch": 0.11866452131938858,
      "grad_norm": 12.5625,
      "learning_rate": 1.9762670957361226e-05,
      "loss": 1.1028,
      "step": 2360
    },
    {
      "epoch": 0.11916733708769107,
      "grad_norm": 7.375,
      "learning_rate": 1.9761665325824618e-05,
      "loss": 1.2908,
      "step": 2370
    },
    {
      "epoch": 0.11967015285599357,
      "grad_norm": 70.0,
      "learning_rate": 1.9760659694288014e-05,
      "loss": 1.004,
      "step": 2380
    },
    {
      "epoch": 0.12017296862429606,
      "grad_norm": 6.25,
      "learning_rate": 1.975965406275141e-05,
      "loss": 1.0106,
      "step": 2390
    },
    {
      "epoch": 0.12067578439259855,
      "grad_norm": 30.125,
      "learning_rate": 1.9758648431214803e-05,
      "loss": 1.4111,
      "step": 2400
    },
    {
      "epoch": 0.12117860016090104,
      "grad_norm": 5.625,
      "learning_rate": 1.9757642799678202e-05,
      "loss": 1.2037,
      "step": 2410
    },
    {
      "epoch": 0.12168141592920353,
      "grad_norm": 37.0,
      "learning_rate": 1.9756637168141594e-05,
      "loss": 1.2363,
      "step": 2420
    },
    {
      "epoch": 0.12218423169750603,
      "grad_norm": 11.75,
      "learning_rate": 1.975563153660499e-05,
      "loss": 1.027,
      "step": 2430
    },
    {
      "epoch": 0.12268704746580852,
      "grad_norm": 24.5,
      "learning_rate": 1.9754625905068386e-05,
      "loss": 1.0427,
      "step": 2440
    },
    {
      "epoch": 0.12318986323411102,
      "grad_norm": 21.125,
      "learning_rate": 1.975362027353178e-05,
      "loss": 0.6839,
      "step": 2450
    },
    {
      "epoch": 0.12369267900241351,
      "grad_norm": 20.75,
      "learning_rate": 1.9752614641995174e-05,
      "loss": 1.0212,
      "step": 2460
    },
    {
      "epoch": 0.12419549477071601,
      "grad_norm": 4.625,
      "learning_rate": 1.975160901045857e-05,
      "loss": 0.7714,
      "step": 2470
    },
    {
      "epoch": 0.1246983105390185,
      "grad_norm": 24.125,
      "learning_rate": 1.9750603378921963e-05,
      "loss": 1.0904,
      "step": 2480
    },
    {
      "epoch": 0.125201126307321,
      "grad_norm": 29.0,
      "learning_rate": 1.9749597747385362e-05,
      "loss": 0.9094,
      "step": 2490
    },
    {
      "epoch": 0.1257039420756235,
      "grad_norm": 10.75,
      "learning_rate": 1.9748592115848755e-05,
      "loss": 0.9452,
      "step": 2500
    },
    {
      "epoch": 0.1257039420756235,
      "eval_accuracy": 0.5054784333168071,
      "eval_loss": 1.1597926616668701,
      "eval_runtime": 462.8178,
      "eval_samples_per_second": 87.162,
      "eval_steps_per_second": 87.162,
      "step": 2500
    },
    {
      "epoch": 0.126206757843926,
      "grad_norm": 51.5,
      "learning_rate": 1.974758648431215e-05,
      "loss": 1.0503,
      "step": 2510
    },
    {
      "epoch": 0.12670957361222848,
      "grad_norm": 38.75,
      "learning_rate": 1.9746580852775546e-05,
      "loss": 1.1281,
      "step": 2520
    },
    {
      "epoch": 0.12721238938053098,
      "grad_norm": 40.5,
      "learning_rate": 1.974557522123894e-05,
      "loss": 1.3772,
      "step": 2530
    },
    {
      "epoch": 0.12771520514883347,
      "grad_norm": 28.125,
      "learning_rate": 1.9744569589702335e-05,
      "loss": 1.1235,
      "step": 2540
    },
    {
      "epoch": 0.12821802091713597,
      "grad_norm": 14.4375,
      "learning_rate": 1.974356395816573e-05,
      "loss": 1.1356,
      "step": 2550
    },
    {
      "epoch": 0.12872083668543846,
      "grad_norm": 6.875,
      "learning_rate": 1.9742558326629123e-05,
      "loss": 0.6989,
      "step": 2560
    },
    {
      "epoch": 0.12922365245374096,
      "grad_norm": 24.375,
      "learning_rate": 1.9741552695092522e-05,
      "loss": 0.9068,
      "step": 2570
    },
    {
      "epoch": 0.12972646822204345,
      "grad_norm": 33.0,
      "learning_rate": 1.9740547063555915e-05,
      "loss": 0.6883,
      "step": 2580
    },
    {
      "epoch": 0.13022928399034595,
      "grad_norm": 9.5,
      "learning_rate": 1.973954143201931e-05,
      "loss": 0.8915,
      "step": 2590
    },
    {
      "epoch": 0.13073209975864844,
      "grad_norm": 10.6875,
      "learning_rate": 1.9738535800482707e-05,
      "loss": 1.1406,
      "step": 2600
    },
    {
      "epoch": 0.13123491552695093,
      "grad_norm": 9.5625,
      "learning_rate": 1.97375301689461e-05,
      "loss": 1.1171,
      "step": 2610
    },
    {
      "epoch": 0.13173773129525343,
      "grad_norm": 8.6875,
      "learning_rate": 1.9736524537409495e-05,
      "loss": 0.8735,
      "step": 2620
    },
    {
      "epoch": 0.13224054706355592,
      "grad_norm": 20.5,
      "learning_rate": 1.973551890587289e-05,
      "loss": 1.0478,
      "step": 2630
    },
    {
      "epoch": 0.13274336283185842,
      "grad_norm": 16.625,
      "learning_rate": 1.9734513274336283e-05,
      "loss": 0.9281,
      "step": 2640
    },
    {
      "epoch": 0.1332461786001609,
      "grad_norm": 25.75,
      "learning_rate": 1.973350764279968e-05,
      "loss": 1.0036,
      "step": 2650
    },
    {
      "epoch": 0.1337489943684634,
      "grad_norm": 42.25,
      "learning_rate": 1.9732502011263075e-05,
      "loss": 1.0214,
      "step": 2660
    },
    {
      "epoch": 0.13425181013676588,
      "grad_norm": 55.75,
      "learning_rate": 1.9731496379726468e-05,
      "loss": 1.173,
      "step": 2670
    },
    {
      "epoch": 0.13475462590506837,
      "grad_norm": 36.75,
      "learning_rate": 1.9730490748189867e-05,
      "loss": 1.1137,
      "step": 2680
    },
    {
      "epoch": 0.13525744167337087,
      "grad_norm": 19.375,
      "learning_rate": 1.972948511665326e-05,
      "loss": 0.8623,
      "step": 2690
    },
    {
      "epoch": 0.13576025744167336,
      "grad_norm": 11.4375,
      "learning_rate": 1.9728479485116655e-05,
      "loss": 0.8267,
      "step": 2700
    },
    {
      "epoch": 0.13626307320997585,
      "grad_norm": 8.25,
      "learning_rate": 1.972747385358005e-05,
      "loss": 1.0373,
      "step": 2710
    },
    {
      "epoch": 0.13676588897827835,
      "grad_norm": 47.25,
      "learning_rate": 1.9726468222043444e-05,
      "loss": 1.0575,
      "step": 2720
    },
    {
      "epoch": 0.13726870474658084,
      "grad_norm": 24.75,
      "learning_rate": 1.972546259050684e-05,
      "loss": 1.1627,
      "step": 2730
    },
    {
      "epoch": 0.13777152051488334,
      "grad_norm": 18.875,
      "learning_rate": 1.9724456958970235e-05,
      "loss": 1.1038,
      "step": 2740
    },
    {
      "epoch": 0.13827433628318583,
      "grad_norm": 58.5,
      "learning_rate": 1.9723451327433628e-05,
      "loss": 1.2842,
      "step": 2750
    },
    {
      "epoch": 0.13877715205148833,
      "grad_norm": 33.0,
      "learning_rate": 1.9722445695897027e-05,
      "loss": 1.2195,
      "step": 2760
    },
    {
      "epoch": 0.13927996781979082,
      "grad_norm": 39.0,
      "learning_rate": 1.972144006436042e-05,
      "loss": 1.0509,
      "step": 2770
    },
    {
      "epoch": 0.13978278358809332,
      "grad_norm": 4.90625,
      "learning_rate": 1.9720434432823815e-05,
      "loss": 0.9176,
      "step": 2780
    },
    {
      "epoch": 0.1402855993563958,
      "grad_norm": 43.75,
      "learning_rate": 1.971942880128721e-05,
      "loss": 1.0877,
      "step": 2790
    },
    {
      "epoch": 0.1407884151246983,
      "grad_norm": 15.75,
      "learning_rate": 1.9718423169750604e-05,
      "loss": 1.0382,
      "step": 2800
    },
    {
      "epoch": 0.1412912308930008,
      "grad_norm": 29.75,
      "learning_rate": 1.9717417538214e-05,
      "loss": 0.9011,
      "step": 2810
    },
    {
      "epoch": 0.1417940466613033,
      "grad_norm": 8.5,
      "learning_rate": 1.9716411906677396e-05,
      "loss": 0.8672,
      "step": 2820
    },
    {
      "epoch": 0.1422968624296058,
      "grad_norm": 17.375,
      "learning_rate": 1.9715406275140788e-05,
      "loss": 0.9634,
      "step": 2830
    },
    {
      "epoch": 0.1427996781979083,
      "grad_norm": 43.0,
      "learning_rate": 1.9714400643604187e-05,
      "loss": 1.1591,
      "step": 2840
    },
    {
      "epoch": 0.14330249396621078,
      "grad_norm": 11.5625,
      "learning_rate": 1.971339501206758e-05,
      "loss": 0.8279,
      "step": 2850
    },
    {
      "epoch": 0.14380530973451328,
      "grad_norm": 16.75,
      "learning_rate": 1.9712389380530976e-05,
      "loss": 1.342,
      "step": 2860
    },
    {
      "epoch": 0.14430812550281577,
      "grad_norm": 43.75,
      "learning_rate": 1.971138374899437e-05,
      "loss": 0.9795,
      "step": 2870
    },
    {
      "epoch": 0.14481094127111827,
      "grad_norm": 19.875,
      "learning_rate": 1.9710378117457764e-05,
      "loss": 1.1341,
      "step": 2880
    },
    {
      "epoch": 0.14531375703942076,
      "grad_norm": 12.3125,
      "learning_rate": 1.970937248592116e-05,
      "loss": 1.4276,
      "step": 2890
    },
    {
      "epoch": 0.14581657280772325,
      "grad_norm": 14.9375,
      "learning_rate": 1.9708366854384556e-05,
      "loss": 1.3269,
      "step": 2900
    },
    {
      "epoch": 0.14631938857602575,
      "grad_norm": 42.0,
      "learning_rate": 1.9707361222847948e-05,
      "loss": 1.1609,
      "step": 2910
    },
    {
      "epoch": 0.14682220434432824,
      "grad_norm": 4.59375,
      "learning_rate": 1.9706355591311344e-05,
      "loss": 0.7524,
      "step": 2920
    },
    {
      "epoch": 0.14732502011263074,
      "grad_norm": 10.6875,
      "learning_rate": 1.970534995977474e-05,
      "loss": 0.9016,
      "step": 2930
    },
    {
      "epoch": 0.14782783588093323,
      "grad_norm": 24.0,
      "learning_rate": 1.9704344328238133e-05,
      "loss": 0.8496,
      "step": 2940
    },
    {
      "epoch": 0.14833065164923573,
      "grad_norm": 5.28125,
      "learning_rate": 1.9703338696701532e-05,
      "loss": 1.0099,
      "step": 2950
    },
    {
      "epoch": 0.14883346741753822,
      "grad_norm": 23.75,
      "learning_rate": 1.9702333065164924e-05,
      "loss": 1.2633,
      "step": 2960
    },
    {
      "epoch": 0.14933628318584072,
      "grad_norm": 6.21875,
      "learning_rate": 1.970132743362832e-05,
      "loss": 1.1463,
      "step": 2970
    },
    {
      "epoch": 0.1498390989541432,
      "grad_norm": 20.375,
      "learning_rate": 1.9700321802091716e-05,
      "loss": 1.2526,
      "step": 2980
    },
    {
      "epoch": 0.1503419147224457,
      "grad_norm": 7.40625,
      "learning_rate": 1.969931617055511e-05,
      "loss": 1.258,
      "step": 2990
    },
    {
      "epoch": 0.1508447304907482,
      "grad_norm": 65.5,
      "learning_rate": 1.9698310539018504e-05,
      "loss": 1.0141,
      "step": 3000
    },
    {
      "epoch": 0.1508447304907482,
      "eval_accuracy": 0.5075359444719881,
      "eval_loss": 1.1231328248977661,
      "eval_runtime": 463.832,
      "eval_samples_per_second": 86.971,
      "eval_steps_per_second": 86.971,
      "step": 3000
    },
    {
      "epoch": 0.1513475462590507,
      "grad_norm": 31.875,
      "learning_rate": 1.96973049074819e-05,
      "loss": 1.0824,
      "step": 3010
    },
    {
      "epoch": 0.15185036202735316,
      "grad_norm": 16.75,
      "learning_rate": 1.9696299275945293e-05,
      "loss": 1.2176,
      "step": 3020
    },
    {
      "epoch": 0.15235317779565566,
      "grad_norm": 21.875,
      "learning_rate": 1.9695293644408692e-05,
      "loss": 0.918,
      "step": 3030
    },
    {
      "epoch": 0.15285599356395815,
      "grad_norm": 18.875,
      "learning_rate": 1.9694288012872085e-05,
      "loss": 0.9132,
      "step": 3040
    },
    {
      "epoch": 0.15335880933226065,
      "grad_norm": 19.375,
      "learning_rate": 1.969328238133548e-05,
      "loss": 1.0541,
      "step": 3050
    },
    {
      "epoch": 0.15386162510056314,
      "grad_norm": 31.0,
      "learning_rate": 1.9692276749798876e-05,
      "loss": 1.023,
      "step": 3060
    },
    {
      "epoch": 0.15436444086886564,
      "grad_norm": 24.625,
      "learning_rate": 1.969127111826227e-05,
      "loss": 1.2168,
      "step": 3070
    },
    {
      "epoch": 0.15486725663716813,
      "grad_norm": 19.5,
      "learning_rate": 1.9690265486725665e-05,
      "loss": 0.968,
      "step": 3080
    },
    {
      "epoch": 0.15537007240547063,
      "grad_norm": 64.5,
      "learning_rate": 1.968925985518906e-05,
      "loss": 1.3459,
      "step": 3090
    },
    {
      "epoch": 0.15587288817377312,
      "grad_norm": 9.0,
      "learning_rate": 1.9688254223652453e-05,
      "loss": 0.8097,
      "step": 3100
    },
    {
      "epoch": 0.15637570394207562,
      "grad_norm": 16.375,
      "learning_rate": 1.9687248592115852e-05,
      "loss": 0.6782,
      "step": 3110
    },
    {
      "epoch": 0.1568785197103781,
      "grad_norm": 9.375,
      "learning_rate": 1.9686242960579245e-05,
      "loss": 1.1597,
      "step": 3120
    },
    {
      "epoch": 0.1573813354786806,
      "grad_norm": 19.875,
      "learning_rate": 1.968523732904264e-05,
      "loss": 1.2039,
      "step": 3130
    },
    {
      "epoch": 0.1578841512469831,
      "grad_norm": 13.875,
      "learning_rate": 1.9684231697506037e-05,
      "loss": 0.9437,
      "step": 3140
    },
    {
      "epoch": 0.1583869670152856,
      "grad_norm": 125.5,
      "learning_rate": 1.968322606596943e-05,
      "loss": 1.0803,
      "step": 3150
    },
    {
      "epoch": 0.1588897827835881,
      "grad_norm": 11.875,
      "learning_rate": 1.9682220434432825e-05,
      "loss": 1.1613,
      "step": 3160
    },
    {
      "epoch": 0.15939259855189059,
      "grad_norm": 17.5,
      "learning_rate": 1.968121480289622e-05,
      "loss": 1.1287,
      "step": 3170
    },
    {
      "epoch": 0.15989541432019308,
      "grad_norm": 27.125,
      "learning_rate": 1.9680209171359613e-05,
      "loss": 1.0549,
      "step": 3180
    },
    {
      "epoch": 0.16039823008849557,
      "grad_norm": 16.0,
      "learning_rate": 1.967920353982301e-05,
      "loss": 1.2829,
      "step": 3190
    },
    {
      "epoch": 0.16090104585679807,
      "grad_norm": 33.5,
      "learning_rate": 1.9678197908286405e-05,
      "loss": 0.6152,
      "step": 3200
    },
    {
      "epoch": 0.16140386162510056,
      "grad_norm": 44.5,
      "learning_rate": 1.96771922767498e-05,
      "loss": 1.0514,
      "step": 3210
    },
    {
      "epoch": 0.16190667739340306,
      "grad_norm": 80.5,
      "learning_rate": 1.9676186645213197e-05,
      "loss": 1.0487,
      "step": 3220
    },
    {
      "epoch": 0.16240949316170555,
      "grad_norm": 48.25,
      "learning_rate": 1.967518101367659e-05,
      "loss": 1.0744,
      "step": 3230
    },
    {
      "epoch": 0.16291230893000805,
      "grad_norm": 24.875,
      "learning_rate": 1.9674175382139985e-05,
      "loss": 1.1655,
      "step": 3240
    },
    {
      "epoch": 0.16341512469831054,
      "grad_norm": 37.0,
      "learning_rate": 1.967316975060338e-05,
      "loss": 1.0573,
      "step": 3250
    },
    {
      "epoch": 0.16391794046661304,
      "grad_norm": 9.0,
      "learning_rate": 1.9672164119066774e-05,
      "loss": 1.0104,
      "step": 3260
    },
    {
      "epoch": 0.16442075623491553,
      "grad_norm": 3.90625,
      "learning_rate": 1.967115848753017e-05,
      "loss": 0.8098,
      "step": 3270
    },
    {
      "epoch": 0.16492357200321803,
      "grad_norm": 15.375,
      "learning_rate": 1.9670152855993565e-05,
      "loss": 0.8626,
      "step": 3280
    },
    {
      "epoch": 0.16542638777152052,
      "grad_norm": 40.5,
      "learning_rate": 1.966914722445696e-05,
      "loss": 0.9948,
      "step": 3290
    },
    {
      "epoch": 0.16592920353982302,
      "grad_norm": 22.375,
      "learning_rate": 1.9668141592920357e-05,
      "loss": 0.9389,
      "step": 3300
    },
    {
      "epoch": 0.1664320193081255,
      "grad_norm": 19.375,
      "learning_rate": 1.966713596138375e-05,
      "loss": 1.3179,
      "step": 3310
    },
    {
      "epoch": 0.166934835076428,
      "grad_norm": 8.9375,
      "learning_rate": 1.9666130329847145e-05,
      "loss": 0.8864,
      "step": 3320
    },
    {
      "epoch": 0.1674376508447305,
      "grad_norm": 17.5,
      "learning_rate": 1.966512469831054e-05,
      "loss": 1.1146,
      "step": 3330
    },
    {
      "epoch": 0.167940466613033,
      "grad_norm": 29.5,
      "learning_rate": 1.9664119066773934e-05,
      "loss": 1.1229,
      "step": 3340
    },
    {
      "epoch": 0.1684432823813355,
      "grad_norm": 54.25,
      "learning_rate": 1.966311343523733e-05,
      "loss": 1.0238,
      "step": 3350
    },
    {
      "epoch": 0.16894609814963799,
      "grad_norm": 33.0,
      "learning_rate": 1.9662107803700726e-05,
      "loss": 1.0284,
      "step": 3360
    },
    {
      "epoch": 0.16944891391794048,
      "grad_norm": 26.375,
      "learning_rate": 1.966110217216412e-05,
      "loss": 1.0578,
      "step": 3370
    },
    {
      "epoch": 0.16995172968624295,
      "grad_norm": 66.5,
      "learning_rate": 1.9660096540627517e-05,
      "loss": 1.3476,
      "step": 3380
    },
    {
      "epoch": 0.17045454545454544,
      "grad_norm": 20.625,
      "learning_rate": 1.965909090909091e-05,
      "loss": 1.038,
      "step": 3390
    },
    {
      "epoch": 0.17095736122284794,
      "grad_norm": 23.625,
      "learning_rate": 1.9658085277554306e-05,
      "loss": 0.8734,
      "step": 3400
    },
    {
      "epoch": 0.17146017699115043,
      "grad_norm": 25.75,
      "learning_rate": 1.96570796460177e-05,
      "loss": 0.9469,
      "step": 3410
    },
    {
      "epoch": 0.17196299275945293,
      "grad_norm": 39.0,
      "learning_rate": 1.9656074014481094e-05,
      "loss": 1.0126,
      "step": 3420
    },
    {
      "epoch": 0.17246580852775542,
      "grad_norm": 11.125,
      "learning_rate": 1.965506838294449e-05,
      "loss": 1.171,
      "step": 3430
    },
    {
      "epoch": 0.17296862429605792,
      "grad_norm": 14.9375,
      "learning_rate": 1.9654062751407886e-05,
      "loss": 0.8855,
      "step": 3440
    },
    {
      "epoch": 0.1734714400643604,
      "grad_norm": 18.375,
      "learning_rate": 1.9653057119871282e-05,
      "loss": 0.826,
      "step": 3450
    },
    {
      "epoch": 0.1739742558326629,
      "grad_norm": 34.25,
      "learning_rate": 1.9652051488334674e-05,
      "loss": 1.0488,
      "step": 3460
    },
    {
      "epoch": 0.1744770716009654,
      "grad_norm": 25.5,
      "learning_rate": 1.965104585679807e-05,
      "loss": 1.3076,
      "step": 3470
    },
    {
      "epoch": 0.1749798873692679,
      "grad_norm": 48.25,
      "learning_rate": 1.9650040225261466e-05,
      "loss": 1.0111,
      "step": 3480
    },
    {
      "epoch": 0.1754827031375704,
      "grad_norm": 12.625,
      "learning_rate": 1.9649034593724862e-05,
      "loss": 0.8922,
      "step": 3490
    },
    {
      "epoch": 0.17598551890587288,
      "grad_norm": 9.375,
      "learning_rate": 1.9648028962188254e-05,
      "loss": 0.8531,
      "step": 3500
    },
    {
      "epoch": 0.17598551890587288,
      "eval_accuracy": 0.5079573624194348,
      "eval_loss": 1.1100460290908813,
      "eval_runtime": 464.6001,
      "eval_samples_per_second": 86.827,
      "eval_steps_per_second": 86.827,
      "step": 3500
    },
    {
      "epoch": 0.17648833467417538,
      "grad_norm": 8.75,
      "learning_rate": 1.964702333065165e-05,
      "loss": 0.9134,
      "step": 3510
    },
    {
      "epoch": 0.17699115044247787,
      "grad_norm": 28.375,
      "learning_rate": 1.9646017699115046e-05,
      "loss": 0.8783,
      "step": 3520
    },
    {
      "epoch": 0.17749396621078037,
      "grad_norm": 50.75,
      "learning_rate": 1.9645012067578442e-05,
      "loss": 1.157,
      "step": 3530
    },
    {
      "epoch": 0.17799678197908286,
      "grad_norm": 7.1875,
      "learning_rate": 1.9644006436041834e-05,
      "loss": 0.9487,
      "step": 3540
    },
    {
      "epoch": 0.17849959774738536,
      "grad_norm": 20.625,
      "learning_rate": 1.964300080450523e-05,
      "loss": 1.0309,
      "step": 3550
    },
    {
      "epoch": 0.17900241351568785,
      "grad_norm": 11.125,
      "learning_rate": 1.9641995172968626e-05,
      "loss": 1.1905,
      "step": 3560
    },
    {
      "epoch": 0.17950522928399035,
      "grad_norm": 48.75,
      "learning_rate": 1.9640989541432022e-05,
      "loss": 1.1218,
      "step": 3570
    },
    {
      "epoch": 0.18000804505229284,
      "grad_norm": 19.0,
      "learning_rate": 1.9639983909895418e-05,
      "loss": 0.6753,
      "step": 3580
    },
    {
      "epoch": 0.18051086082059534,
      "grad_norm": 77.5,
      "learning_rate": 1.963897827835881e-05,
      "loss": 1.2511,
      "step": 3590
    },
    {
      "epoch": 0.18101367658889783,
      "grad_norm": 5.40625,
      "learning_rate": 1.9637972646822206e-05,
      "loss": 0.9225,
      "step": 3600
    },
    {
      "epoch": 0.18151649235720033,
      "grad_norm": 19.625,
      "learning_rate": 1.9636967015285602e-05,
      "loss": 0.8115,
      "step": 3610
    },
    {
      "epoch": 0.18201930812550282,
      "grad_norm": 32.0,
      "learning_rate": 1.9635961383748995e-05,
      "loss": 1.2921,
      "step": 3620
    },
    {
      "epoch": 0.18252212389380532,
      "grad_norm": 8.375,
      "learning_rate": 1.963495575221239e-05,
      "loss": 1.0419,
      "step": 3630
    },
    {
      "epoch": 0.1830249396621078,
      "grad_norm": 20.875,
      "learning_rate": 1.9633950120675786e-05,
      "loss": 0.98,
      "step": 3640
    },
    {
      "epoch": 0.1835277554304103,
      "grad_norm": 9.1875,
      "learning_rate": 1.9632944489139182e-05,
      "loss": 0.9442,
      "step": 3650
    },
    {
      "epoch": 0.1840305711987128,
      "grad_norm": 45.75,
      "learning_rate": 1.9631938857602578e-05,
      "loss": 1.0846,
      "step": 3660
    },
    {
      "epoch": 0.1845333869670153,
      "grad_norm": 22.125,
      "learning_rate": 1.963093322606597e-05,
      "loss": 1.0126,
      "step": 3670
    },
    {
      "epoch": 0.1850362027353178,
      "grad_norm": 43.75,
      "learning_rate": 1.9629927594529367e-05,
      "loss": 1.1099,
      "step": 3680
    },
    {
      "epoch": 0.18553901850362028,
      "grad_norm": 46.75,
      "learning_rate": 1.9628921962992762e-05,
      "loss": 1.1183,
      "step": 3690
    },
    {
      "epoch": 0.18604183427192278,
      "grad_norm": 13.75,
      "learning_rate": 1.9627916331456155e-05,
      "loss": 0.9883,
      "step": 3700
    },
    {
      "epoch": 0.18654465004022527,
      "grad_norm": 25.875,
      "learning_rate": 1.962691069991955e-05,
      "loss": 1.2724,
      "step": 3710
    },
    {
      "epoch": 0.18704746580852777,
      "grad_norm": 24.625,
      "learning_rate": 1.9625905068382947e-05,
      "loss": 1.1897,
      "step": 3720
    },
    {
      "epoch": 0.18755028157683024,
      "grad_norm": 6.1875,
      "learning_rate": 1.962489943684634e-05,
      "loss": 1.0065,
      "step": 3730
    },
    {
      "epoch": 0.18805309734513273,
      "grad_norm": 31.75,
      "learning_rate": 1.962389380530974e-05,
      "loss": 0.7861,
      "step": 3740
    },
    {
      "epoch": 0.18855591311343523,
      "grad_norm": 10.375,
      "learning_rate": 1.962288817377313e-05,
      "loss": 1.023,
      "step": 3750
    },
    {
      "epoch": 0.18905872888173772,
      "grad_norm": 44.75,
      "learning_rate": 1.9621882542236527e-05,
      "loss": 1.2087,
      "step": 3760
    },
    {
      "epoch": 0.18956154465004021,
      "grad_norm": 35.75,
      "learning_rate": 1.9620876910699923e-05,
      "loss": 1.0604,
      "step": 3770
    },
    {
      "epoch": 0.1900643604183427,
      "grad_norm": 12.75,
      "learning_rate": 1.9619871279163315e-05,
      "loss": 1.0656,
      "step": 3780
    },
    {
      "epoch": 0.1905671761866452,
      "grad_norm": 12.375,
      "learning_rate": 1.961886564762671e-05,
      "loss": 0.9752,
      "step": 3790
    },
    {
      "epoch": 0.1910699919549477,
      "grad_norm": 17.0,
      "learning_rate": 1.9617860016090107e-05,
      "loss": 1.2524,
      "step": 3800
    },
    {
      "epoch": 0.1915728077232502,
      "grad_norm": 19.375,
      "learning_rate": 1.96168543845535e-05,
      "loss": 0.7638,
      "step": 3810
    },
    {
      "epoch": 0.1920756234915527,
      "grad_norm": 26.75,
      "learning_rate": 1.96158487530169e-05,
      "loss": 1.0806,
      "step": 3820
    },
    {
      "epoch": 0.19257843925985518,
      "grad_norm": 17.125,
      "learning_rate": 1.961484312148029e-05,
      "loss": 0.9267,
      "step": 3830
    },
    {
      "epoch": 0.19308125502815768,
      "grad_norm": 6.09375,
      "learning_rate": 1.9613837489943687e-05,
      "loss": 1.0405,
      "step": 3840
    },
    {
      "epoch": 0.19358407079646017,
      "grad_norm": 10.5,
      "learning_rate": 1.9612831858407083e-05,
      "loss": 0.968,
      "step": 3850
    },
    {
      "epoch": 0.19408688656476267,
      "grad_norm": 52.25,
      "learning_rate": 1.9611826226870475e-05,
      "loss": 1.0611,
      "step": 3860
    },
    {
      "epoch": 0.19458970233306516,
      "grad_norm": 16.875,
      "learning_rate": 1.961082059533387e-05,
      "loss": 1.0277,
      "step": 3870
    },
    {
      "epoch": 0.19509251810136766,
      "grad_norm": 29.125,
      "learning_rate": 1.9609814963797267e-05,
      "loss": 1.0115,
      "step": 3880
    },
    {
      "epoch": 0.19559533386967015,
      "grad_norm": 17.125,
      "learning_rate": 1.960880933226066e-05,
      "loss": 0.7579,
      "step": 3890
    },
    {
      "epoch": 0.19609814963797265,
      "grad_norm": 12.3125,
      "learning_rate": 1.960780370072406e-05,
      "loss": 0.9264,
      "step": 3900
    },
    {
      "epoch": 0.19660096540627514,
      "grad_norm": 10.25,
      "learning_rate": 1.960679806918745e-05,
      "loss": 0.982,
      "step": 3910
    },
    {
      "epoch": 0.19710378117457764,
      "grad_norm": 17.375,
      "learning_rate": 1.9605792437650847e-05,
      "loss": 1.0946,
      "step": 3920
    },
    {
      "epoch": 0.19760659694288013,
      "grad_norm": 8.3125,
      "learning_rate": 1.9604786806114243e-05,
      "loss": 0.8287,
      "step": 3930
    },
    {
      "epoch": 0.19810941271118263,
      "grad_norm": 60.25,
      "learning_rate": 1.9603781174577636e-05,
      "loss": 0.6994,
      "step": 3940
    },
    {
      "epoch": 0.19861222847948512,
      "grad_norm": 25.375,
      "learning_rate": 1.960277554304103e-05,
      "loss": 1.2245,
      "step": 3950
    },
    {
      "epoch": 0.19911504424778761,
      "grad_norm": 8.5625,
      "learning_rate": 1.9601769911504427e-05,
      "loss": 0.9013,
      "step": 3960
    },
    {
      "epoch": 0.1996178600160901,
      "grad_norm": 27.5,
      "learning_rate": 1.960076427996782e-05,
      "loss": 1.1515,
      "step": 3970
    },
    {
      "epoch": 0.2001206757843926,
      "grad_norm": 10.25,
      "learning_rate": 1.9599758648431216e-05,
      "loss": 1.1107,
      "step": 3980
    },
    {
      "epoch": 0.2006234915526951,
      "grad_norm": 28.75,
      "learning_rate": 1.9598753016894612e-05,
      "loss": 1.2571,
      "step": 3990
    },
    {
      "epoch": 0.2011263073209976,
      "grad_norm": 33.5,
      "learning_rate": 1.9597747385358004e-05,
      "loss": 1.0971,
      "step": 4000
    },
    {
      "epoch": 0.2011263073209976,
      "eval_accuracy": 0.5077590480912246,
      "eval_loss": 1.097894310951233,
      "eval_runtime": 464.6177,
      "eval_samples_per_second": 86.824,
      "eval_steps_per_second": 86.824,
      "step": 4000
    },
    {
      "epoch": 0.2016291230893001,
      "grad_norm": 3.84375,
      "learning_rate": 1.9596741753821404e-05,
      "loss": 1.1303,
      "step": 4010
    },
    {
      "epoch": 0.20213193885760258,
      "grad_norm": 22.375,
      "learning_rate": 1.9595736122284796e-05,
      "loss": 1.0239,
      "step": 4020
    },
    {
      "epoch": 0.20263475462590508,
      "grad_norm": 16.625,
      "learning_rate": 1.9594730490748192e-05,
      "loss": 1.1429,
      "step": 4030
    },
    {
      "epoch": 0.20313757039420757,
      "grad_norm": 36.0,
      "learning_rate": 1.9593724859211588e-05,
      "loss": 0.7378,
      "step": 4040
    },
    {
      "epoch": 0.20364038616251007,
      "grad_norm": 28.375,
      "learning_rate": 1.959271922767498e-05,
      "loss": 0.969,
      "step": 4050
    },
    {
      "epoch": 0.20414320193081256,
      "grad_norm": 14.75,
      "learning_rate": 1.9591713596138376e-05,
      "loss": 0.9995,
      "step": 4060
    },
    {
      "epoch": 0.20464601769911506,
      "grad_norm": 20.125,
      "learning_rate": 1.9590707964601772e-05,
      "loss": 1.1259,
      "step": 4070
    },
    {
      "epoch": 0.20514883346741755,
      "grad_norm": 51.0,
      "learning_rate": 1.9589702333065164e-05,
      "loss": 1.1447,
      "step": 4080
    },
    {
      "epoch": 0.20565164923572002,
      "grad_norm": 23.875,
      "learning_rate": 1.9588696701528564e-05,
      "loss": 0.8061,
      "step": 4090
    },
    {
      "epoch": 0.2061544650040225,
      "grad_norm": 8.6875,
      "learning_rate": 1.9587691069991956e-05,
      "loss": 1.1893,
      "step": 4100
    },
    {
      "epoch": 0.206657280772325,
      "grad_norm": 22.375,
      "learning_rate": 1.9586685438455352e-05,
      "loss": 0.7757,
      "step": 4110
    },
    {
      "epoch": 0.2071600965406275,
      "grad_norm": 22.75,
      "learning_rate": 1.9585679806918748e-05,
      "loss": 1.3131,
      "step": 4120
    },
    {
      "epoch": 0.20766291230893,
      "grad_norm": 9.4375,
      "learning_rate": 1.958467417538214e-05,
      "loss": 0.9484,
      "step": 4130
    },
    {
      "epoch": 0.2081657280772325,
      "grad_norm": 19.875,
      "learning_rate": 1.9583668543845536e-05,
      "loss": 1.1217,
      "step": 4140
    },
    {
      "epoch": 0.208668543845535,
      "grad_norm": 21.25,
      "learning_rate": 1.9582662912308932e-05,
      "loss": 0.8891,
      "step": 4150
    },
    {
      "epoch": 0.20917135961383748,
      "grad_norm": 29.875,
      "learning_rate": 1.9581657280772325e-05,
      "loss": 1.356,
      "step": 4160
    },
    {
      "epoch": 0.20967417538213998,
      "grad_norm": 27.5,
      "learning_rate": 1.9580651649235724e-05,
      "loss": 0.953,
      "step": 4170
    },
    {
      "epoch": 0.21017699115044247,
      "grad_norm": 16.375,
      "learning_rate": 1.9579646017699117e-05,
      "loss": 1.2229,
      "step": 4180
    },
    {
      "epoch": 0.21067980691874497,
      "grad_norm": 10.8125,
      "learning_rate": 1.9578640386162512e-05,
      "loss": 0.8228,
      "step": 4190
    },
    {
      "epoch": 0.21118262268704746,
      "grad_norm": 38.0,
      "learning_rate": 1.9577634754625908e-05,
      "loss": 1.3557,
      "step": 4200
    },
    {
      "epoch": 0.21168543845534996,
      "grad_norm": 39.5,
      "learning_rate": 1.95766291230893e-05,
      "loss": 1.24,
      "step": 4210
    },
    {
      "epoch": 0.21218825422365245,
      "grad_norm": 35.0,
      "learning_rate": 1.9575623491552697e-05,
      "loss": 1.0844,
      "step": 4220
    },
    {
      "epoch": 0.21269106999195495,
      "grad_norm": 28.375,
      "learning_rate": 1.9574617860016093e-05,
      "loss": 1.1495,
      "step": 4230
    },
    {
      "epoch": 0.21319388576025744,
      "grad_norm": 18.75,
      "learning_rate": 1.9573612228479485e-05,
      "loss": 0.9143,
      "step": 4240
    },
    {
      "epoch": 0.21369670152855993,
      "grad_norm": 25.375,
      "learning_rate": 1.957260659694288e-05,
      "loss": 1.1959,
      "step": 4250
    },
    {
      "epoch": 0.21419951729686243,
      "grad_norm": 17.875,
      "learning_rate": 1.9571600965406277e-05,
      "loss": 1.0442,
      "step": 4260
    },
    {
      "epoch": 0.21470233306516492,
      "grad_norm": 13.25,
      "learning_rate": 1.957059533386967e-05,
      "loss": 1.0244,
      "step": 4270
    },
    {
      "epoch": 0.21520514883346742,
      "grad_norm": 21.0,
      "learning_rate": 1.956958970233307e-05,
      "loss": 1.0442,
      "step": 4280
    },
    {
      "epoch": 0.2157079646017699,
      "grad_norm": 15.375,
      "learning_rate": 1.956858407079646e-05,
      "loss": 0.861,
      "step": 4290
    },
    {
      "epoch": 0.2162107803700724,
      "grad_norm": 65.5,
      "learning_rate": 1.9567578439259857e-05,
      "loss": 1.4535,
      "step": 4300
    },
    {
      "epoch": 0.2167135961383749,
      "grad_norm": 23.25,
      "learning_rate": 1.9566572807723253e-05,
      "loss": 1.0114,
      "step": 4310
    },
    {
      "epoch": 0.2172164119066774,
      "grad_norm": 14.1875,
      "learning_rate": 1.9565567176186645e-05,
      "loss": 1.2991,
      "step": 4320
    },
    {
      "epoch": 0.2177192276749799,
      "grad_norm": 48.75,
      "learning_rate": 1.956456154465004e-05,
      "loss": 1.323,
      "step": 4330
    },
    {
      "epoch": 0.2182220434432824,
      "grad_norm": 25.75,
      "learning_rate": 1.9563555913113437e-05,
      "loss": 1.0964,
      "step": 4340
    },
    {
      "epoch": 0.21872485921158488,
      "grad_norm": 12.3125,
      "learning_rate": 1.956255028157683e-05,
      "loss": 1.1695,
      "step": 4350
    },
    {
      "epoch": 0.21922767497988738,
      "grad_norm": 28.625,
      "learning_rate": 1.956154465004023e-05,
      "loss": 1.1963,
      "step": 4360
    },
    {
      "epoch": 0.21973049074818987,
      "grad_norm": 11.9375,
      "learning_rate": 1.956053901850362e-05,
      "loss": 0.8997,
      "step": 4370
    },
    {
      "epoch": 0.22023330651649237,
      "grad_norm": 18.5,
      "learning_rate": 1.9559533386967017e-05,
      "loss": 1.2917,
      "step": 4380
    },
    {
      "epoch": 0.22073612228479486,
      "grad_norm": 6.96875,
      "learning_rate": 1.9558527755430413e-05,
      "loss": 0.684,
      "step": 4390
    },
    {
      "epoch": 0.22123893805309736,
      "grad_norm": 22.25,
      "learning_rate": 1.9557522123893806e-05,
      "loss": 0.9889,
      "step": 4400
    },
    {
      "epoch": 0.22174175382139985,
      "grad_norm": 39.25,
      "learning_rate": 1.95565164923572e-05,
      "loss": 0.9339,
      "step": 4410
    },
    {
      "epoch": 0.22224456958970235,
      "grad_norm": 46.25,
      "learning_rate": 1.9555510860820597e-05,
      "loss": 0.9435,
      "step": 4420
    },
    {
      "epoch": 0.22274738535800484,
      "grad_norm": 36.75,
      "learning_rate": 1.955450522928399e-05,
      "loss": 1.0856,
      "step": 4430
    },
    {
      "epoch": 0.2232502011263073,
      "grad_norm": 15.625,
      "learning_rate": 1.955349959774739e-05,
      "loss": 0.9348,
      "step": 4440
    },
    {
      "epoch": 0.2237530168946098,
      "grad_norm": 27.25,
      "learning_rate": 1.955249396621078e-05,
      "loss": 0.9071,
      "step": 4450
    },
    {
      "epoch": 0.2242558326629123,
      "grad_norm": 25.75,
      "learning_rate": 1.9551488334674177e-05,
      "loss": 0.9518,
      "step": 4460
    },
    {
      "epoch": 0.2247586484312148,
      "grad_norm": 38.25,
      "learning_rate": 1.9550482703137573e-05,
      "loss": 1.0416,
      "step": 4470
    },
    {
      "epoch": 0.2252614641995173,
      "grad_norm": 7.625,
      "learning_rate": 1.9549477071600966e-05,
      "loss": 1.0962,
      "step": 4480
    },
    {
      "epoch": 0.22576427996781978,
      "grad_norm": 47.5,
      "learning_rate": 1.954847144006436e-05,
      "loss": 1.1588,
      "step": 4490
    },
    {
      "epoch": 0.22626709573612228,
      "grad_norm": 31.25,
      "learning_rate": 1.9547465808527758e-05,
      "loss": 1.0928,
      "step": 4500
    },
    {
      "epoch": 0.22626709573612228,
      "eval_accuracy": 0.5092464055528012,
      "eval_loss": 1.0858546495437622,
      "eval_runtime": 465.0066,
      "eval_samples_per_second": 86.751,
      "eval_steps_per_second": 86.751,
      "step": 4500
    },
    {
      "epoch": 0.22676991150442477,
      "grad_norm": 42.75,
      "learning_rate": 1.954646017699115e-05,
      "loss": 0.8462,
      "step": 4510
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 10.625,
      "learning_rate": 1.9545454545454546e-05,
      "loss": 1.0109,
      "step": 4520
    },
    {
      "epoch": 0.22777554304102976,
      "grad_norm": 16.375,
      "learning_rate": 1.9544448913917942e-05,
      "loss": 0.9981,
      "step": 4530
    },
    {
      "epoch": 0.22827835880933225,
      "grad_norm": 23.5,
      "learning_rate": 1.9543443282381338e-05,
      "loss": 0.9579,
      "step": 4540
    },
    {
      "epoch": 0.22878117457763475,
      "grad_norm": 6.0625,
      "learning_rate": 1.9542437650844734e-05,
      "loss": 1.1276,
      "step": 4550
    },
    {
      "epoch": 0.22928399034593724,
      "grad_norm": 29.375,
      "learning_rate": 1.9541432019308126e-05,
      "loss": 1.0457,
      "step": 4560
    },
    {
      "epoch": 0.22978680611423974,
      "grad_norm": 39.5,
      "learning_rate": 1.9540426387771522e-05,
      "loss": 1.1814,
      "step": 4570
    },
    {
      "epoch": 0.23028962188254223,
      "grad_norm": 11.0625,
      "learning_rate": 1.9539420756234918e-05,
      "loss": 1.5126,
      "step": 4580
    },
    {
      "epoch": 0.23079243765084473,
      "grad_norm": 26.0,
      "learning_rate": 1.953841512469831e-05,
      "loss": 0.8664,
      "step": 4590
    },
    {
      "epoch": 0.23129525341914722,
      "grad_norm": 14.4375,
      "learning_rate": 1.9537409493161706e-05,
      "loss": 1.2398,
      "step": 4600
    },
    {
      "epoch": 0.23179806918744972,
      "grad_norm": 25.25,
      "learning_rate": 1.9536403861625102e-05,
      "loss": 1.1701,
      "step": 4610
    },
    {
      "epoch": 0.2323008849557522,
      "grad_norm": 14.125,
      "learning_rate": 1.9535398230088498e-05,
      "loss": 1.1474,
      "step": 4620
    },
    {
      "epoch": 0.2328037007240547,
      "grad_norm": 38.0,
      "learning_rate": 1.9534392598551894e-05,
      "loss": 0.9085,
      "step": 4630
    },
    {
      "epoch": 0.2333065164923572,
      "grad_norm": 63.0,
      "learning_rate": 1.9533386967015286e-05,
      "loss": 0.9802,
      "step": 4640
    },
    {
      "epoch": 0.2338093322606597,
      "grad_norm": 10.6875,
      "learning_rate": 1.9532381335478682e-05,
      "loss": 0.9867,
      "step": 4650
    },
    {
      "epoch": 0.2343121480289622,
      "grad_norm": 7.9375,
      "learning_rate": 1.9531375703942078e-05,
      "loss": 0.8567,
      "step": 4660
    },
    {
      "epoch": 0.2348149637972647,
      "grad_norm": 14.3125,
      "learning_rate": 1.953037007240547e-05,
      "loss": 0.9659,
      "step": 4670
    },
    {
      "epoch": 0.23531777956556718,
      "grad_norm": 22.125,
      "learning_rate": 1.9529364440868866e-05,
      "loss": 0.9663,
      "step": 4680
    },
    {
      "epoch": 0.23582059533386968,
      "grad_norm": 12.3125,
      "learning_rate": 1.9528358809332262e-05,
      "loss": 1.0533,
      "step": 4690
    },
    {
      "epoch": 0.23632341110217217,
      "grad_norm": 41.75,
      "learning_rate": 1.9527353177795658e-05,
      "loss": 1.3924,
      "step": 4700
    },
    {
      "epoch": 0.23682622687047467,
      "grad_norm": 20.25,
      "learning_rate": 1.9526347546259054e-05,
      "loss": 0.8891,
      "step": 4710
    },
    {
      "epoch": 0.23732904263877716,
      "grad_norm": 7.84375,
      "learning_rate": 1.9525341914722447e-05,
      "loss": 1.4057,
      "step": 4720
    },
    {
      "epoch": 0.23783185840707965,
      "grad_norm": 24.25,
      "learning_rate": 1.9524336283185842e-05,
      "loss": 0.9,
      "step": 4730
    },
    {
      "epoch": 0.23833467417538215,
      "grad_norm": 16.0,
      "learning_rate": 1.9523330651649238e-05,
      "loss": 0.7198,
      "step": 4740
    },
    {
      "epoch": 0.23883748994368464,
      "grad_norm": 52.25,
      "learning_rate": 1.952232502011263e-05,
      "loss": 1.1407,
      "step": 4750
    },
    {
      "epoch": 0.23934030571198714,
      "grad_norm": 24.25,
      "learning_rate": 1.9521319388576027e-05,
      "loss": 0.9783,
      "step": 4760
    },
    {
      "epoch": 0.23984312148028963,
      "grad_norm": 16.625,
      "learning_rate": 1.9520313757039423e-05,
      "loss": 0.7954,
      "step": 4770
    },
    {
      "epoch": 0.24034593724859213,
      "grad_norm": 36.0,
      "learning_rate": 1.951930812550282e-05,
      "loss": 0.7206,
      "step": 4780
    },
    {
      "epoch": 0.24084875301689462,
      "grad_norm": 47.0,
      "learning_rate": 1.951830249396621e-05,
      "loss": 1.1326,
      "step": 4790
    },
    {
      "epoch": 0.2413515687851971,
      "grad_norm": 11.25,
      "learning_rate": 1.9517296862429607e-05,
      "loss": 1.0167,
      "step": 4800
    },
    {
      "epoch": 0.24185438455349959,
      "grad_norm": 24.875,
      "learning_rate": 1.9516291230893003e-05,
      "loss": 0.9624,
      "step": 4810
    },
    {
      "epoch": 0.24235720032180208,
      "grad_norm": 20.125,
      "learning_rate": 1.95152855993564e-05,
      "loss": 1.3877,
      "step": 4820
    },
    {
      "epoch": 0.24286001609010457,
      "grad_norm": 8.8125,
      "learning_rate": 1.951427996781979e-05,
      "loss": 1.1241,
      "step": 4830
    },
    {
      "epoch": 0.24336283185840707,
      "grad_norm": 11.4375,
      "learning_rate": 1.9513274336283187e-05,
      "loss": 1.27,
      "step": 4840
    },
    {
      "epoch": 0.24386564762670956,
      "grad_norm": 31.625,
      "learning_rate": 1.9512268704746583e-05,
      "loss": 1.2343,
      "step": 4850
    },
    {
      "epoch": 0.24436846339501206,
      "grad_norm": 34.5,
      "learning_rate": 1.951126307320998e-05,
      "loss": 1.0984,
      "step": 4860
    },
    {
      "epoch": 0.24487127916331455,
      "grad_norm": 15.875,
      "learning_rate": 1.951025744167337e-05,
      "loss": 1.3032,
      "step": 4870
    },
    {
      "epoch": 0.24537409493161705,
      "grad_norm": 59.75,
      "learning_rate": 1.9509251810136767e-05,
      "loss": 1.1212,
      "step": 4880
    },
    {
      "epoch": 0.24587691069991954,
      "grad_norm": 60.0,
      "learning_rate": 1.9508246178600163e-05,
      "loss": 1.2908,
      "step": 4890
    },
    {
      "epoch": 0.24637972646822204,
      "grad_norm": 23.125,
      "learning_rate": 1.950724054706356e-05,
      "loss": 0.9796,
      "step": 4900
    },
    {
      "epoch": 0.24688254223652453,
      "grad_norm": 6.875,
      "learning_rate": 1.950623491552695e-05,
      "loss": 0.8464,
      "step": 4910
    },
    {
      "epoch": 0.24738535800482703,
      "grad_norm": 31.25,
      "learning_rate": 1.9505229283990347e-05,
      "loss": 1.0916,
      "step": 4920
    },
    {
      "epoch": 0.24788817377312952,
      "grad_norm": 8.8125,
      "learning_rate": 1.9504223652453743e-05,
      "loss": 0.9614,
      "step": 4930
    },
    {
      "epoch": 0.24839098954143202,
      "grad_norm": 17.0,
      "learning_rate": 1.950321802091714e-05,
      "loss": 0.9078,
      "step": 4940
    },
    {
      "epoch": 0.2488938053097345,
      "grad_norm": 26.0,
      "learning_rate": 1.950221238938053e-05,
      "loss": 1.055,
      "step": 4950
    },
    {
      "epoch": 0.249396621078037,
      "grad_norm": 35.75,
      "learning_rate": 1.9501206757843927e-05,
      "loss": 1.2193,
      "step": 4960
    },
    {
      "epoch": 0.2498994368463395,
      "grad_norm": 55.75,
      "learning_rate": 1.9500201126307323e-05,
      "loss": 1.2054,
      "step": 4970
    },
    {
      "epoch": 0.250402252614642,
      "grad_norm": 28.0,
      "learning_rate": 1.949919549477072e-05,
      "loss": 1.0101,
      "step": 4980
    },
    {
      "epoch": 0.25090506838294446,
      "grad_norm": 32.75,
      "learning_rate": 1.949818986323411e-05,
      "loss": 1.0498,
      "step": 4990
    },
    {
      "epoch": 0.251407884151247,
      "grad_norm": 19.0,
      "learning_rate": 1.9497184231697507e-05,
      "loss": 1.1074,
      "step": 5000
    },
    {
      "epoch": 0.251407884151247,
      "eval_accuracy": 0.5091224590976698,
      "eval_loss": 1.0767416954040527,
      "eval_runtime": 464.9769,
      "eval_samples_per_second": 86.757,
      "eval_steps_per_second": 86.757,
      "step": 5000
    },
    {
      "epoch": 0.25191069991954945,
      "grad_norm": 30.0,
      "learning_rate": 1.9496178600160903e-05,
      "loss": 0.7497,
      "step": 5010
    },
    {
      "epoch": 0.252413515687852,
      "grad_norm": 33.25,
      "learning_rate": 1.94951729686243e-05,
      "loss": 0.9702,
      "step": 5020
    },
    {
      "epoch": 0.25291633145615444,
      "grad_norm": 6.59375,
      "learning_rate": 1.949416733708769e-05,
      "loss": 1.0224,
      "step": 5030
    },
    {
      "epoch": 0.25341914722445696,
      "grad_norm": 121.0,
      "learning_rate": 1.9493161705551088e-05,
      "loss": 1.2505,
      "step": 5040
    },
    {
      "epoch": 0.25392196299275943,
      "grad_norm": 20.75,
      "learning_rate": 1.9492156074014483e-05,
      "loss": 0.9511,
      "step": 5050
    },
    {
      "epoch": 0.25442477876106195,
      "grad_norm": 9.5625,
      "learning_rate": 1.9491150442477876e-05,
      "loss": 1.1465,
      "step": 5060
    },
    {
      "epoch": 0.2549275945293644,
      "grad_norm": 41.25,
      "learning_rate": 1.9490144810941272e-05,
      "loss": 1.0936,
      "step": 5070
    },
    {
      "epoch": 0.25543041029766694,
      "grad_norm": 24.75,
      "learning_rate": 1.9489139179404668e-05,
      "loss": 1.0785,
      "step": 5080
    },
    {
      "epoch": 0.2559332260659694,
      "grad_norm": 13.6875,
      "learning_rate": 1.9488133547868064e-05,
      "loss": 1.1495,
      "step": 5090
    },
    {
      "epoch": 0.25643604183427193,
      "grad_norm": 34.25,
      "learning_rate": 1.948712791633146e-05,
      "loss": 0.8534,
      "step": 5100
    },
    {
      "epoch": 0.2569388576025744,
      "grad_norm": 27.625,
      "learning_rate": 1.9486122284794852e-05,
      "loss": 1.2319,
      "step": 5110
    },
    {
      "epoch": 0.2574416733708769,
      "grad_norm": 24.5,
      "learning_rate": 1.9485116653258248e-05,
      "loss": 1.2113,
      "step": 5120
    },
    {
      "epoch": 0.2579444891391794,
      "grad_norm": 53.0,
      "learning_rate": 1.9484111021721644e-05,
      "loss": 0.9822,
      "step": 5130
    },
    {
      "epoch": 0.2584473049074819,
      "grad_norm": 43.0,
      "learning_rate": 1.9483105390185036e-05,
      "loss": 1.168,
      "step": 5140
    },
    {
      "epoch": 0.2589501206757844,
      "grad_norm": 12.1875,
      "learning_rate": 1.9482099758648432e-05,
      "loss": 1.1717,
      "step": 5150
    },
    {
      "epoch": 0.2594529364440869,
      "grad_norm": 52.0,
      "learning_rate": 1.9481094127111828e-05,
      "loss": 1.0938,
      "step": 5160
    },
    {
      "epoch": 0.25995575221238937,
      "grad_norm": 26.875,
      "learning_rate": 1.9480088495575224e-05,
      "loss": 0.9132,
      "step": 5170
    },
    {
      "epoch": 0.2604585679806919,
      "grad_norm": 78.5,
      "learning_rate": 1.947908286403862e-05,
      "loss": 1.2098,
      "step": 5180
    },
    {
      "epoch": 0.26096138374899436,
      "grad_norm": 36.75,
      "learning_rate": 1.9478077232502012e-05,
      "loss": 1.2515,
      "step": 5190
    },
    {
      "epoch": 0.2614641995172969,
      "grad_norm": 24.5,
      "learning_rate": 1.9477071600965408e-05,
      "loss": 0.9122,
      "step": 5200
    },
    {
      "epoch": 0.26196701528559935,
      "grad_norm": 7.53125,
      "learning_rate": 1.9476065969428804e-05,
      "loss": 1.2771,
      "step": 5210
    },
    {
      "epoch": 0.26246983105390187,
      "grad_norm": 13.25,
      "learning_rate": 1.9475060337892196e-05,
      "loss": 1.1985,
      "step": 5220
    },
    {
      "epoch": 0.26297264682220434,
      "grad_norm": 43.0,
      "learning_rate": 1.9474054706355592e-05,
      "loss": 1.1263,
      "step": 5230
    },
    {
      "epoch": 0.26347546259050686,
      "grad_norm": 11.4375,
      "learning_rate": 1.9473049074818988e-05,
      "loss": 1.2956,
      "step": 5240
    },
    {
      "epoch": 0.2639782783588093,
      "grad_norm": 53.5,
      "learning_rate": 1.9472043443282384e-05,
      "loss": 1.0463,
      "step": 5250
    },
    {
      "epoch": 0.26448109412711185,
      "grad_norm": 17.375,
      "learning_rate": 1.947103781174578e-05,
      "loss": 0.8553,
      "step": 5260
    },
    {
      "epoch": 0.2649839098954143,
      "grad_norm": 7.1875,
      "learning_rate": 1.9470032180209172e-05,
      "loss": 1.4328,
      "step": 5270
    },
    {
      "epoch": 0.26548672566371684,
      "grad_norm": 13.875,
      "learning_rate": 1.946902654867257e-05,
      "loss": 0.9863,
      "step": 5280
    },
    {
      "epoch": 0.2659895414320193,
      "grad_norm": 26.625,
      "learning_rate": 1.9468020917135964e-05,
      "loss": 1.0275,
      "step": 5290
    },
    {
      "epoch": 0.2664923572003218,
      "grad_norm": 18.25,
      "learning_rate": 1.9467015285599357e-05,
      "loss": 1.0513,
      "step": 5300
    },
    {
      "epoch": 0.2669951729686243,
      "grad_norm": 23.25,
      "learning_rate": 1.9466009654062753e-05,
      "loss": 0.8166,
      "step": 5310
    },
    {
      "epoch": 0.2674979887369268,
      "grad_norm": 50.0,
      "learning_rate": 1.946500402252615e-05,
      "loss": 0.9392,
      "step": 5320
    },
    {
      "epoch": 0.2680008045052293,
      "grad_norm": 64.5,
      "learning_rate": 1.946399839098954e-05,
      "loss": 0.8568,
      "step": 5330
    },
    {
      "epoch": 0.26850362027353175,
      "grad_norm": 19.375,
      "learning_rate": 1.946299275945294e-05,
      "loss": 0.9158,
      "step": 5340
    },
    {
      "epoch": 0.2690064360418343,
      "grad_norm": 12.3125,
      "learning_rate": 1.9461987127916333e-05,
      "loss": 1.0967,
      "step": 5350
    },
    {
      "epoch": 0.26950925181013674,
      "grad_norm": 51.75,
      "learning_rate": 1.946098149637973e-05,
      "loss": 1.1054,
      "step": 5360
    },
    {
      "epoch": 0.27001206757843926,
      "grad_norm": 6.40625,
      "learning_rate": 1.9459975864843124e-05,
      "loss": 1.2348,
      "step": 5370
    },
    {
      "epoch": 0.27051488334674173,
      "grad_norm": 14.9375,
      "learning_rate": 1.9458970233306517e-05,
      "loss": 0.613,
      "step": 5380
    },
    {
      "epoch": 0.27101769911504425,
      "grad_norm": 40.5,
      "learning_rate": 1.9457964601769913e-05,
      "loss": 0.9572,
      "step": 5390
    },
    {
      "epoch": 0.2715205148833467,
      "grad_norm": 56.75,
      "learning_rate": 1.945695897023331e-05,
      "loss": 0.7692,
      "step": 5400
    },
    {
      "epoch": 0.27202333065164924,
      "grad_norm": 25.75,
      "learning_rate": 1.94559533386967e-05,
      "loss": 0.8777,
      "step": 5410
    },
    {
      "epoch": 0.2725261464199517,
      "grad_norm": 16.625,
      "learning_rate": 1.94549477071601e-05,
      "loss": 0.9062,
      "step": 5420
    },
    {
      "epoch": 0.27302896218825423,
      "grad_norm": 50.5,
      "learning_rate": 1.9453942075623493e-05,
      "loss": 1.1149,
      "step": 5430
    },
    {
      "epoch": 0.2735317779565567,
      "grad_norm": 24.625,
      "learning_rate": 1.945293644408689e-05,
      "loss": 0.9286,
      "step": 5440
    },
    {
      "epoch": 0.2740345937248592,
      "grad_norm": 14.5,
      "learning_rate": 1.9451930812550285e-05,
      "loss": 1.0962,
      "step": 5450
    },
    {
      "epoch": 0.2745374094931617,
      "grad_norm": 16.75,
      "learning_rate": 1.9450925181013677e-05,
      "loss": 1.1608,
      "step": 5460
    },
    {
      "epoch": 0.2750402252614642,
      "grad_norm": 51.75,
      "learning_rate": 1.9449919549477073e-05,
      "loss": 1.1965,
      "step": 5470
    },
    {
      "epoch": 0.2755430410297667,
      "grad_norm": 6.3125,
      "learning_rate": 1.944891391794047e-05,
      "loss": 1.126,
      "step": 5480
    },
    {
      "epoch": 0.2760458567980692,
      "grad_norm": 43.25,
      "learning_rate": 1.944790828640386e-05,
      "loss": 0.8564,
      "step": 5490
    },
    {
      "epoch": 0.27654867256637167,
      "grad_norm": 28.5,
      "learning_rate": 1.944690265486726e-05,
      "loss": 1.4304,
      "step": 5500
    },
    {
      "epoch": 0.27654867256637167,
      "eval_accuracy": 0.5097421913733268,
      "eval_loss": 1.0700719356536865,
      "eval_runtime": 464.6496,
      "eval_samples_per_second": 86.818,
      "eval_steps_per_second": 86.818,
      "step": 5500
    },
    {
      "epoch": 0.2770514883346742,
      "grad_norm": 47.25,
      "learning_rate": 1.9445897023330653e-05,
      "loss": 0.8632,
      "step": 5510
    },
    {
      "epoch": 0.27755430410297666,
      "grad_norm": 6.96875,
      "learning_rate": 1.9444891391794046e-05,
      "loss": 0.9396,
      "step": 5520
    },
    {
      "epoch": 0.2780571198712792,
      "grad_norm": 35.0,
      "learning_rate": 1.9443885760257445e-05,
      "loss": 0.9021,
      "step": 5530
    },
    {
      "epoch": 0.27855993563958165,
      "grad_norm": 13.5,
      "learning_rate": 1.9442880128720837e-05,
      "loss": 0.9664,
      "step": 5540
    },
    {
      "epoch": 0.27906275140788417,
      "grad_norm": 31.25,
      "learning_rate": 1.9441874497184233e-05,
      "loss": 1.5082,
      "step": 5550
    },
    {
      "epoch": 0.27956556717618664,
      "grad_norm": 39.75,
      "learning_rate": 1.944086886564763e-05,
      "loss": 1.3065,
      "step": 5560
    },
    {
      "epoch": 0.28006838294448916,
      "grad_norm": 22.125,
      "learning_rate": 1.9439863234111022e-05,
      "loss": 1.3688,
      "step": 5570
    },
    {
      "epoch": 0.2805711987127916,
      "grad_norm": 37.75,
      "learning_rate": 1.9438857602574418e-05,
      "loss": 0.8464,
      "step": 5580
    },
    {
      "epoch": 0.28107401448109415,
      "grad_norm": 19.625,
      "learning_rate": 1.9437851971037813e-05,
      "loss": 1.3876,
      "step": 5590
    },
    {
      "epoch": 0.2815768302493966,
      "grad_norm": 8.5,
      "learning_rate": 1.9436846339501206e-05,
      "loss": 0.9045,
      "step": 5600
    },
    {
      "epoch": 0.28207964601769914,
      "grad_norm": 5.28125,
      "learning_rate": 1.9435840707964605e-05,
      "loss": 0.9288,
      "step": 5610
    },
    {
      "epoch": 0.2825824617860016,
      "grad_norm": 36.0,
      "learning_rate": 1.9434835076427998e-05,
      "loss": 0.8522,
      "step": 5620
    },
    {
      "epoch": 0.2830852775543041,
      "grad_norm": 12.3125,
      "learning_rate": 1.9433829444891394e-05,
      "loss": 0.9965,
      "step": 5630
    },
    {
      "epoch": 0.2835880933226066,
      "grad_norm": 7.71875,
      "learning_rate": 1.943282381335479e-05,
      "loss": 0.8793,
      "step": 5640
    },
    {
      "epoch": 0.2840909090909091,
      "grad_norm": 3.859375,
      "learning_rate": 1.9431818181818182e-05,
      "loss": 0.8295,
      "step": 5650
    },
    {
      "epoch": 0.2845937248592116,
      "grad_norm": 17.875,
      "learning_rate": 1.9430812550281578e-05,
      "loss": 1.0257,
      "step": 5660
    },
    {
      "epoch": 0.2850965406275141,
      "grad_norm": 23.625,
      "learning_rate": 1.9429806918744974e-05,
      "loss": 1.0378,
      "step": 5670
    },
    {
      "epoch": 0.2855993563958166,
      "grad_norm": 23.125,
      "learning_rate": 1.9428801287208366e-05,
      "loss": 0.9957,
      "step": 5680
    },
    {
      "epoch": 0.28610217216411904,
      "grad_norm": 27.625,
      "learning_rate": 1.9427795655671765e-05,
      "loss": 0.9428,
      "step": 5690
    },
    {
      "epoch": 0.28660498793242156,
      "grad_norm": 14.8125,
      "learning_rate": 1.9426790024135158e-05,
      "loss": 0.8269,
      "step": 5700
    },
    {
      "epoch": 0.28710780370072403,
      "grad_norm": 10.3125,
      "learning_rate": 1.9425784392598554e-05,
      "loss": 1.0813,
      "step": 5710
    },
    {
      "epoch": 0.28761061946902655,
      "grad_norm": 62.0,
      "learning_rate": 1.942477876106195e-05,
      "loss": 0.9799,
      "step": 5720
    },
    {
      "epoch": 0.288113435237329,
      "grad_norm": 51.25,
      "learning_rate": 1.9423773129525342e-05,
      "loss": 0.9181,
      "step": 5730
    },
    {
      "epoch": 0.28861625100563154,
      "grad_norm": 9.5,
      "learning_rate": 1.9422767497988738e-05,
      "loss": 0.9958,
      "step": 5740
    },
    {
      "epoch": 0.289119066773934,
      "grad_norm": 59.0,
      "learning_rate": 1.9421761866452134e-05,
      "loss": 1.1421,
      "step": 5750
    },
    {
      "epoch": 0.28962188254223653,
      "grad_norm": 52.5,
      "learning_rate": 1.9420756234915526e-05,
      "loss": 1.0733,
      "step": 5760
    },
    {
      "epoch": 0.290124698310539,
      "grad_norm": 25.625,
      "learning_rate": 1.9419750603378922e-05,
      "loss": 1.0999,
      "step": 5770
    },
    {
      "epoch": 0.2906275140788415,
      "grad_norm": 20.625,
      "learning_rate": 1.9418744971842318e-05,
      "loss": 1.2945,
      "step": 5780
    },
    {
      "epoch": 0.291130329847144,
      "grad_norm": 29.125,
      "learning_rate": 1.9417739340305714e-05,
      "loss": 1.2831,
      "step": 5790
    },
    {
      "epoch": 0.2916331456154465,
      "grad_norm": 35.0,
      "learning_rate": 1.941673370876911e-05,
      "loss": 0.8187,
      "step": 5800
    },
    {
      "epoch": 0.292135961383749,
      "grad_norm": 7.09375,
      "learning_rate": 1.9415728077232502e-05,
      "loss": 0.9297,
      "step": 5810
    },
    {
      "epoch": 0.2926387771520515,
      "grad_norm": 18.5,
      "learning_rate": 1.94147224456959e-05,
      "loss": 1.0773,
      "step": 5820
    },
    {
      "epoch": 0.29314159292035397,
      "grad_norm": 30.75,
      "learning_rate": 1.9413716814159294e-05,
      "loss": 1.1129,
      "step": 5830
    },
    {
      "epoch": 0.2936444086886565,
      "grad_norm": 14.5,
      "learning_rate": 1.9412711182622687e-05,
      "loss": 1.1675,
      "step": 5840
    },
    {
      "epoch": 0.29414722445695896,
      "grad_norm": 40.25,
      "learning_rate": 1.9411705551086083e-05,
      "loss": 0.9529,
      "step": 5850
    },
    {
      "epoch": 0.2946500402252615,
      "grad_norm": 23.375,
      "learning_rate": 1.941069991954948e-05,
      "loss": 1.122,
      "step": 5860
    },
    {
      "epoch": 0.29515285599356395,
      "grad_norm": 25.5,
      "learning_rate": 1.9409694288012874e-05,
      "loss": 0.8783,
      "step": 5870
    },
    {
      "epoch": 0.29565567176186647,
      "grad_norm": 18.125,
      "learning_rate": 1.940868865647627e-05,
      "loss": 1.1482,
      "step": 5880
    },
    {
      "epoch": 0.29615848753016893,
      "grad_norm": 9.6875,
      "learning_rate": 1.9407683024939663e-05,
      "loss": 1.1647,
      "step": 5890
    },
    {
      "epoch": 0.29666130329847146,
      "grad_norm": 18.875,
      "learning_rate": 1.940667739340306e-05,
      "loss": 1.0886,
      "step": 5900
    },
    {
      "epoch": 0.2971641190667739,
      "grad_norm": 27.375,
      "learning_rate": 1.9405671761866454e-05,
      "loss": 0.9522,
      "step": 5910
    },
    {
      "epoch": 0.29766693483507645,
      "grad_norm": 27.75,
      "learning_rate": 1.9404666130329847e-05,
      "loss": 1.321,
      "step": 5920
    },
    {
      "epoch": 0.2981697506033789,
      "grad_norm": 38.75,
      "learning_rate": 1.9403660498793243e-05,
      "loss": 0.8669,
      "step": 5930
    },
    {
      "epoch": 0.29867256637168144,
      "grad_norm": 56.0,
      "learning_rate": 1.940265486725664e-05,
      "loss": 1.1234,
      "step": 5940
    },
    {
      "epoch": 0.2991753821399839,
      "grad_norm": 15.5,
      "learning_rate": 1.9401649235720035e-05,
      "loss": 1.1081,
      "step": 5950
    },
    {
      "epoch": 0.2996781979082864,
      "grad_norm": 30.25,
      "learning_rate": 1.940064360418343e-05,
      "loss": 0.8737,
      "step": 5960
    },
    {
      "epoch": 0.3001810136765889,
      "grad_norm": 17.5,
      "learning_rate": 1.9399637972646823e-05,
      "loss": 0.9869,
      "step": 5970
    },
    {
      "epoch": 0.3006838294448914,
      "grad_norm": 33.0,
      "learning_rate": 1.939863234111022e-05,
      "loss": 1.0905,
      "step": 5980
    },
    {
      "epoch": 0.3011866452131939,
      "grad_norm": 11.5625,
      "learning_rate": 1.9397626709573615e-05,
      "loss": 0.9725,
      "step": 5990
    },
    {
      "epoch": 0.3016894609814964,
      "grad_norm": 57.0,
      "learning_rate": 1.9396621078037007e-05,
      "loss": 1.0491,
      "step": 6000
    },
    {
      "epoch": 0.3016894609814964,
      "eval_accuracy": 0.5098909271194844,
      "eval_loss": 1.0633625984191895,
      "eval_runtime": 465.0206,
      "eval_samples_per_second": 86.749,
      "eval_steps_per_second": 86.749,
      "step": 6000
    },
    {
      "epoch": 0.30219227674979887,
      "grad_norm": 28.25,
      "learning_rate": 1.9395615446500403e-05,
      "loss": 0.9322,
      "step": 6010
    },
    {
      "epoch": 0.3026950925181014,
      "grad_norm": 12.3125,
      "learning_rate": 1.93946098149638e-05,
      "loss": 0.8184,
      "step": 6020
    },
    {
      "epoch": 0.30319790828640386,
      "grad_norm": 19.625,
      "learning_rate": 1.9393604183427195e-05,
      "loss": 1.1346,
      "step": 6030
    },
    {
      "epoch": 0.30370072405470633,
      "grad_norm": 6.53125,
      "learning_rate": 1.9392598551890587e-05,
      "loss": 0.8971,
      "step": 6040
    },
    {
      "epoch": 0.30420353982300885,
      "grad_norm": 16.875,
      "learning_rate": 1.9391592920353983e-05,
      "loss": 0.8224,
      "step": 6050
    },
    {
      "epoch": 0.3047063555913113,
      "grad_norm": 9.6875,
      "learning_rate": 1.939058728881738e-05,
      "loss": 0.9422,
      "step": 6060
    },
    {
      "epoch": 0.30520917135961384,
      "grad_norm": 8.5625,
      "learning_rate": 1.9389581657280775e-05,
      "loss": 1.1874,
      "step": 6070
    },
    {
      "epoch": 0.3057119871279163,
      "grad_norm": 6.71875,
      "learning_rate": 1.9388576025744167e-05,
      "loss": 0.9917,
      "step": 6080
    },
    {
      "epoch": 0.30621480289621883,
      "grad_norm": 23.125,
      "learning_rate": 1.9387570394207563e-05,
      "loss": 0.8756,
      "step": 6090
    },
    {
      "epoch": 0.3067176186645213,
      "grad_norm": 7.21875,
      "learning_rate": 1.938656476267096e-05,
      "loss": 0.9201,
      "step": 6100
    },
    {
      "epoch": 0.3072204344328238,
      "grad_norm": 47.0,
      "learning_rate": 1.9385559131134355e-05,
      "loss": 0.9502,
      "step": 6110
    },
    {
      "epoch": 0.3077232502011263,
      "grad_norm": 4.625,
      "learning_rate": 1.9384553499597748e-05,
      "loss": 0.9943,
      "step": 6120
    },
    {
      "epoch": 0.3082260659694288,
      "grad_norm": 18.5,
      "learning_rate": 1.9383547868061143e-05,
      "loss": 1.2862,
      "step": 6130
    },
    {
      "epoch": 0.3087288817377313,
      "grad_norm": 40.5,
      "learning_rate": 1.938254223652454e-05,
      "loss": 1.1277,
      "step": 6140
    },
    {
      "epoch": 0.3092316975060338,
      "grad_norm": 57.5,
      "learning_rate": 1.9381536604987935e-05,
      "loss": 1.1259,
      "step": 6150
    },
    {
      "epoch": 0.30973451327433627,
      "grad_norm": 21.5,
      "learning_rate": 1.9380530973451328e-05,
      "loss": 0.8125,
      "step": 6160
    },
    {
      "epoch": 0.3102373290426388,
      "grad_norm": 5.03125,
      "learning_rate": 1.9379525341914724e-05,
      "loss": 0.6966,
      "step": 6170
    },
    {
      "epoch": 0.31074014481094125,
      "grad_norm": 4.8125,
      "learning_rate": 1.937851971037812e-05,
      "loss": 0.847,
      "step": 6180
    },
    {
      "epoch": 0.3112429605792438,
      "grad_norm": 9.5625,
      "learning_rate": 1.9377514078841515e-05,
      "loss": 1.0952,
      "step": 6190
    },
    {
      "epoch": 0.31174577634754624,
      "grad_norm": 18.875,
      "learning_rate": 1.9376508447304908e-05,
      "loss": 0.9307,
      "step": 6200
    },
    {
      "epoch": 0.31224859211584877,
      "grad_norm": 6.625,
      "learning_rate": 1.9375502815768304e-05,
      "loss": 0.9254,
      "step": 6210
    },
    {
      "epoch": 0.31275140788415123,
      "grad_norm": 23.875,
      "learning_rate": 1.93744971842317e-05,
      "loss": 1.2069,
      "step": 6220
    },
    {
      "epoch": 0.31325422365245376,
      "grad_norm": 65.0,
      "learning_rate": 1.9373491552695096e-05,
      "loss": 1.0764,
      "step": 6230
    },
    {
      "epoch": 0.3137570394207562,
      "grad_norm": 29.875,
      "learning_rate": 1.9372485921158488e-05,
      "loss": 1.1759,
      "step": 6240
    },
    {
      "epoch": 0.31425985518905875,
      "grad_norm": 32.75,
      "learning_rate": 1.9371480289621884e-05,
      "loss": 1.2464,
      "step": 6250
    },
    {
      "epoch": 0.3147626709573612,
      "grad_norm": 25.75,
      "learning_rate": 1.937047465808528e-05,
      "loss": 1.1105,
      "step": 6260
    },
    {
      "epoch": 0.31526548672566373,
      "grad_norm": 10.5,
      "learning_rate": 1.9369469026548676e-05,
      "loss": 1.0945,
      "step": 6270
    },
    {
      "epoch": 0.3157683024939662,
      "grad_norm": 15.0,
      "learning_rate": 1.9368463395012068e-05,
      "loss": 0.8642,
      "step": 6280
    },
    {
      "epoch": 0.3162711182622687,
      "grad_norm": 16.125,
      "learning_rate": 1.9367457763475464e-05,
      "loss": 1.229,
      "step": 6290
    },
    {
      "epoch": 0.3167739340305712,
      "grad_norm": 65.5,
      "learning_rate": 1.936645213193886e-05,
      "loss": 1.4241,
      "step": 6300
    },
    {
      "epoch": 0.3172767497988737,
      "grad_norm": 9.0625,
      "learning_rate": 1.9365446500402252e-05,
      "loss": 0.9,
      "step": 6310
    },
    {
      "epoch": 0.3177795655671762,
      "grad_norm": 16.875,
      "learning_rate": 1.9364440868865648e-05,
      "loss": 1.0545,
      "step": 6320
    },
    {
      "epoch": 0.3182823813354787,
      "grad_norm": 12.8125,
      "learning_rate": 1.9363435237329044e-05,
      "loss": 0.6815,
      "step": 6330
    },
    {
      "epoch": 0.31878519710378117,
      "grad_norm": 32.5,
      "learning_rate": 1.936242960579244e-05,
      "loss": 1.2467,
      "step": 6340
    },
    {
      "epoch": 0.3192880128720837,
      "grad_norm": 34.5,
      "learning_rate": 1.9361423974255836e-05,
      "loss": 1.2204,
      "step": 6350
    },
    {
      "epoch": 0.31979082864038616,
      "grad_norm": 54.0,
      "learning_rate": 1.936041834271923e-05,
      "loss": 1.23,
      "step": 6360
    },
    {
      "epoch": 0.3202936444086887,
      "grad_norm": 25.625,
      "learning_rate": 1.9359412711182624e-05,
      "loss": 1.0943,
      "step": 6370
    },
    {
      "epoch": 0.32079646017699115,
      "grad_norm": 19.5,
      "learning_rate": 1.935840707964602e-05,
      "loss": 1.0874,
      "step": 6380
    },
    {
      "epoch": 0.32129927594529367,
      "grad_norm": 35.75,
      "learning_rate": 1.9357401448109413e-05,
      "loss": 1.1238,
      "step": 6390
    },
    {
      "epoch": 0.32180209171359614,
      "grad_norm": 7.3125,
      "learning_rate": 1.935639581657281e-05,
      "loss": 0.7692,
      "step": 6400
    },
    {
      "epoch": 0.3223049074818986,
      "grad_norm": 16.125,
      "learning_rate": 1.9355390185036204e-05,
      "loss": 0.8467,
      "step": 6410
    },
    {
      "epoch": 0.32280772325020113,
      "grad_norm": 47.75,
      "learning_rate": 1.93543845534996e-05,
      "loss": 1.0626,
      "step": 6420
    },
    {
      "epoch": 0.3233105390185036,
      "grad_norm": 21.375,
      "learning_rate": 1.9353378921962996e-05,
      "loss": 0.8573,
      "step": 6430
    },
    {
      "epoch": 0.3238133547868061,
      "grad_norm": 16.625,
      "learning_rate": 1.935237329042639e-05,
      "loss": 0.9439,
      "step": 6440
    },
    {
      "epoch": 0.3243161705551086,
      "grad_norm": 21.25,
      "learning_rate": 1.9351367658889785e-05,
      "loss": 0.9823,
      "step": 6450
    },
    {
      "epoch": 0.3248189863234111,
      "grad_norm": 10.75,
      "learning_rate": 1.935036202735318e-05,
      "loss": 1.2026,
      "step": 6460
    },
    {
      "epoch": 0.3253218020917136,
      "grad_norm": 21.625,
      "learning_rate": 1.9349356395816573e-05,
      "loss": 0.8784,
      "step": 6470
    },
    {
      "epoch": 0.3258246178600161,
      "grad_norm": 18.375,
      "learning_rate": 1.934835076427997e-05,
      "loss": 0.8289,
      "step": 6480
    },
    {
      "epoch": 0.32632743362831856,
      "grad_norm": 9.4375,
      "learning_rate": 1.9347345132743365e-05,
      "loss": 0.7673,
      "step": 6490
    },
    {
      "epoch": 0.3268302493966211,
      "grad_norm": 8.25,
      "learning_rate": 1.934633950120676e-05,
      "loss": 1.2352,
      "step": 6500
    },
    {
      "epoch": 0.3268302493966211,
      "eval_accuracy": 0.5103123450669311,
      "eval_loss": 1.0592018365859985,
      "eval_runtime": 465.1263,
      "eval_samples_per_second": 86.729,
      "eval_steps_per_second": 86.729,
      "step": 6500
    },
    {
      "epoch": 0.32733306516492355,
      "grad_norm": 29.25,
      "learning_rate": 1.9345333869670156e-05,
      "loss": 1.6512,
      "step": 6510
    },
    {
      "epoch": 0.3278358809332261,
      "grad_norm": 20.5,
      "learning_rate": 1.934432823813355e-05,
      "loss": 1.0621,
      "step": 6520
    },
    {
      "epoch": 0.32833869670152854,
      "grad_norm": 52.0,
      "learning_rate": 1.9343322606596945e-05,
      "loss": 1.0706,
      "step": 6530
    },
    {
      "epoch": 0.32884151246983107,
      "grad_norm": 61.5,
      "learning_rate": 1.934231697506034e-05,
      "loss": 1.037,
      "step": 6540
    },
    {
      "epoch": 0.32934432823813353,
      "grad_norm": 6.65625,
      "learning_rate": 1.9341311343523733e-05,
      "loss": 1.4612,
      "step": 6550
    },
    {
      "epoch": 0.32984714400643605,
      "grad_norm": 4.28125,
      "learning_rate": 1.934030571198713e-05,
      "loss": 1.2114,
      "step": 6560
    },
    {
      "epoch": 0.3303499597747385,
      "grad_norm": 51.25,
      "learning_rate": 1.9339300080450525e-05,
      "loss": 1.1068,
      "step": 6570
    },
    {
      "epoch": 0.33085277554304104,
      "grad_norm": 44.0,
      "learning_rate": 1.9338294448913917e-05,
      "loss": 1.0055,
      "step": 6580
    },
    {
      "epoch": 0.3313555913113435,
      "grad_norm": 22.125,
      "learning_rate": 1.9337288817377317e-05,
      "loss": 1.1777,
      "step": 6590
    },
    {
      "epoch": 0.33185840707964603,
      "grad_norm": 18.125,
      "learning_rate": 1.933628318584071e-05,
      "loss": 1.0778,
      "step": 6600
    },
    {
      "epoch": 0.3323612228479485,
      "grad_norm": 30.375,
      "learning_rate": 1.9335277554304105e-05,
      "loss": 1.153,
      "step": 6610
    },
    {
      "epoch": 0.332864038616251,
      "grad_norm": 11.0,
      "learning_rate": 1.93342719227675e-05,
      "loss": 0.6577,
      "step": 6620
    },
    {
      "epoch": 0.3333668543845535,
      "grad_norm": 34.0,
      "learning_rate": 1.9333266291230893e-05,
      "loss": 1.2735,
      "step": 6630
    },
    {
      "epoch": 0.333869670152856,
      "grad_norm": 32.5,
      "learning_rate": 1.933226065969429e-05,
      "loss": 1.0711,
      "step": 6640
    },
    {
      "epoch": 0.3343724859211585,
      "grad_norm": 37.25,
      "learning_rate": 1.9331255028157685e-05,
      "loss": 0.8636,
      "step": 6650
    },
    {
      "epoch": 0.334875301689461,
      "grad_norm": 26.125,
      "learning_rate": 1.9330249396621078e-05,
      "loss": 0.9447,
      "step": 6660
    },
    {
      "epoch": 0.33537811745776347,
      "grad_norm": 90.5,
      "learning_rate": 1.9329243765084477e-05,
      "loss": 1.0429,
      "step": 6670
    },
    {
      "epoch": 0.335880933226066,
      "grad_norm": 30.875,
      "learning_rate": 1.932823813354787e-05,
      "loss": 0.9965,
      "step": 6680
    },
    {
      "epoch": 0.33638374899436846,
      "grad_norm": 17.375,
      "learning_rate": 1.9327232502011265e-05,
      "loss": 1.1872,
      "step": 6690
    },
    {
      "epoch": 0.336886564762671,
      "grad_norm": 83.0,
      "learning_rate": 1.932622687047466e-05,
      "loss": 1.1232,
      "step": 6700
    },
    {
      "epoch": 0.33738938053097345,
      "grad_norm": 27.5,
      "learning_rate": 1.9325221238938054e-05,
      "loss": 0.7805,
      "step": 6710
    },
    {
      "epoch": 0.33789219629927597,
      "grad_norm": 12.125,
      "learning_rate": 1.932421560740145e-05,
      "loss": 1.1476,
      "step": 6720
    },
    {
      "epoch": 0.33839501206757844,
      "grad_norm": 53.0,
      "learning_rate": 1.9323209975864845e-05,
      "loss": 0.8124,
      "step": 6730
    },
    {
      "epoch": 0.33889782783588096,
      "grad_norm": 8.25,
      "learning_rate": 1.9322204344328238e-05,
      "loss": 0.7595,
      "step": 6740
    },
    {
      "epoch": 0.3394006436041834,
      "grad_norm": 12.5,
      "learning_rate": 1.9321198712791637e-05,
      "loss": 1.0598,
      "step": 6750
    },
    {
      "epoch": 0.3399034593724859,
      "grad_norm": 49.0,
      "learning_rate": 1.932019308125503e-05,
      "loss": 1.1691,
      "step": 6760
    },
    {
      "epoch": 0.3404062751407884,
      "grad_norm": 38.75,
      "learning_rate": 1.9319187449718426e-05,
      "loss": 0.9699,
      "step": 6770
    },
    {
      "epoch": 0.3409090909090909,
      "grad_norm": 31.875,
      "learning_rate": 1.931818181818182e-05,
      "loss": 1.1171,
      "step": 6780
    },
    {
      "epoch": 0.3414119066773934,
      "grad_norm": 25.125,
      "learning_rate": 1.9317176186645214e-05,
      "loss": 0.5776,
      "step": 6790
    },
    {
      "epoch": 0.3419147224456959,
      "grad_norm": 17.375,
      "learning_rate": 1.931617055510861e-05,
      "loss": 0.9779,
      "step": 6800
    },
    {
      "epoch": 0.3424175382139984,
      "grad_norm": 14.3125,
      "learning_rate": 1.9315164923572006e-05,
      "loss": 1.0571,
      "step": 6810
    },
    {
      "epoch": 0.34292035398230086,
      "grad_norm": 24.75,
      "learning_rate": 1.9314159292035398e-05,
      "loss": 0.9465,
      "step": 6820
    },
    {
      "epoch": 0.3434231697506034,
      "grad_norm": 7.15625,
      "learning_rate": 1.9313153660498794e-05,
      "loss": 0.9995,
      "step": 6830
    },
    {
      "epoch": 0.34392598551890585,
      "grad_norm": 26.875,
      "learning_rate": 1.931214802896219e-05,
      "loss": 1.1786,
      "step": 6840
    },
    {
      "epoch": 0.3444288012872084,
      "grad_norm": 6.09375,
      "learning_rate": 1.9311142397425582e-05,
      "loss": 0.754,
      "step": 6850
    },
    {
      "epoch": 0.34493161705551084,
      "grad_norm": 9.0,
      "learning_rate": 1.931013676588898e-05,
      "loss": 0.8876,
      "step": 6860
    },
    {
      "epoch": 0.34543443282381336,
      "grad_norm": 30.5,
      "learning_rate": 1.9309131134352374e-05,
      "loss": 1.1487,
      "step": 6870
    },
    {
      "epoch": 0.34593724859211583,
      "grad_norm": 43.5,
      "learning_rate": 1.930812550281577e-05,
      "loss": 1.1542,
      "step": 6880
    },
    {
      "epoch": 0.34644006436041835,
      "grad_norm": 26.5,
      "learning_rate": 1.9307119871279166e-05,
      "loss": 0.9956,
      "step": 6890
    },
    {
      "epoch": 0.3469428801287208,
      "grad_norm": 8.25,
      "learning_rate": 1.930611423974256e-05,
      "loss": 0.6942,
      "step": 6900
    },
    {
      "epoch": 0.34744569589702334,
      "grad_norm": 53.0,
      "learning_rate": 1.9305108608205954e-05,
      "loss": 1.3938,
      "step": 6910
    },
    {
      "epoch": 0.3479485116653258,
      "grad_norm": 22.25,
      "learning_rate": 1.930410297666935e-05,
      "loss": 0.9943,
      "step": 6920
    },
    {
      "epoch": 0.34845132743362833,
      "grad_norm": 21.0,
      "learning_rate": 1.9303097345132743e-05,
      "loss": 0.8032,
      "step": 6930
    },
    {
      "epoch": 0.3489541432019308,
      "grad_norm": 8.8125,
      "learning_rate": 1.9302091713596142e-05,
      "loss": 1.0394,
      "step": 6940
    },
    {
      "epoch": 0.3494569589702333,
      "grad_norm": 62.0,
      "learning_rate": 1.9301086082059534e-05,
      "loss": 0.9424,
      "step": 6950
    },
    {
      "epoch": 0.3499597747385358,
      "grad_norm": 31.375,
      "learning_rate": 1.930008045052293e-05,
      "loss": 1.0775,
      "step": 6960
    },
    {
      "epoch": 0.3504625905068383,
      "grad_norm": 10.4375,
      "learning_rate": 1.9299074818986326e-05,
      "loss": 0.6933,
      "step": 6970
    },
    {
      "epoch": 0.3509654062751408,
      "grad_norm": 9.625,
      "learning_rate": 1.929806918744972e-05,
      "loss": 0.8061,
      "step": 6980
    },
    {
      "epoch": 0.3514682220434433,
      "grad_norm": 20.125,
      "learning_rate": 1.9297063555913115e-05,
      "loss": 1.1225,
      "step": 6990
    },
    {
      "epoch": 0.35197103781174577,
      "grad_norm": 23.5,
      "learning_rate": 1.929605792437651e-05,
      "loss": 0.9743,
      "step": 7000
    },
    {
      "epoch": 0.35197103781174577,
      "eval_accuracy": 0.5102379771938522,
      "eval_loss": 1.0577977895736694,
      "eval_runtime": 464.4627,
      "eval_samples_per_second": 86.853,
      "eval_steps_per_second": 86.853,
      "step": 7000
    },
    {
      "epoch": 0.3524738535800483,
      "grad_norm": 47.75,
      "learning_rate": 1.9295052292839903e-05,
      "loss": 1.0009,
      "step": 7010
    },
    {
      "epoch": 0.35297666934835076,
      "grad_norm": 49.25,
      "learning_rate": 1.9294046661303302e-05,
      "loss": 0.9239,
      "step": 7020
    },
    {
      "epoch": 0.3534794851166533,
      "grad_norm": 22.875,
      "learning_rate": 1.9293041029766695e-05,
      "loss": 1.0584,
      "step": 7030
    },
    {
      "epoch": 0.35398230088495575,
      "grad_norm": 9.75,
      "learning_rate": 1.929203539823009e-05,
      "loss": 1.0942,
      "step": 7040
    },
    {
      "epoch": 0.35448511665325827,
      "grad_norm": 16.625,
      "learning_rate": 1.9291029766693486e-05,
      "loss": 0.8516,
      "step": 7050
    },
    {
      "epoch": 0.35498793242156074,
      "grad_norm": 33.5,
      "learning_rate": 1.929002413515688e-05,
      "loss": 1.2032,
      "step": 7060
    },
    {
      "epoch": 0.35549074818986326,
      "grad_norm": 54.5,
      "learning_rate": 1.9289018503620275e-05,
      "loss": 0.899,
      "step": 7070
    },
    {
      "epoch": 0.3559935639581657,
      "grad_norm": 8.375,
      "learning_rate": 1.928801287208367e-05,
      "loss": 1.1606,
      "step": 7080
    },
    {
      "epoch": 0.35649637972646825,
      "grad_norm": 12.6875,
      "learning_rate": 1.9287007240547063e-05,
      "loss": 1.0961,
      "step": 7090
    },
    {
      "epoch": 0.3569991954947707,
      "grad_norm": 24.5,
      "learning_rate": 1.928600160901046e-05,
      "loss": 0.9918,
      "step": 7100
    },
    {
      "epoch": 0.3575020112630732,
      "grad_norm": 18.75,
      "learning_rate": 1.9284995977473855e-05,
      "loss": 1.0949,
      "step": 7110
    },
    {
      "epoch": 0.3580048270313757,
      "grad_norm": 8.4375,
      "learning_rate": 1.928399034593725e-05,
      "loss": 1.0329,
      "step": 7120
    },
    {
      "epoch": 0.3585076427996782,
      "grad_norm": 16.25,
      "learning_rate": 1.9282984714400647e-05,
      "loss": 1.004,
      "step": 7130
    },
    {
      "epoch": 0.3590104585679807,
      "grad_norm": 21.0,
      "learning_rate": 1.928197908286404e-05,
      "loss": 0.9468,
      "step": 7140
    },
    {
      "epoch": 0.35951327433628316,
      "grad_norm": 30.75,
      "learning_rate": 1.9280973451327435e-05,
      "loss": 1.0689,
      "step": 7150
    },
    {
      "epoch": 0.3600160901045857,
      "grad_norm": 10.375,
      "learning_rate": 1.927996781979083e-05,
      "loss": 0.9706,
      "step": 7160
    },
    {
      "epoch": 0.36051890587288815,
      "grad_norm": 14.8125,
      "learning_rate": 1.9278962188254223e-05,
      "loss": 1.1856,
      "step": 7170
    },
    {
      "epoch": 0.3610217216411907,
      "grad_norm": 10.75,
      "learning_rate": 1.927795655671762e-05,
      "loss": 1.1321,
      "step": 7180
    },
    {
      "epoch": 0.36152453740949314,
      "grad_norm": 9.5,
      "learning_rate": 1.9276950925181015e-05,
      "loss": 0.8051,
      "step": 7190
    },
    {
      "epoch": 0.36202735317779566,
      "grad_norm": 10.9375,
      "learning_rate": 1.927594529364441e-05,
      "loss": 1.1006,
      "step": 7200
    },
    {
      "epoch": 0.36253016894609813,
      "grad_norm": 10.125,
      "learning_rate": 1.9274939662107807e-05,
      "loss": 1.0675,
      "step": 7210
    },
    {
      "epoch": 0.36303298471440065,
      "grad_norm": 51.5,
      "learning_rate": 1.92739340305712e-05,
      "loss": 0.897,
      "step": 7220
    },
    {
      "epoch": 0.3635358004827031,
      "grad_norm": 13.6875,
      "learning_rate": 1.9272928399034595e-05,
      "loss": 0.9654,
      "step": 7230
    },
    {
      "epoch": 0.36403861625100564,
      "grad_norm": 20.0,
      "learning_rate": 1.927192276749799e-05,
      "loss": 0.7575,
      "step": 7240
    },
    {
      "epoch": 0.3645414320193081,
      "grad_norm": 6.4375,
      "learning_rate": 1.9270917135961384e-05,
      "loss": 1.1458,
      "step": 7250
    },
    {
      "epoch": 0.36504424778761063,
      "grad_norm": 21.625,
      "learning_rate": 1.926991150442478e-05,
      "loss": 1.418,
      "step": 7260
    },
    {
      "epoch": 0.3655470635559131,
      "grad_norm": 27.75,
      "learning_rate": 1.9268905872888175e-05,
      "loss": 1.0138,
      "step": 7270
    },
    {
      "epoch": 0.3660498793242156,
      "grad_norm": 24.375,
      "learning_rate": 1.926790024135157e-05,
      "loss": 1.2269,
      "step": 7280
    },
    {
      "epoch": 0.3665526950925181,
      "grad_norm": 10.3125,
      "learning_rate": 1.9266894609814967e-05,
      "loss": 0.933,
      "step": 7290
    },
    {
      "epoch": 0.3670555108608206,
      "grad_norm": 22.0,
      "learning_rate": 1.926588897827836e-05,
      "loss": 1.0832,
      "step": 7300
    },
    {
      "epoch": 0.3675583266291231,
      "grad_norm": 7.8125,
      "learning_rate": 1.9264883346741756e-05,
      "loss": 0.9513,
      "step": 7310
    },
    {
      "epoch": 0.3680611423974256,
      "grad_norm": 24.5,
      "learning_rate": 1.926387771520515e-05,
      "loss": 1.1514,
      "step": 7320
    },
    {
      "epoch": 0.36856395816572807,
      "grad_norm": 89.0,
      "learning_rate": 1.9262872083668544e-05,
      "loss": 1.2868,
      "step": 7330
    },
    {
      "epoch": 0.3690667739340306,
      "grad_norm": 16.25,
      "learning_rate": 1.926186645213194e-05,
      "loss": 1.0162,
      "step": 7340
    },
    {
      "epoch": 0.36956958970233306,
      "grad_norm": 22.5,
      "learning_rate": 1.9260860820595336e-05,
      "loss": 0.9011,
      "step": 7350
    },
    {
      "epoch": 0.3700724054706356,
      "grad_norm": 40.0,
      "learning_rate": 1.925985518905873e-05,
      "loss": 0.9481,
      "step": 7360
    },
    {
      "epoch": 0.37057522123893805,
      "grad_norm": 86.0,
      "learning_rate": 1.9258849557522124e-05,
      "loss": 1.1046,
      "step": 7370
    },
    {
      "epoch": 0.37107803700724057,
      "grad_norm": 14.6875,
      "learning_rate": 1.925784392598552e-05,
      "loss": 0.8537,
      "step": 7380
    },
    {
      "epoch": 0.37158085277554304,
      "grad_norm": 11.6875,
      "learning_rate": 1.9256838294448916e-05,
      "loss": 1.0178,
      "step": 7390
    },
    {
      "epoch": 0.37208366854384556,
      "grad_norm": 11.9375,
      "learning_rate": 1.9255832662912312e-05,
      "loss": 1.2512,
      "step": 7400
    },
    {
      "epoch": 0.372586484312148,
      "grad_norm": 9.1875,
      "learning_rate": 1.9254827031375704e-05,
      "loss": 0.8969,
      "step": 7410
    },
    {
      "epoch": 0.37308930008045055,
      "grad_norm": 7.625,
      "learning_rate": 1.92538213998391e-05,
      "loss": 0.7851,
      "step": 7420
    },
    {
      "epoch": 0.373592115848753,
      "grad_norm": 22.0,
      "learning_rate": 1.9252815768302496e-05,
      "loss": 1.025,
      "step": 7430
    },
    {
      "epoch": 0.37409493161705554,
      "grad_norm": 22.0,
      "learning_rate": 1.9251810136765892e-05,
      "loss": 1.5303,
      "step": 7440
    },
    {
      "epoch": 0.374597747385358,
      "grad_norm": 21.5,
      "learning_rate": 1.9250804505229284e-05,
      "loss": 0.9467,
      "step": 7450
    },
    {
      "epoch": 0.37510056315366047,
      "grad_norm": 15.75,
      "learning_rate": 1.924979887369268e-05,
      "loss": 0.9006,
      "step": 7460
    },
    {
      "epoch": 0.375603378921963,
      "grad_norm": 37.75,
      "learning_rate": 1.9248793242156076e-05,
      "loss": 1.4195,
      "step": 7470
    },
    {
      "epoch": 0.37610619469026546,
      "grad_norm": 30.875,
      "learning_rate": 1.9247787610619472e-05,
      "loss": 0.9651,
      "step": 7480
    },
    {
      "epoch": 0.376609010458568,
      "grad_norm": 17.5,
      "learning_rate": 1.9246781979082864e-05,
      "loss": 1.0637,
      "step": 7490
    },
    {
      "epoch": 0.37711182622687045,
      "grad_norm": 8.5,
      "learning_rate": 1.924577634754626e-05,
      "loss": 0.8582,
      "step": 7500
    },
    {
      "epoch": 0.37711182622687045,
      "eval_accuracy": 0.5097174020823004,
      "eval_loss": 1.0517877340316772,
      "eval_runtime": 466.1231,
      "eval_samples_per_second": 86.544,
      "eval_steps_per_second": 86.544,
      "step": 7500
    },
    {
      "epoch": 0.377614641995173,
      "grad_norm": 7.40625,
      "learning_rate": 1.9244770716009656e-05,
      "loss": 1.2873,
      "step": 7510
    },
    {
      "epoch": 0.37811745776347544,
      "grad_norm": 20.75,
      "learning_rate": 1.9243765084473052e-05,
      "loss": 1.0061,
      "step": 7520
    },
    {
      "epoch": 0.37862027353177796,
      "grad_norm": 39.5,
      "learning_rate": 1.9242759452936445e-05,
      "loss": 1.1145,
      "step": 7530
    },
    {
      "epoch": 0.37912308930008043,
      "grad_norm": 43.25,
      "learning_rate": 1.924175382139984e-05,
      "loss": 0.8651,
      "step": 7540
    },
    {
      "epoch": 0.37962590506838295,
      "grad_norm": 25.0,
      "learning_rate": 1.9240748189863236e-05,
      "loss": 0.8312,
      "step": 7550
    },
    {
      "epoch": 0.3801287208366854,
      "grad_norm": 11.875,
      "learning_rate": 1.9239742558326632e-05,
      "loss": 1.1551,
      "step": 7560
    },
    {
      "epoch": 0.38063153660498794,
      "grad_norm": 37.25,
      "learning_rate": 1.9238736926790025e-05,
      "loss": 0.7049,
      "step": 7570
    },
    {
      "epoch": 0.3811343523732904,
      "grad_norm": 18.375,
      "learning_rate": 1.923773129525342e-05,
      "loss": 1.0855,
      "step": 7580
    },
    {
      "epoch": 0.38163716814159293,
      "grad_norm": 10.5625,
      "learning_rate": 1.9236725663716816e-05,
      "loss": 1.0569,
      "step": 7590
    },
    {
      "epoch": 0.3821399839098954,
      "grad_norm": 25.25,
      "learning_rate": 1.9235720032180212e-05,
      "loss": 1.1306,
      "step": 7600
    },
    {
      "epoch": 0.3826427996781979,
      "grad_norm": 10.875,
      "learning_rate": 1.9234714400643605e-05,
      "loss": 0.9195,
      "step": 7610
    },
    {
      "epoch": 0.3831456154465004,
      "grad_norm": 12.875,
      "learning_rate": 1.9233708769107e-05,
      "loss": 0.9547,
      "step": 7620
    },
    {
      "epoch": 0.3836484312148029,
      "grad_norm": 28.625,
      "learning_rate": 1.9232703137570397e-05,
      "loss": 1.0324,
      "step": 7630
    },
    {
      "epoch": 0.3841512469831054,
      "grad_norm": 5.78125,
      "learning_rate": 1.923169750603379e-05,
      "loss": 0.6095,
      "step": 7640
    },
    {
      "epoch": 0.3846540627514079,
      "grad_norm": 36.75,
      "learning_rate": 1.9230691874497185e-05,
      "loss": 0.9417,
      "step": 7650
    },
    {
      "epoch": 0.38515687851971037,
      "grad_norm": 21.5,
      "learning_rate": 1.922968624296058e-05,
      "loss": 1.4055,
      "step": 7660
    },
    {
      "epoch": 0.3856596942880129,
      "grad_norm": 55.75,
      "learning_rate": 1.9228680611423977e-05,
      "loss": 1.0479,
      "step": 7670
    },
    {
      "epoch": 0.38616251005631536,
      "grad_norm": 16.625,
      "learning_rate": 1.9227674979887373e-05,
      "loss": 0.9061,
      "step": 7680
    },
    {
      "epoch": 0.3866653258246179,
      "grad_norm": 31.625,
      "learning_rate": 1.9226669348350765e-05,
      "loss": 1.1156,
      "step": 7690
    },
    {
      "epoch": 0.38716814159292035,
      "grad_norm": 6.09375,
      "learning_rate": 1.922566371681416e-05,
      "loss": 1.0987,
      "step": 7700
    },
    {
      "epoch": 0.38767095736122287,
      "grad_norm": 18.5,
      "learning_rate": 1.9224658085277557e-05,
      "loss": 0.5755,
      "step": 7710
    },
    {
      "epoch": 0.38817377312952533,
      "grad_norm": 28.625,
      "learning_rate": 1.922365245374095e-05,
      "loss": 1.0541,
      "step": 7720
    },
    {
      "epoch": 0.38867658889782786,
      "grad_norm": 12.375,
      "learning_rate": 1.9222646822204345e-05,
      "loss": 0.8172,
      "step": 7730
    },
    {
      "epoch": 0.3891794046661303,
      "grad_norm": 50.25,
      "learning_rate": 1.922164119066774e-05,
      "loss": 1.1797,
      "step": 7740
    },
    {
      "epoch": 0.38968222043443285,
      "grad_norm": 16.0,
      "learning_rate": 1.9220635559131137e-05,
      "loss": 1.157,
      "step": 7750
    },
    {
      "epoch": 0.3901850362027353,
      "grad_norm": 12.6875,
      "learning_rate": 1.9219629927594533e-05,
      "loss": 0.9178,
      "step": 7760
    },
    {
      "epoch": 0.39068785197103784,
      "grad_norm": 19.875,
      "learning_rate": 1.9218624296057925e-05,
      "loss": 1.0154,
      "step": 7770
    },
    {
      "epoch": 0.3911906677393403,
      "grad_norm": 49.5,
      "learning_rate": 1.921761866452132e-05,
      "loss": 1.3162,
      "step": 7780
    },
    {
      "epoch": 0.3916934835076428,
      "grad_norm": 12.1875,
      "learning_rate": 1.9216613032984717e-05,
      "loss": 1.0808,
      "step": 7790
    },
    {
      "epoch": 0.3921962992759453,
      "grad_norm": 7.09375,
      "learning_rate": 1.921560740144811e-05,
      "loss": 1.0398,
      "step": 7800
    },
    {
      "epoch": 0.3926991150442478,
      "grad_norm": 41.5,
      "learning_rate": 1.9214601769911505e-05,
      "loss": 0.9079,
      "step": 7810
    },
    {
      "epoch": 0.3932019308125503,
      "grad_norm": 37.0,
      "learning_rate": 1.92135961383749e-05,
      "loss": 1.0544,
      "step": 7820
    },
    {
      "epoch": 0.39370474658085275,
      "grad_norm": 24.875,
      "learning_rate": 1.9212590506838297e-05,
      "loss": 1.2406,
      "step": 7830
    },
    {
      "epoch": 0.39420756234915527,
      "grad_norm": 26.25,
      "learning_rate": 1.9211584875301693e-05,
      "loss": 0.9638,
      "step": 7840
    },
    {
      "epoch": 0.39471037811745774,
      "grad_norm": 15.25,
      "learning_rate": 1.9210579243765086e-05,
      "loss": 1.0341,
      "step": 7850
    },
    {
      "epoch": 0.39521319388576026,
      "grad_norm": 12.5625,
      "learning_rate": 1.920957361222848e-05,
      "loss": 1.0407,
      "step": 7860
    },
    {
      "epoch": 0.39571600965406273,
      "grad_norm": 16.625,
      "learning_rate": 1.9208567980691877e-05,
      "loss": 0.8385,
      "step": 7870
    },
    {
      "epoch": 0.39621882542236525,
      "grad_norm": 34.5,
      "learning_rate": 1.920756234915527e-05,
      "loss": 0.9981,
      "step": 7880
    },
    {
      "epoch": 0.3967216411906677,
      "grad_norm": 7.5,
      "learning_rate": 1.9206556717618666e-05,
      "loss": 0.8401,
      "step": 7890
    },
    {
      "epoch": 0.39722445695897024,
      "grad_norm": 9.4375,
      "learning_rate": 1.920555108608206e-05,
      "loss": 1.1007,
      "step": 7900
    },
    {
      "epoch": 0.3977272727272727,
      "grad_norm": 19.75,
      "learning_rate": 1.9204545454545454e-05,
      "loss": 1.3505,
      "step": 7910
    },
    {
      "epoch": 0.39823008849557523,
      "grad_norm": 9.625,
      "learning_rate": 1.9203539823008853e-05,
      "loss": 1.5682,
      "step": 7920
    },
    {
      "epoch": 0.3987329042638777,
      "grad_norm": 12.5625,
      "learning_rate": 1.9202534191472246e-05,
      "loss": 0.9939,
      "step": 7930
    },
    {
      "epoch": 0.3992357200321802,
      "grad_norm": 16.25,
      "learning_rate": 1.9201528559935642e-05,
      "loss": 1.0358,
      "step": 7940
    },
    {
      "epoch": 0.3997385358004827,
      "grad_norm": 15.3125,
      "learning_rate": 1.9200522928399038e-05,
      "loss": 0.9366,
      "step": 7950
    },
    {
      "epoch": 0.4002413515687852,
      "grad_norm": 47.5,
      "learning_rate": 1.919951729686243e-05,
      "loss": 1.0754,
      "step": 7960
    },
    {
      "epoch": 0.4007441673370877,
      "grad_norm": 12.0,
      "learning_rate": 1.9198511665325826e-05,
      "loss": 0.9934,
      "step": 7970
    },
    {
      "epoch": 0.4012469831053902,
      "grad_norm": 26.125,
      "learning_rate": 1.9197506033789222e-05,
      "loss": 1.0099,
      "step": 7980
    },
    {
      "epoch": 0.40174979887369267,
      "grad_norm": 3.578125,
      "learning_rate": 1.9196500402252614e-05,
      "loss": 0.9386,
      "step": 7990
    },
    {
      "epoch": 0.4022526146419952,
      "grad_norm": 5.25,
      "learning_rate": 1.9195494770716014e-05,
      "loss": 0.9987,
      "step": 8000
    },
    {
      "epoch": 0.4022526146419952,
      "eval_accuracy": 0.5106346058502726,
      "eval_loss": 1.047487497329712,
      "eval_runtime": 466.1439,
      "eval_samples_per_second": 86.54,
      "eval_steps_per_second": 86.54,
      "step": 8000
    },
    {
      "epoch": 0.40275543041029765,
      "grad_norm": 11.0,
      "learning_rate": 1.9194489139179406e-05,
      "loss": 1.0605,
      "step": 8010
    },
    {
      "epoch": 0.4032582461786002,
      "grad_norm": 13.75,
      "learning_rate": 1.9193483507642802e-05,
      "loss": 1.0363,
      "step": 8020
    },
    {
      "epoch": 0.40376106194690264,
      "grad_norm": 22.0,
      "learning_rate": 1.9192477876106198e-05,
      "loss": 1.0794,
      "step": 8030
    },
    {
      "epoch": 0.40426387771520517,
      "grad_norm": 12.625,
      "learning_rate": 1.919147224456959e-05,
      "loss": 0.7906,
      "step": 8040
    },
    {
      "epoch": 0.40476669348350763,
      "grad_norm": 12.0625,
      "learning_rate": 1.9190466613032986e-05,
      "loss": 0.931,
      "step": 8050
    },
    {
      "epoch": 0.40526950925181016,
      "grad_norm": 22.0,
      "learning_rate": 1.9189460981496382e-05,
      "loss": 1.0059,
      "step": 8060
    },
    {
      "epoch": 0.4057723250201126,
      "grad_norm": 34.5,
      "learning_rate": 1.9188455349959775e-05,
      "loss": 1.0417,
      "step": 8070
    },
    {
      "epoch": 0.40627514078841515,
      "grad_norm": 21.25,
      "learning_rate": 1.9187449718423174e-05,
      "loss": 1.1063,
      "step": 8080
    },
    {
      "epoch": 0.4067779565567176,
      "grad_norm": 76.5,
      "learning_rate": 1.9186444086886566e-05,
      "loss": 1.2771,
      "step": 8090
    },
    {
      "epoch": 0.40728077232502014,
      "grad_norm": 15.9375,
      "learning_rate": 1.9185438455349962e-05,
      "loss": 1.1099,
      "step": 8100
    },
    {
      "epoch": 0.4077835880933226,
      "grad_norm": 20.75,
      "learning_rate": 1.9184432823813358e-05,
      "loss": 0.8827,
      "step": 8110
    },
    {
      "epoch": 0.4082864038616251,
      "grad_norm": 67.0,
      "learning_rate": 1.918342719227675e-05,
      "loss": 1.0049,
      "step": 8120
    },
    {
      "epoch": 0.4087892196299276,
      "grad_norm": 22.875,
      "learning_rate": 1.9182421560740146e-05,
      "loss": 1.0271,
      "step": 8130
    },
    {
      "epoch": 0.4092920353982301,
      "grad_norm": 10.5,
      "learning_rate": 1.9181415929203542e-05,
      "loss": 0.8669,
      "step": 8140
    },
    {
      "epoch": 0.4097948511665326,
      "grad_norm": 26.0,
      "learning_rate": 1.9180410297666935e-05,
      "loss": 1.3271,
      "step": 8150
    },
    {
      "epoch": 0.4102976669348351,
      "grad_norm": 16.375,
      "learning_rate": 1.917940466613033e-05,
      "loss": 0.8798,
      "step": 8160
    },
    {
      "epoch": 0.41080048270313757,
      "grad_norm": 27.875,
      "learning_rate": 1.9178399034593727e-05,
      "loss": 1.0289,
      "step": 8170
    },
    {
      "epoch": 0.41130329847144004,
      "grad_norm": 6.21875,
      "learning_rate": 1.917739340305712e-05,
      "loss": 0.9026,
      "step": 8180
    },
    {
      "epoch": 0.41180611423974256,
      "grad_norm": 23.75,
      "learning_rate": 1.917638777152052e-05,
      "loss": 1.0456,
      "step": 8190
    },
    {
      "epoch": 0.412308930008045,
      "grad_norm": 30.875,
      "learning_rate": 1.917538213998391e-05,
      "loss": 1.0572,
      "step": 8200
    },
    {
      "epoch": 0.41281174577634755,
      "grad_norm": 18.875,
      "learning_rate": 1.9174376508447307e-05,
      "loss": 0.9083,
      "step": 8210
    },
    {
      "epoch": 0.41331456154465,
      "grad_norm": 30.625,
      "learning_rate": 1.9173370876910703e-05,
      "loss": 0.9577,
      "step": 8220
    },
    {
      "epoch": 0.41381737731295254,
      "grad_norm": 5.40625,
      "learning_rate": 1.9172365245374095e-05,
      "loss": 0.9914,
      "step": 8230
    },
    {
      "epoch": 0.414320193081255,
      "grad_norm": 8.0625,
      "learning_rate": 1.917135961383749e-05,
      "loss": 1.0267,
      "step": 8240
    },
    {
      "epoch": 0.41482300884955753,
      "grad_norm": 18.25,
      "learning_rate": 1.9170353982300887e-05,
      "loss": 1.1447,
      "step": 8250
    },
    {
      "epoch": 0.41532582461786,
      "grad_norm": 16.875,
      "learning_rate": 1.916934835076428e-05,
      "loss": 0.9569,
      "step": 8260
    },
    {
      "epoch": 0.4158286403861625,
      "grad_norm": 18.25,
      "learning_rate": 1.916834271922768e-05,
      "loss": 0.7938,
      "step": 8270
    },
    {
      "epoch": 0.416331456154465,
      "grad_norm": 34.75,
      "learning_rate": 1.916733708769107e-05,
      "loss": 1.0018,
      "step": 8280
    },
    {
      "epoch": 0.4168342719227675,
      "grad_norm": 9.6875,
      "learning_rate": 1.9166331456154467e-05,
      "loss": 0.8144,
      "step": 8290
    },
    {
      "epoch": 0.41733708769107,
      "grad_norm": 10.375,
      "learning_rate": 1.9165325824617863e-05,
      "loss": 0.8868,
      "step": 8300
    },
    {
      "epoch": 0.4178399034593725,
      "grad_norm": 5.40625,
      "learning_rate": 1.9164320193081255e-05,
      "loss": 0.9223,
      "step": 8310
    },
    {
      "epoch": 0.41834271922767496,
      "grad_norm": 27.0,
      "learning_rate": 1.916331456154465e-05,
      "loss": 1.0824,
      "step": 8320
    },
    {
      "epoch": 0.4188455349959775,
      "grad_norm": 24.5,
      "learning_rate": 1.9162308930008047e-05,
      "loss": 1.0215,
      "step": 8330
    },
    {
      "epoch": 0.41934835076427995,
      "grad_norm": 9.0,
      "learning_rate": 1.916130329847144e-05,
      "loss": 0.9315,
      "step": 8340
    },
    {
      "epoch": 0.4198511665325825,
      "grad_norm": 31.5,
      "learning_rate": 1.916029766693484e-05,
      "loss": 1.4123,
      "step": 8350
    },
    {
      "epoch": 0.42035398230088494,
      "grad_norm": 6.78125,
      "learning_rate": 1.915929203539823e-05,
      "loss": 0.9441,
      "step": 8360
    },
    {
      "epoch": 0.42085679806918747,
      "grad_norm": 9.625,
      "learning_rate": 1.9158286403861627e-05,
      "loss": 0.8382,
      "step": 8370
    },
    {
      "epoch": 0.42135961383748993,
      "grad_norm": 45.75,
      "learning_rate": 1.9157280772325023e-05,
      "loss": 1.0135,
      "step": 8380
    },
    {
      "epoch": 0.42186242960579245,
      "grad_norm": 3.609375,
      "learning_rate": 1.9156275140788416e-05,
      "loss": 1.0677,
      "step": 8390
    },
    {
      "epoch": 0.4223652453740949,
      "grad_norm": 65.5,
      "learning_rate": 1.915526950925181e-05,
      "loss": 1.0144,
      "step": 8400
    },
    {
      "epoch": 0.42286806114239744,
      "grad_norm": 51.25,
      "learning_rate": 1.9154263877715207e-05,
      "loss": 1.6016,
      "step": 8410
    },
    {
      "epoch": 0.4233708769106999,
      "grad_norm": 8.875,
      "learning_rate": 1.91532582461786e-05,
      "loss": 1.062,
      "step": 8420
    },
    {
      "epoch": 0.42387369267900243,
      "grad_norm": 7.0,
      "learning_rate": 1.9152252614641996e-05,
      "loss": 0.8623,
      "step": 8430
    },
    {
      "epoch": 0.4243765084473049,
      "grad_norm": 35.25,
      "learning_rate": 1.915124698310539e-05,
      "loss": 1.0106,
      "step": 8440
    },
    {
      "epoch": 0.4248793242156074,
      "grad_norm": 36.5,
      "learning_rate": 1.9150241351568784e-05,
      "loss": 0.9767,
      "step": 8450
    },
    {
      "epoch": 0.4253821399839099,
      "grad_norm": 36.5,
      "learning_rate": 1.9149235720032183e-05,
      "loss": 1.1468,
      "step": 8460
    },
    {
      "epoch": 0.4258849557522124,
      "grad_norm": 23.375,
      "learning_rate": 1.9148230088495576e-05,
      "loss": 1.1806,
      "step": 8470
    },
    {
      "epoch": 0.4263877715205149,
      "grad_norm": 29.0,
      "learning_rate": 1.9147224456958972e-05,
      "loss": 0.8954,
      "step": 8480
    },
    {
      "epoch": 0.4268905872888174,
      "grad_norm": 31.125,
      "learning_rate": 1.9146218825422368e-05,
      "loss": 0.9266,
      "step": 8490
    },
    {
      "epoch": 0.42739340305711987,
      "grad_norm": 17.5,
      "learning_rate": 1.914521319388576e-05,
      "loss": 1.126,
      "step": 8500
    },
    {
      "epoch": 0.42739340305711987,
      "eval_accuracy": 0.5114278631631135,
      "eval_loss": 1.0403167009353638,
      "eval_runtime": 466.2786,
      "eval_samples_per_second": 86.515,
      "eval_steps_per_second": 86.515,
      "step": 8500
    },
    {
      "epoch": 0.4278962188254224,
      "grad_norm": 32.25,
      "learning_rate": 1.9144207562349156e-05,
      "loss": 0.9443,
      "step": 8510
    },
    {
      "epoch": 0.42839903459372486,
      "grad_norm": 47.5,
      "learning_rate": 1.9143201930812552e-05,
      "loss": 0.9892,
      "step": 8520
    },
    {
      "epoch": 0.4289018503620273,
      "grad_norm": 59.5,
      "learning_rate": 1.9142196299275944e-05,
      "loss": 1.1795,
      "step": 8530
    },
    {
      "epoch": 0.42940466613032985,
      "grad_norm": 41.75,
      "learning_rate": 1.9141190667739344e-05,
      "loss": 1.228,
      "step": 8540
    },
    {
      "epoch": 0.4299074818986323,
      "grad_norm": 9.125,
      "learning_rate": 1.9140185036202736e-05,
      "loss": 0.9201,
      "step": 8550
    },
    {
      "epoch": 0.43041029766693484,
      "grad_norm": 36.5,
      "learning_rate": 1.9139179404666132e-05,
      "loss": 0.8624,
      "step": 8560
    },
    {
      "epoch": 0.4309131134352373,
      "grad_norm": 12.8125,
      "learning_rate": 1.9138173773129528e-05,
      "loss": 1.1639,
      "step": 8570
    },
    {
      "epoch": 0.4314159292035398,
      "grad_norm": 48.5,
      "learning_rate": 1.913716814159292e-05,
      "loss": 0.8888,
      "step": 8580
    },
    {
      "epoch": 0.4319187449718423,
      "grad_norm": 23.125,
      "learning_rate": 1.9136162510056316e-05,
      "loss": 0.9679,
      "step": 8590
    },
    {
      "epoch": 0.4324215607401448,
      "grad_norm": 62.75,
      "learning_rate": 1.9135156878519712e-05,
      "loss": 0.9227,
      "step": 8600
    },
    {
      "epoch": 0.4329243765084473,
      "grad_norm": 15.625,
      "learning_rate": 1.9134151246983105e-05,
      "loss": 1.0606,
      "step": 8610
    },
    {
      "epoch": 0.4334271922767498,
      "grad_norm": 14.9375,
      "learning_rate": 1.9133145615446504e-05,
      "loss": 1.3711,
      "step": 8620
    },
    {
      "epoch": 0.4339300080450523,
      "grad_norm": 23.75,
      "learning_rate": 1.9132139983909896e-05,
      "loss": 1.304,
      "step": 8630
    },
    {
      "epoch": 0.4344328238133548,
      "grad_norm": 10.0625,
      "learning_rate": 1.9131134352373292e-05,
      "loss": 1.2273,
      "step": 8640
    },
    {
      "epoch": 0.43493563958165726,
      "grad_norm": 20.25,
      "learning_rate": 1.9130128720836688e-05,
      "loss": 1.1622,
      "step": 8650
    },
    {
      "epoch": 0.4354384553499598,
      "grad_norm": 13.0,
      "learning_rate": 1.912912308930008e-05,
      "loss": 0.9036,
      "step": 8660
    },
    {
      "epoch": 0.43594127111826225,
      "grad_norm": 17.75,
      "learning_rate": 1.9128117457763477e-05,
      "loss": 0.9222,
      "step": 8670
    },
    {
      "epoch": 0.4364440868865648,
      "grad_norm": 24.25,
      "learning_rate": 1.9127111826226872e-05,
      "loss": 0.6133,
      "step": 8680
    },
    {
      "epoch": 0.43694690265486724,
      "grad_norm": 6.28125,
      "learning_rate": 1.9126106194690265e-05,
      "loss": 0.8539,
      "step": 8690
    },
    {
      "epoch": 0.43744971842316976,
      "grad_norm": 21.375,
      "learning_rate": 1.912510056315366e-05,
      "loss": 1.1073,
      "step": 8700
    },
    {
      "epoch": 0.43795253419147223,
      "grad_norm": 28.75,
      "learning_rate": 1.9124094931617057e-05,
      "loss": 1.1409,
      "step": 8710
    },
    {
      "epoch": 0.43845534995977475,
      "grad_norm": 30.5,
      "learning_rate": 1.9123089300080453e-05,
      "loss": 0.989,
      "step": 8720
    },
    {
      "epoch": 0.4389581657280772,
      "grad_norm": 13.3125,
      "learning_rate": 1.912208366854385e-05,
      "loss": 1.1508,
      "step": 8730
    },
    {
      "epoch": 0.43946098149637974,
      "grad_norm": 29.625,
      "learning_rate": 1.912107803700724e-05,
      "loss": 1.114,
      "step": 8740
    },
    {
      "epoch": 0.4399637972646822,
      "grad_norm": 16.625,
      "learning_rate": 1.9120072405470637e-05,
      "loss": 0.8936,
      "step": 8750
    },
    {
      "epoch": 0.44046661303298473,
      "grad_norm": 15.375,
      "learning_rate": 1.9119066773934033e-05,
      "loss": 0.9774,
      "step": 8760
    },
    {
      "epoch": 0.4409694288012872,
      "grad_norm": 63.0,
      "learning_rate": 1.9118061142397425e-05,
      "loss": 1.1199,
      "step": 8770
    },
    {
      "epoch": 0.4414722445695897,
      "grad_norm": 58.75,
      "learning_rate": 1.911705551086082e-05,
      "loss": 0.8529,
      "step": 8780
    },
    {
      "epoch": 0.4419750603378922,
      "grad_norm": 49.75,
      "learning_rate": 1.9116049879324217e-05,
      "loss": 1.2643,
      "step": 8790
    },
    {
      "epoch": 0.4424778761061947,
      "grad_norm": 13.5,
      "learning_rate": 1.9115044247787613e-05,
      "loss": 1.2814,
      "step": 8800
    },
    {
      "epoch": 0.4429806918744972,
      "grad_norm": 13.4375,
      "learning_rate": 1.911403861625101e-05,
      "loss": 1.1046,
      "step": 8810
    },
    {
      "epoch": 0.4434835076427997,
      "grad_norm": 5.71875,
      "learning_rate": 1.91130329847144e-05,
      "loss": 0.9602,
      "step": 8820
    },
    {
      "epoch": 0.44398632341110217,
      "grad_norm": 9.25,
      "learning_rate": 1.9112027353177797e-05,
      "loss": 0.7677,
      "step": 8830
    },
    {
      "epoch": 0.4444891391794047,
      "grad_norm": 7.5625,
      "learning_rate": 1.9111021721641193e-05,
      "loss": 0.9796,
      "step": 8840
    },
    {
      "epoch": 0.44499195494770716,
      "grad_norm": 48.75,
      "learning_rate": 1.9110016090104585e-05,
      "loss": 1.1555,
      "step": 8850
    },
    {
      "epoch": 0.4454947707160097,
      "grad_norm": 6.8125,
      "learning_rate": 1.910901045856798e-05,
      "loss": 0.8604,
      "step": 8860
    },
    {
      "epoch": 0.44599758648431215,
      "grad_norm": 42.25,
      "learning_rate": 1.9108004827031377e-05,
      "loss": 1.484,
      "step": 8870
    },
    {
      "epoch": 0.4465004022526146,
      "grad_norm": 10.3125,
      "learning_rate": 1.9106999195494773e-05,
      "loss": 0.7972,
      "step": 8880
    },
    {
      "epoch": 0.44700321802091714,
      "grad_norm": 54.5,
      "learning_rate": 1.910599356395817e-05,
      "loss": 0.8643,
      "step": 8890
    },
    {
      "epoch": 0.4475060337892196,
      "grad_norm": 6.59375,
      "learning_rate": 1.910498793242156e-05,
      "loss": 1.2997,
      "step": 8900
    },
    {
      "epoch": 0.4480088495575221,
      "grad_norm": 10.125,
      "learning_rate": 1.9103982300884957e-05,
      "loss": 1.0439,
      "step": 8910
    },
    {
      "epoch": 0.4485116653258246,
      "grad_norm": 13.4375,
      "learning_rate": 1.9102976669348353e-05,
      "loss": 0.929,
      "step": 8920
    },
    {
      "epoch": 0.4490144810941271,
      "grad_norm": 9.1875,
      "learning_rate": 1.910197103781175e-05,
      "loss": 1.2822,
      "step": 8930
    },
    {
      "epoch": 0.4495172968624296,
      "grad_norm": 10.9375,
      "learning_rate": 1.910096540627514e-05,
      "loss": 0.9477,
      "step": 8940
    },
    {
      "epoch": 0.4500201126307321,
      "grad_norm": 22.375,
      "learning_rate": 1.9099959774738537e-05,
      "loss": 1.0526,
      "step": 8950
    },
    {
      "epoch": 0.4505229283990346,
      "grad_norm": 17.5,
      "learning_rate": 1.9098954143201933e-05,
      "loss": 0.9101,
      "step": 8960
    },
    {
      "epoch": 0.4510257441673371,
      "grad_norm": 11.0,
      "learning_rate": 1.9097948511665326e-05,
      "loss": 0.9082,
      "step": 8970
    },
    {
      "epoch": 0.45152855993563956,
      "grad_norm": 11.6875,
      "learning_rate": 1.909694288012872e-05,
      "loss": 1.013,
      "step": 8980
    },
    {
      "epoch": 0.4520313757039421,
      "grad_norm": 35.5,
      "learning_rate": 1.9095937248592118e-05,
      "loss": 0.7793,
      "step": 8990
    },
    {
      "epoch": 0.45253419147224455,
      "grad_norm": 21.5,
      "learning_rate": 1.9094931617055513e-05,
      "loss": 1.3949,
      "step": 9000
    },
    {
      "epoch": 0.45253419147224455,
      "eval_accuracy": 0.5107833415964304,
      "eval_loss": 1.0375735759735107,
      "eval_runtime": 465.7876,
      "eval_samples_per_second": 86.606,
      "eval_steps_per_second": 86.606,
      "step": 9000
    },
    {
      "epoch": 0.4530370072405471,
      "grad_norm": 36.0,
      "learning_rate": 1.909392598551891e-05,
      "loss": 1.1965,
      "step": 9010
    },
    {
      "epoch": 0.45353982300884954,
      "grad_norm": 35.75,
      "learning_rate": 1.9092920353982302e-05,
      "loss": 0.9698,
      "step": 9020
    },
    {
      "epoch": 0.45404263877715206,
      "grad_norm": 9.9375,
      "learning_rate": 1.9091914722445698e-05,
      "loss": 1.0362,
      "step": 9030
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 42.5,
      "learning_rate": 1.9090909090909094e-05,
      "loss": 1.1106,
      "step": 9040
    },
    {
      "epoch": 0.45504827031375705,
      "grad_norm": 8.4375,
      "learning_rate": 1.9089903459372486e-05,
      "loss": 1.1679,
      "step": 9050
    },
    {
      "epoch": 0.4555510860820595,
      "grad_norm": 47.5,
      "learning_rate": 1.9088897827835882e-05,
      "loss": 1.0969,
      "step": 9060
    },
    {
      "epoch": 0.45605390185036204,
      "grad_norm": 14.3125,
      "learning_rate": 1.9087892196299278e-05,
      "loss": 1.2004,
      "step": 9070
    },
    {
      "epoch": 0.4565567176186645,
      "grad_norm": 25.25,
      "learning_rate": 1.9086886564762674e-05,
      "loss": 1.2328,
      "step": 9080
    },
    {
      "epoch": 0.45705953338696703,
      "grad_norm": 13.4375,
      "learning_rate": 1.908588093322607e-05,
      "loss": 0.9409,
      "step": 9090
    },
    {
      "epoch": 0.4575623491552695,
      "grad_norm": 34.25,
      "learning_rate": 1.9084875301689462e-05,
      "loss": 0.9461,
      "step": 9100
    },
    {
      "epoch": 0.458065164923572,
      "grad_norm": 62.0,
      "learning_rate": 1.9083869670152858e-05,
      "loss": 1.2634,
      "step": 9110
    },
    {
      "epoch": 0.4585679806918745,
      "grad_norm": 33.75,
      "learning_rate": 1.9082864038616254e-05,
      "loss": 0.9378,
      "step": 9120
    },
    {
      "epoch": 0.459070796460177,
      "grad_norm": 24.25,
      "learning_rate": 1.9081858407079646e-05,
      "loss": 0.7848,
      "step": 9130
    },
    {
      "epoch": 0.4595736122284795,
      "grad_norm": 6.6875,
      "learning_rate": 1.9080852775543042e-05,
      "loss": 0.9517,
      "step": 9140
    },
    {
      "epoch": 0.460076427996782,
      "grad_norm": 5.84375,
      "learning_rate": 1.9079847144006438e-05,
      "loss": 1.0956,
      "step": 9150
    },
    {
      "epoch": 0.46057924376508447,
      "grad_norm": 21.75,
      "learning_rate": 1.9078841512469834e-05,
      "loss": 0.8775,
      "step": 9160
    },
    {
      "epoch": 0.461082059533387,
      "grad_norm": 41.25,
      "learning_rate": 1.907783588093323e-05,
      "loss": 0.8674,
      "step": 9170
    },
    {
      "epoch": 0.46158487530168946,
      "grad_norm": 19.375,
      "learning_rate": 1.9076830249396622e-05,
      "loss": 1.0514,
      "step": 9180
    },
    {
      "epoch": 0.462087691069992,
      "grad_norm": 19.5,
      "learning_rate": 1.9075824617860018e-05,
      "loss": 0.9587,
      "step": 9190
    },
    {
      "epoch": 0.46259050683829445,
      "grad_norm": 38.75,
      "learning_rate": 1.9074818986323414e-05,
      "loss": 1.097,
      "step": 9200
    },
    {
      "epoch": 0.46309332260659697,
      "grad_norm": 73.0,
      "learning_rate": 1.9073813354786807e-05,
      "loss": 1.2164,
      "step": 9210
    },
    {
      "epoch": 0.46359613837489944,
      "grad_norm": 33.0,
      "learning_rate": 1.9072807723250202e-05,
      "loss": 1.4168,
      "step": 9220
    },
    {
      "epoch": 0.46409895414320196,
      "grad_norm": 11.9375,
      "learning_rate": 1.90718020917136e-05,
      "loss": 0.9627,
      "step": 9230
    },
    {
      "epoch": 0.4646017699115044,
      "grad_norm": 18.625,
      "learning_rate": 1.907079646017699e-05,
      "loss": 0.8799,
      "step": 9240
    },
    {
      "epoch": 0.4651045856798069,
      "grad_norm": 15.8125,
      "learning_rate": 1.906979082864039e-05,
      "loss": 1.0375,
      "step": 9250
    },
    {
      "epoch": 0.4656074014481094,
      "grad_norm": 4.90625,
      "learning_rate": 1.9068785197103783e-05,
      "loss": 0.8948,
      "step": 9260
    },
    {
      "epoch": 0.4661102172164119,
      "grad_norm": 17.625,
      "learning_rate": 1.906777956556718e-05,
      "loss": 1.2458,
      "step": 9270
    },
    {
      "epoch": 0.4666130329847144,
      "grad_norm": 51.0,
      "learning_rate": 1.9066773934030574e-05,
      "loss": 0.9732,
      "step": 9280
    },
    {
      "epoch": 0.46711584875301687,
      "grad_norm": 14.5,
      "learning_rate": 1.9065768302493967e-05,
      "loss": 1.1307,
      "step": 9290
    },
    {
      "epoch": 0.4676186645213194,
      "grad_norm": 13.6875,
      "learning_rate": 1.9064762670957363e-05,
      "loss": 0.967,
      "step": 9300
    },
    {
      "epoch": 0.46812148028962186,
      "grad_norm": 32.25,
      "learning_rate": 1.906375703942076e-05,
      "loss": 1.0604,
      "step": 9310
    },
    {
      "epoch": 0.4686242960579244,
      "grad_norm": 18.25,
      "learning_rate": 1.906275140788415e-05,
      "loss": 0.8459,
      "step": 9320
    },
    {
      "epoch": 0.46912711182622685,
      "grad_norm": 22.0,
      "learning_rate": 1.906174577634755e-05,
      "loss": 1.0903,
      "step": 9330
    },
    {
      "epoch": 0.4696299275945294,
      "grad_norm": 48.75,
      "learning_rate": 1.9060740144810943e-05,
      "loss": 1.1277,
      "step": 9340
    },
    {
      "epoch": 0.47013274336283184,
      "grad_norm": 40.0,
      "learning_rate": 1.905973451327434e-05,
      "loss": 0.9395,
      "step": 9350
    },
    {
      "epoch": 0.47063555913113436,
      "grad_norm": 6.21875,
      "learning_rate": 1.9058728881737735e-05,
      "loss": 0.93,
      "step": 9360
    },
    {
      "epoch": 0.47113837489943683,
      "grad_norm": 29.25,
      "learning_rate": 1.9057723250201127e-05,
      "loss": 1.3166,
      "step": 9370
    },
    {
      "epoch": 0.47164119066773935,
      "grad_norm": 34.0,
      "learning_rate": 1.9056717618664523e-05,
      "loss": 0.9168,
      "step": 9380
    },
    {
      "epoch": 0.4721440064360418,
      "grad_norm": 44.0,
      "learning_rate": 1.905571198712792e-05,
      "loss": 1.0779,
      "step": 9390
    },
    {
      "epoch": 0.47264682220434434,
      "grad_norm": 41.25,
      "learning_rate": 1.905470635559131e-05,
      "loss": 0.8738,
      "step": 9400
    },
    {
      "epoch": 0.4731496379726468,
      "grad_norm": 27.125,
      "learning_rate": 1.905370072405471e-05,
      "loss": 0.9966,
      "step": 9410
    },
    {
      "epoch": 0.47365245374094933,
      "grad_norm": 61.5,
      "learning_rate": 1.9052695092518103e-05,
      "loss": 1.3641,
      "step": 9420
    },
    {
      "epoch": 0.4741552695092518,
      "grad_norm": 13.6875,
      "learning_rate": 1.90516894609815e-05,
      "loss": 0.9884,
      "step": 9430
    },
    {
      "epoch": 0.4746580852775543,
      "grad_norm": 10.4375,
      "learning_rate": 1.9050683829444895e-05,
      "loss": 1.0463,
      "step": 9440
    },
    {
      "epoch": 0.4751609010458568,
      "grad_norm": 4.9375,
      "learning_rate": 1.9049678197908287e-05,
      "loss": 0.8838,
      "step": 9450
    },
    {
      "epoch": 0.4756637168141593,
      "grad_norm": 51.0,
      "learning_rate": 1.9048672566371683e-05,
      "loss": 0.8219,
      "step": 9460
    },
    {
      "epoch": 0.4761665325824618,
      "grad_norm": 42.25,
      "learning_rate": 1.904766693483508e-05,
      "loss": 1.2924,
      "step": 9470
    },
    {
      "epoch": 0.4766693483507643,
      "grad_norm": 17.875,
      "learning_rate": 1.904666130329847e-05,
      "loss": 0.7876,
      "step": 9480
    },
    {
      "epoch": 0.47717216411906677,
      "grad_norm": 21.0,
      "learning_rate": 1.9045655671761867e-05,
      "loss": 1.0719,
      "step": 9490
    },
    {
      "epoch": 0.4776749798873693,
      "grad_norm": 8.25,
      "learning_rate": 1.9044650040225263e-05,
      "loss": 1.0151,
      "step": 9500
    },
    {
      "epoch": 0.4776749798873693,
      "eval_accuracy": 0.5104610808130887,
      "eval_loss": 1.034855604171753,
      "eval_runtime": 466.3895,
      "eval_samples_per_second": 86.494,
      "eval_steps_per_second": 86.494,
      "step": 9500
    },
    {
      "epoch": 0.47817779565567176,
      "grad_norm": 51.0,
      "learning_rate": 1.9043644408688656e-05,
      "loss": 1.219,
      "step": 9510
    },
    {
      "epoch": 0.4786806114239743,
      "grad_norm": 9.25,
      "learning_rate": 1.9042638777152055e-05,
      "loss": 1.0742,
      "step": 9520
    },
    {
      "epoch": 0.47918342719227675,
      "grad_norm": 29.25,
      "learning_rate": 1.9041633145615448e-05,
      "loss": 0.8912,
      "step": 9530
    },
    {
      "epoch": 0.47968624296057927,
      "grad_norm": 17.25,
      "learning_rate": 1.9040627514078843e-05,
      "loss": 1.0665,
      "step": 9540
    },
    {
      "epoch": 0.48018905872888173,
      "grad_norm": 28.375,
      "learning_rate": 1.903962188254224e-05,
      "loss": 1.1278,
      "step": 9550
    },
    {
      "epoch": 0.48069187449718426,
      "grad_norm": 10.25,
      "learning_rate": 1.9038616251005632e-05,
      "loss": 1.1241,
      "step": 9560
    },
    {
      "epoch": 0.4811946902654867,
      "grad_norm": 29.25,
      "learning_rate": 1.9037610619469028e-05,
      "loss": 0.9313,
      "step": 9570
    },
    {
      "epoch": 0.48169750603378925,
      "grad_norm": 18.25,
      "learning_rate": 1.9036604987932424e-05,
      "loss": 1.1297,
      "step": 9580
    },
    {
      "epoch": 0.4822003218020917,
      "grad_norm": 71.5,
      "learning_rate": 1.9035599356395816e-05,
      "loss": 1.1165,
      "step": 9590
    },
    {
      "epoch": 0.4827031375703942,
      "grad_norm": 23.0,
      "learning_rate": 1.9034593724859215e-05,
      "loss": 0.93,
      "step": 9600
    },
    {
      "epoch": 0.4832059533386967,
      "grad_norm": 65.0,
      "learning_rate": 1.9033588093322608e-05,
      "loss": 0.8615,
      "step": 9610
    },
    {
      "epoch": 0.48370876910699917,
      "grad_norm": 37.5,
      "learning_rate": 1.9032582461786004e-05,
      "loss": 0.8139,
      "step": 9620
    },
    {
      "epoch": 0.4842115848753017,
      "grad_norm": 20.5,
      "learning_rate": 1.90315768302494e-05,
      "loss": 1.2609,
      "step": 9630
    },
    {
      "epoch": 0.48471440064360416,
      "grad_norm": 12.8125,
      "learning_rate": 1.9030571198712792e-05,
      "loss": 0.9516,
      "step": 9640
    },
    {
      "epoch": 0.4852172164119067,
      "grad_norm": 33.0,
      "learning_rate": 1.9029565567176188e-05,
      "loss": 1.1764,
      "step": 9650
    },
    {
      "epoch": 0.48572003218020915,
      "grad_norm": 22.625,
      "learning_rate": 1.9028559935639584e-05,
      "loss": 1.2671,
      "step": 9660
    },
    {
      "epoch": 0.48622284794851167,
      "grad_norm": 23.0,
      "learning_rate": 1.9027554304102976e-05,
      "loss": 1.1002,
      "step": 9670
    },
    {
      "epoch": 0.48672566371681414,
      "grad_norm": 64.5,
      "learning_rate": 1.9026548672566376e-05,
      "loss": 1.049,
      "step": 9680
    },
    {
      "epoch": 0.48722847948511666,
      "grad_norm": 15.75,
      "learning_rate": 1.9025543041029768e-05,
      "loss": 0.9913,
      "step": 9690
    },
    {
      "epoch": 0.48773129525341913,
      "grad_norm": 49.0,
      "learning_rate": 1.9024537409493164e-05,
      "loss": 1.1387,
      "step": 9700
    },
    {
      "epoch": 0.48823411102172165,
      "grad_norm": 22.5,
      "learning_rate": 1.902353177795656e-05,
      "loss": 1.103,
      "step": 9710
    },
    {
      "epoch": 0.4887369267900241,
      "grad_norm": 23.375,
      "learning_rate": 1.9022526146419952e-05,
      "loss": 0.8626,
      "step": 9720
    },
    {
      "epoch": 0.48923974255832664,
      "grad_norm": 42.75,
      "learning_rate": 1.9021520514883348e-05,
      "loss": 1.0339,
      "step": 9730
    },
    {
      "epoch": 0.4897425583266291,
      "grad_norm": 14.8125,
      "learning_rate": 1.9020514883346744e-05,
      "loss": 0.8951,
      "step": 9740
    },
    {
      "epoch": 0.49024537409493163,
      "grad_norm": 34.25,
      "learning_rate": 1.9019509251810137e-05,
      "loss": 1.2779,
      "step": 9750
    },
    {
      "epoch": 0.4907481898632341,
      "grad_norm": 17.125,
      "learning_rate": 1.9018503620273532e-05,
      "loss": 1.1898,
      "step": 9760
    },
    {
      "epoch": 0.4912510056315366,
      "grad_norm": 19.125,
      "learning_rate": 1.901749798873693e-05,
      "loss": 0.9314,
      "step": 9770
    },
    {
      "epoch": 0.4917538213998391,
      "grad_norm": 22.125,
      "learning_rate": 1.901649235720032e-05,
      "loss": 0.9469,
      "step": 9780
    },
    {
      "epoch": 0.4922566371681416,
      "grad_norm": 30.375,
      "learning_rate": 1.901548672566372e-05,
      "loss": 1.0304,
      "step": 9790
    },
    {
      "epoch": 0.4927594529364441,
      "grad_norm": 10.625,
      "learning_rate": 1.9014481094127113e-05,
      "loss": 1.0151,
      "step": 9800
    },
    {
      "epoch": 0.4932622687047466,
      "grad_norm": 21.875,
      "learning_rate": 1.901347546259051e-05,
      "loss": 0.7649,
      "step": 9810
    },
    {
      "epoch": 0.49376508447304907,
      "grad_norm": 26.375,
      "learning_rate": 1.9012469831053904e-05,
      "loss": 0.9071,
      "step": 9820
    },
    {
      "epoch": 0.4942679002413516,
      "grad_norm": 14.3125,
      "learning_rate": 1.9011464199517297e-05,
      "loss": 1.0004,
      "step": 9830
    },
    {
      "epoch": 0.49477071600965405,
      "grad_norm": 8.5,
      "learning_rate": 1.9010458567980693e-05,
      "loss": 0.8321,
      "step": 9840
    },
    {
      "epoch": 0.4952735317779566,
      "grad_norm": 10.9375,
      "learning_rate": 1.900945293644409e-05,
      "loss": 1.0412,
      "step": 9850
    },
    {
      "epoch": 0.49577634754625904,
      "grad_norm": 26.0,
      "learning_rate": 1.900844730490748e-05,
      "loss": 0.8688,
      "step": 9860
    },
    {
      "epoch": 0.49627916331456157,
      "grad_norm": 12.5,
      "learning_rate": 1.900744167337088e-05,
      "loss": 0.8936,
      "step": 9870
    },
    {
      "epoch": 0.49678197908286403,
      "grad_norm": 11.875,
      "learning_rate": 1.9006436041834273e-05,
      "loss": 1.2581,
      "step": 9880
    },
    {
      "epoch": 0.49728479485116656,
      "grad_norm": 20.5,
      "learning_rate": 1.900543041029767e-05,
      "loss": 0.9878,
      "step": 9890
    },
    {
      "epoch": 0.497787610619469,
      "grad_norm": 21.125,
      "learning_rate": 1.9004424778761065e-05,
      "loss": 0.9876,
      "step": 9900
    },
    {
      "epoch": 0.49829042638777155,
      "grad_norm": 8.25,
      "learning_rate": 1.9003419147224457e-05,
      "loss": 0.9665,
      "step": 9910
    },
    {
      "epoch": 0.498793242156074,
      "grad_norm": 24.125,
      "learning_rate": 1.9002413515687853e-05,
      "loss": 1.3668,
      "step": 9920
    },
    {
      "epoch": 0.49929605792437654,
      "grad_norm": 6.875,
      "learning_rate": 1.900140788415125e-05,
      "loss": 0.7967,
      "step": 9930
    },
    {
      "epoch": 0.499798873692679,
      "grad_norm": 15.4375,
      "learning_rate": 1.900040225261464e-05,
      "loss": 0.9284,
      "step": 9940
    },
    {
      "epoch": 0.5003016894609815,
      "grad_norm": 18.5,
      "learning_rate": 1.899939662107804e-05,
      "loss": 0.9197,
      "step": 9950
    },
    {
      "epoch": 0.500804505229284,
      "grad_norm": 25.875,
      "learning_rate": 1.8998390989541433e-05,
      "loss": 0.9592,
      "step": 9960
    },
    {
      "epoch": 0.5013073209975865,
      "grad_norm": 28.0,
      "learning_rate": 1.899738535800483e-05,
      "loss": 1.2216,
      "step": 9970
    },
    {
      "epoch": 0.5018101367658889,
      "grad_norm": 20.75,
      "learning_rate": 1.8996379726468225e-05,
      "loss": 1.1536,
      "step": 9980
    },
    {
      "epoch": 0.5023129525341915,
      "grad_norm": 54.25,
      "learning_rate": 1.8995374094931617e-05,
      "loss": 1.3039,
      "step": 9990
    },
    {
      "epoch": 0.502815768302494,
      "grad_norm": 18.5,
      "learning_rate": 1.8994368463395013e-05,
      "loss": 0.9787,
      "step": 10000
    },
    {
      "epoch": 0.502815768302494,
      "eval_accuracy": 0.5106346058502726,
      "eval_loss": 1.0351945161819458,
      "eval_runtime": 466.2164,
      "eval_samples_per_second": 86.526,
      "eval_steps_per_second": 86.526,
      "step": 10000
    },
    {
      "epoch": 0.5033185840707964,
      "grad_norm": 19.75,
      "learning_rate": 1.899336283185841e-05,
      "loss": 0.9965,
      "step": 10010
    },
    {
      "epoch": 0.5038213998390989,
      "grad_norm": 21.125,
      "learning_rate": 1.89923572003218e-05,
      "loss": 1.0149,
      "step": 10020
    },
    {
      "epoch": 0.5043242156074015,
      "grad_norm": 5.09375,
      "learning_rate": 1.8991351568785197e-05,
      "loss": 0.9242,
      "step": 10030
    },
    {
      "epoch": 0.504827031375704,
      "grad_norm": 17.375,
      "learning_rate": 1.8990345937248593e-05,
      "loss": 1.2138,
      "step": 10040
    },
    {
      "epoch": 0.5053298471440064,
      "grad_norm": 5.9375,
      "learning_rate": 1.898934030571199e-05,
      "loss": 0.8992,
      "step": 10050
    },
    {
      "epoch": 0.5058326629123089,
      "grad_norm": 13.0,
      "learning_rate": 1.8988334674175385e-05,
      "loss": 1.0119,
      "step": 10060
    },
    {
      "epoch": 0.5063354786806115,
      "grad_norm": 22.0,
      "learning_rate": 1.8987329042638778e-05,
      "loss": 0.798,
      "step": 10070
    },
    {
      "epoch": 0.5068382944489139,
      "grad_norm": 5.59375,
      "learning_rate": 1.8986323411102173e-05,
      "loss": 1.0676,
      "step": 10080
    },
    {
      "epoch": 0.5073411102172164,
      "grad_norm": 52.5,
      "learning_rate": 1.898531777956557e-05,
      "loss": 1.0114,
      "step": 10090
    },
    {
      "epoch": 0.5078439259855189,
      "grad_norm": 14.3125,
      "learning_rate": 1.8984312148028962e-05,
      "loss": 1.0609,
      "step": 10100
    },
    {
      "epoch": 0.5083467417538214,
      "grad_norm": 76.0,
      "learning_rate": 1.8983306516492358e-05,
      "loss": 0.9045,
      "step": 10110
    },
    {
      "epoch": 0.5088495575221239,
      "grad_norm": 60.0,
      "learning_rate": 1.8982300884955754e-05,
      "loss": 1.2703,
      "step": 10120
    },
    {
      "epoch": 0.5093523732904264,
      "grad_norm": 44.0,
      "learning_rate": 1.898129525341915e-05,
      "loss": 0.9816,
      "step": 10130
    },
    {
      "epoch": 0.5098551890587288,
      "grad_norm": 11.3125,
      "learning_rate": 1.8980289621882545e-05,
      "loss": 1.0559,
      "step": 10140
    },
    {
      "epoch": 0.5103580048270314,
      "grad_norm": 11.875,
      "learning_rate": 1.8979283990345938e-05,
      "loss": 0.7811,
      "step": 10150
    },
    {
      "epoch": 0.5108608205953339,
      "grad_norm": 7.46875,
      "learning_rate": 1.8978278358809334e-05,
      "loss": 0.9885,
      "step": 10160
    },
    {
      "epoch": 0.5113636363636364,
      "grad_norm": 23.625,
      "learning_rate": 1.897727272727273e-05,
      "loss": 1.1783,
      "step": 10170
    },
    {
      "epoch": 0.5118664521319388,
      "grad_norm": 21.25,
      "learning_rate": 1.8976267095736122e-05,
      "loss": 1.1508,
      "step": 10180
    },
    {
      "epoch": 0.5123692679002414,
      "grad_norm": 14.4375,
      "learning_rate": 1.8975261464199518e-05,
      "loss": 0.7945,
      "step": 10190
    },
    {
      "epoch": 0.5128720836685439,
      "grad_norm": 38.75,
      "learning_rate": 1.8974255832662914e-05,
      "loss": 1.182,
      "step": 10200
    },
    {
      "epoch": 0.5133748994368463,
      "grad_norm": 19.375,
      "learning_rate": 1.897325020112631e-05,
      "loss": 0.8987,
      "step": 10210
    },
    {
      "epoch": 0.5138777152051488,
      "grad_norm": 10.1875,
      "learning_rate": 1.8972244569589706e-05,
      "loss": 0.994,
      "step": 10220
    },
    {
      "epoch": 0.5143805309734514,
      "grad_norm": 21.25,
      "learning_rate": 1.8971238938053098e-05,
      "loss": 0.9795,
      "step": 10230
    },
    {
      "epoch": 0.5148833467417538,
      "grad_norm": 31.0,
      "learning_rate": 1.8970233306516494e-05,
      "loss": 1.0142,
      "step": 10240
    },
    {
      "epoch": 0.5153861625100563,
      "grad_norm": 27.625,
      "learning_rate": 1.896922767497989e-05,
      "loss": 1.1256,
      "step": 10250
    },
    {
      "epoch": 0.5158889782783588,
      "grad_norm": 11.4375,
      "learning_rate": 1.8968222043443282e-05,
      "loss": 1.1286,
      "step": 10260
    },
    {
      "epoch": 0.5163917940466614,
      "grad_norm": 23.625,
      "learning_rate": 1.8967216411906678e-05,
      "loss": 1.1225,
      "step": 10270
    },
    {
      "epoch": 0.5168946098149638,
      "grad_norm": 47.75,
      "learning_rate": 1.8966210780370074e-05,
      "loss": 1.25,
      "step": 10280
    },
    {
      "epoch": 0.5173974255832663,
      "grad_norm": 33.25,
      "learning_rate": 1.896520514883347e-05,
      "loss": 1.1116,
      "step": 10290
    },
    {
      "epoch": 0.5179002413515688,
      "grad_norm": 22.5,
      "learning_rate": 1.8964199517296862e-05,
      "loss": 0.6778,
      "step": 10300
    },
    {
      "epoch": 0.5184030571198712,
      "grad_norm": 48.25,
      "learning_rate": 1.896319388576026e-05,
      "loss": 0.957,
      "step": 10310
    },
    {
      "epoch": 0.5189058728881738,
      "grad_norm": 4.5,
      "learning_rate": 1.8962188254223654e-05,
      "loss": 1.1111,
      "step": 10320
    },
    {
      "epoch": 0.5194086886564763,
      "grad_norm": 43.0,
      "learning_rate": 1.896118262268705e-05,
      "loss": 1.0707,
      "step": 10330
    },
    {
      "epoch": 0.5199115044247787,
      "grad_norm": 21.625,
      "learning_rate": 1.8960176991150443e-05,
      "loss": 0.8648,
      "step": 10340
    },
    {
      "epoch": 0.5204143201930812,
      "grad_norm": 38.75,
      "learning_rate": 1.895917135961384e-05,
      "loss": 0.766,
      "step": 10350
    },
    {
      "epoch": 0.5209171359613838,
      "grad_norm": 12.0,
      "learning_rate": 1.8958165728077234e-05,
      "loss": 1.0474,
      "step": 10360
    },
    {
      "epoch": 0.5214199517296862,
      "grad_norm": 37.75,
      "learning_rate": 1.895716009654063e-05,
      "loss": 1.0527,
      "step": 10370
    },
    {
      "epoch": 0.5219227674979887,
      "grad_norm": 5.0625,
      "learning_rate": 1.8956154465004023e-05,
      "loss": 1.2607,
      "step": 10380
    },
    {
      "epoch": 0.5224255832662912,
      "grad_norm": 8.25,
      "learning_rate": 1.895514883346742e-05,
      "loss": 0.8445,
      "step": 10390
    },
    {
      "epoch": 0.5229283990345938,
      "grad_norm": 26.875,
      "learning_rate": 1.8954143201930815e-05,
      "loss": 1.0285,
      "step": 10400
    },
    {
      "epoch": 0.5234312148028962,
      "grad_norm": 28.0,
      "learning_rate": 1.895313757039421e-05,
      "loss": 1.4137,
      "step": 10410
    },
    {
      "epoch": 0.5239340305711987,
      "grad_norm": 6.5625,
      "learning_rate": 1.8952131938857603e-05,
      "loss": 1.0679,
      "step": 10420
    },
    {
      "epoch": 0.5244368463395012,
      "grad_norm": 17.0,
      "learning_rate": 1.8951126307321e-05,
      "loss": 0.9416,
      "step": 10430
    },
    {
      "epoch": 0.5249396621078037,
      "grad_norm": 65.0,
      "learning_rate": 1.8950120675784395e-05,
      "loss": 1.0251,
      "step": 10440
    },
    {
      "epoch": 0.5254424778761062,
      "grad_norm": 14.3125,
      "learning_rate": 1.894911504424779e-05,
      "loss": 1.2048,
      "step": 10450
    },
    {
      "epoch": 0.5259452936444087,
      "grad_norm": 12.1875,
      "learning_rate": 1.8948109412711183e-05,
      "loss": 0.9678,
      "step": 10460
    },
    {
      "epoch": 0.5264481094127111,
      "grad_norm": 28.25,
      "learning_rate": 1.894710378117458e-05,
      "loss": 1.3121,
      "step": 10470
    },
    {
      "epoch": 0.5269509251810137,
      "grad_norm": 18.125,
      "learning_rate": 1.8946098149637975e-05,
      "loss": 1.1789,
      "step": 10480
    },
    {
      "epoch": 0.5274537409493162,
      "grad_norm": 33.75,
      "learning_rate": 1.894509251810137e-05,
      "loss": 0.9325,
      "step": 10490
    },
    {
      "epoch": 0.5279565567176187,
      "grad_norm": 10.75,
      "learning_rate": 1.8944086886564763e-05,
      "loss": 1.1319,
      "step": 10500
    },
    {
      "epoch": 0.5279565567176187,
      "eval_accuracy": 0.5112543381259296,
      "eval_loss": 1.0339479446411133,
      "eval_runtime": 465.1461,
      "eval_samples_per_second": 86.725,
      "eval_steps_per_second": 86.725,
      "step": 10500
    },
    {
      "epoch": 0.5284593724859211,
      "grad_norm": 9.75,
      "learning_rate": 1.894308125502816e-05,
      "loss": 1.4224,
      "step": 10510
    },
    {
      "epoch": 0.5289621882542237,
      "grad_norm": 10.1875,
      "learning_rate": 1.8942075623491555e-05,
      "loss": 0.937,
      "step": 10520
    },
    {
      "epoch": 0.5294650040225262,
      "grad_norm": 20.0,
      "learning_rate": 1.894106999195495e-05,
      "loss": 0.9158,
      "step": 10530
    },
    {
      "epoch": 0.5299678197908286,
      "grad_norm": 6.84375,
      "learning_rate": 1.8940064360418343e-05,
      "loss": 1.0714,
      "step": 10540
    },
    {
      "epoch": 0.5304706355591311,
      "grad_norm": 25.125,
      "learning_rate": 1.893905872888174e-05,
      "loss": 1.2289,
      "step": 10550
    },
    {
      "epoch": 0.5309734513274337,
      "grad_norm": 70.0,
      "learning_rate": 1.8938053097345135e-05,
      "loss": 1.3075,
      "step": 10560
    },
    {
      "epoch": 0.5314762670957361,
      "grad_norm": 33.25,
      "learning_rate": 1.8937047465808528e-05,
      "loss": 1.114,
      "step": 10570
    },
    {
      "epoch": 0.5319790828640386,
      "grad_norm": 11.75,
      "learning_rate": 1.8936041834271923e-05,
      "loss": 0.8508,
      "step": 10580
    },
    {
      "epoch": 0.5324818986323411,
      "grad_norm": 25.5,
      "learning_rate": 1.893503620273532e-05,
      "loss": 0.9953,
      "step": 10590
    },
    {
      "epoch": 0.5329847144006437,
      "grad_norm": 39.25,
      "learning_rate": 1.8934030571198715e-05,
      "loss": 1.2255,
      "step": 10600
    },
    {
      "epoch": 0.5334875301689461,
      "grad_norm": 32.75,
      "learning_rate": 1.893302493966211e-05,
      "loss": 1.0123,
      "step": 10610
    },
    {
      "epoch": 0.5339903459372486,
      "grad_norm": 21.125,
      "learning_rate": 1.8932019308125504e-05,
      "loss": 0.9988,
      "step": 10620
    },
    {
      "epoch": 0.5344931617055511,
      "grad_norm": 40.0,
      "learning_rate": 1.89310136765889e-05,
      "loss": 0.9863,
      "step": 10630
    },
    {
      "epoch": 0.5349959774738536,
      "grad_norm": 14.875,
      "learning_rate": 1.8930008045052295e-05,
      "loss": 0.8027,
      "step": 10640
    },
    {
      "epoch": 0.5354987932421561,
      "grad_norm": 29.0,
      "learning_rate": 1.8929002413515688e-05,
      "loss": 1.098,
      "step": 10650
    },
    {
      "epoch": 0.5360016090104586,
      "grad_norm": 35.75,
      "learning_rate": 1.8927996781979087e-05,
      "loss": 1.0168,
      "step": 10660
    },
    {
      "epoch": 0.536504424778761,
      "grad_norm": 28.375,
      "learning_rate": 1.892699115044248e-05,
      "loss": 1.1876,
      "step": 10670
    },
    {
      "epoch": 0.5370072405470635,
      "grad_norm": 41.5,
      "learning_rate": 1.8925985518905875e-05,
      "loss": 1.1681,
      "step": 10680
    },
    {
      "epoch": 0.5375100563153661,
      "grad_norm": 25.625,
      "learning_rate": 1.892497988736927e-05,
      "loss": 0.8655,
      "step": 10690
    },
    {
      "epoch": 0.5380128720836685,
      "grad_norm": 15.0,
      "learning_rate": 1.8923974255832664e-05,
      "loss": 0.802,
      "step": 10700
    },
    {
      "epoch": 0.538515687851971,
      "grad_norm": 37.25,
      "learning_rate": 1.892296862429606e-05,
      "loss": 1.0533,
      "step": 10710
    },
    {
      "epoch": 0.5390185036202735,
      "grad_norm": 14.75,
      "learning_rate": 1.8921962992759456e-05,
      "loss": 1.1517,
      "step": 10720
    },
    {
      "epoch": 0.5395213193885761,
      "grad_norm": 10.0625,
      "learning_rate": 1.8920957361222848e-05,
      "loss": 0.7469,
      "step": 10730
    },
    {
      "epoch": 0.5400241351568785,
      "grad_norm": 12.9375,
      "learning_rate": 1.8919951729686247e-05,
      "loss": 0.8551,
      "step": 10740
    },
    {
      "epoch": 0.540526950925181,
      "grad_norm": 16.75,
      "learning_rate": 1.891894609814964e-05,
      "loss": 0.9784,
      "step": 10750
    },
    {
      "epoch": 0.5410297666934835,
      "grad_norm": 25.25,
      "learning_rate": 1.8917940466613036e-05,
      "loss": 0.6296,
      "step": 10760
    },
    {
      "epoch": 0.541532582461786,
      "grad_norm": 45.25,
      "learning_rate": 1.891693483507643e-05,
      "loss": 0.9257,
      "step": 10770
    },
    {
      "epoch": 0.5420353982300885,
      "grad_norm": 13.0625,
      "learning_rate": 1.8915929203539824e-05,
      "loss": 1.2737,
      "step": 10780
    },
    {
      "epoch": 0.542538213998391,
      "grad_norm": 60.75,
      "learning_rate": 1.891492357200322e-05,
      "loss": 1.1031,
      "step": 10790
    },
    {
      "epoch": 0.5430410297666934,
      "grad_norm": 21.0,
      "learning_rate": 1.8913917940466616e-05,
      "loss": 1.1281,
      "step": 10800
    },
    {
      "epoch": 0.543543845534996,
      "grad_norm": 88.5,
      "learning_rate": 1.8912912308930008e-05,
      "loss": 1.237,
      "step": 10810
    },
    {
      "epoch": 0.5440466613032985,
      "grad_norm": 51.0,
      "learning_rate": 1.8911906677393404e-05,
      "loss": 1.1597,
      "step": 10820
    },
    {
      "epoch": 0.544549477071601,
      "grad_norm": 6.875,
      "learning_rate": 1.89109010458568e-05,
      "loss": 0.9978,
      "step": 10830
    },
    {
      "epoch": 0.5450522928399034,
      "grad_norm": 42.5,
      "learning_rate": 1.8909895414320193e-05,
      "loss": 1.0994,
      "step": 10840
    },
    {
      "epoch": 0.545555108608206,
      "grad_norm": 5.71875,
      "learning_rate": 1.8908889782783592e-05,
      "loss": 0.7362,
      "step": 10850
    },
    {
      "epoch": 0.5460579243765085,
      "grad_norm": 19.5,
      "learning_rate": 1.8907884151246984e-05,
      "loss": 1.0743,
      "step": 10860
    },
    {
      "epoch": 0.5465607401448109,
      "grad_norm": 49.25,
      "learning_rate": 1.890687851971038e-05,
      "loss": 1.1288,
      "step": 10870
    },
    {
      "epoch": 0.5470635559131134,
      "grad_norm": 33.25,
      "learning_rate": 1.8905872888173776e-05,
      "loss": 1.2567,
      "step": 10880
    },
    {
      "epoch": 0.547566371681416,
      "grad_norm": 9.8125,
      "learning_rate": 1.890486725663717e-05,
      "loss": 0.8506,
      "step": 10890
    },
    {
      "epoch": 0.5480691874497184,
      "grad_norm": 19.0,
      "learning_rate": 1.8903861625100564e-05,
      "loss": 0.8912,
      "step": 10900
    },
    {
      "epoch": 0.5485720032180209,
      "grad_norm": 24.0,
      "learning_rate": 1.890285599356396e-05,
      "loss": 0.8131,
      "step": 10910
    },
    {
      "epoch": 0.5490748189863234,
      "grad_norm": 12.9375,
      "learning_rate": 1.8901850362027353e-05,
      "loss": 0.6741,
      "step": 10920
    },
    {
      "epoch": 0.549577634754626,
      "grad_norm": 62.75,
      "learning_rate": 1.8900844730490752e-05,
      "loss": 1.4142,
      "step": 10930
    },
    {
      "epoch": 0.5500804505229284,
      "grad_norm": 9.1875,
      "learning_rate": 1.8899839098954145e-05,
      "loss": 1.1467,
      "step": 10940
    },
    {
      "epoch": 0.5505832662912309,
      "grad_norm": 14.375,
      "learning_rate": 1.889883346741754e-05,
      "loss": 0.9077,
      "step": 10950
    },
    {
      "epoch": 0.5510860820595334,
      "grad_norm": 8.0,
      "learning_rate": 1.8897827835880936e-05,
      "loss": 0.8574,
      "step": 10960
    },
    {
      "epoch": 0.5515888978278359,
      "grad_norm": 30.25,
      "learning_rate": 1.889682220434433e-05,
      "loss": 0.7793,
      "step": 10970
    },
    {
      "epoch": 0.5520917135961384,
      "grad_norm": 36.0,
      "learning_rate": 1.8895816572807725e-05,
      "loss": 0.936,
      "step": 10980
    },
    {
      "epoch": 0.5525945293644409,
      "grad_norm": 6.6875,
      "learning_rate": 1.889481094127112e-05,
      "loss": 0.7789,
      "step": 10990
    },
    {
      "epoch": 0.5530973451327433,
      "grad_norm": 7.3125,
      "learning_rate": 1.8893805309734513e-05,
      "loss": 0.9668,
      "step": 11000
    },
    {
      "epoch": 0.5530973451327433,
      "eval_accuracy": 0.5116757560733763,
      "eval_loss": 1.0370365381240845,
      "eval_runtime": 464.6848,
      "eval_samples_per_second": 86.812,
      "eval_steps_per_second": 86.812,
      "step": 11000
    },
    {
      "epoch": 0.5536001609010458,
      "grad_norm": 15.8125,
      "learning_rate": 1.8892799678197912e-05,
      "loss": 0.8382,
      "step": 11010
    },
    {
      "epoch": 0.5541029766693484,
      "grad_norm": 41.5,
      "learning_rate": 1.8891794046661305e-05,
      "loss": 0.9494,
      "step": 11020
    },
    {
      "epoch": 0.5546057924376508,
      "grad_norm": 5.5625,
      "learning_rate": 1.8890788415124697e-05,
      "loss": 0.9633,
      "step": 11030
    },
    {
      "epoch": 0.5551086082059533,
      "grad_norm": 5.46875,
      "learning_rate": 1.8889782783588097e-05,
      "loss": 1.0201,
      "step": 11040
    },
    {
      "epoch": 0.5556114239742558,
      "grad_norm": 57.0,
      "learning_rate": 1.888877715205149e-05,
      "loss": 1.0862,
      "step": 11050
    },
    {
      "epoch": 0.5561142397425584,
      "grad_norm": 54.75,
      "learning_rate": 1.8887771520514885e-05,
      "loss": 1.0075,
      "step": 11060
    },
    {
      "epoch": 0.5566170555108608,
      "grad_norm": 32.25,
      "learning_rate": 1.888676588897828e-05,
      "loss": 1.0024,
      "step": 11070
    },
    {
      "epoch": 0.5571198712791633,
      "grad_norm": 58.25,
      "learning_rate": 1.8885760257441673e-05,
      "loss": 0.9603,
      "step": 11080
    },
    {
      "epoch": 0.5576226870474658,
      "grad_norm": 4.71875,
      "learning_rate": 1.888475462590507e-05,
      "loss": 1.2026,
      "step": 11090
    },
    {
      "epoch": 0.5581255028157683,
      "grad_norm": 11.5,
      "learning_rate": 1.8883748994368465e-05,
      "loss": 1.1603,
      "step": 11100
    },
    {
      "epoch": 0.5586283185840708,
      "grad_norm": 25.25,
      "learning_rate": 1.8882743362831858e-05,
      "loss": 0.9529,
      "step": 11110
    },
    {
      "epoch": 0.5591311343523733,
      "grad_norm": 24.0,
      "learning_rate": 1.8881737731295257e-05,
      "loss": 1.1152,
      "step": 11120
    },
    {
      "epoch": 0.5596339501206757,
      "grad_norm": 10.3125,
      "learning_rate": 1.888073209975865e-05,
      "loss": 0.7266,
      "step": 11130
    },
    {
      "epoch": 0.5601367658889783,
      "grad_norm": 22.5,
      "learning_rate": 1.8879726468222045e-05,
      "loss": 0.9547,
      "step": 11140
    },
    {
      "epoch": 0.5606395816572808,
      "grad_norm": 35.75,
      "learning_rate": 1.887872083668544e-05,
      "loss": 0.9877,
      "step": 11150
    },
    {
      "epoch": 0.5611423974255833,
      "grad_norm": 15.125,
      "learning_rate": 1.8877715205148834e-05,
      "loss": 1.116,
      "step": 11160
    },
    {
      "epoch": 0.5616452131938857,
      "grad_norm": 61.25,
      "learning_rate": 1.887670957361223e-05,
      "loss": 1.3199,
      "step": 11170
    },
    {
      "epoch": 0.5621480289621883,
      "grad_norm": 17.125,
      "learning_rate": 1.8875703942075625e-05,
      "loss": 1.2451,
      "step": 11180
    },
    {
      "epoch": 0.5626508447304908,
      "grad_norm": 32.0,
      "learning_rate": 1.8874698310539018e-05,
      "loss": 1.0004,
      "step": 11190
    },
    {
      "epoch": 0.5631536604987932,
      "grad_norm": 13.5625,
      "learning_rate": 1.8873692679002417e-05,
      "loss": 0.9459,
      "step": 11200
    },
    {
      "epoch": 0.5636564762670957,
      "grad_norm": 28.5,
      "learning_rate": 1.887268704746581e-05,
      "loss": 1.4365,
      "step": 11210
    },
    {
      "epoch": 0.5641592920353983,
      "grad_norm": 19.875,
      "learning_rate": 1.8871681415929205e-05,
      "loss": 0.9857,
      "step": 11220
    },
    {
      "epoch": 0.5646621078037007,
      "grad_norm": 8.0,
      "learning_rate": 1.88706757843926e-05,
      "loss": 1.1586,
      "step": 11230
    },
    {
      "epoch": 0.5651649235720032,
      "grad_norm": 15.5,
      "learning_rate": 1.8869670152855994e-05,
      "loss": 1.0178,
      "step": 11240
    },
    {
      "epoch": 0.5656677393403057,
      "grad_norm": 34.75,
      "learning_rate": 1.886866452131939e-05,
      "loss": 0.9332,
      "step": 11250
    },
    {
      "epoch": 0.5661705551086083,
      "grad_norm": 30.25,
      "learning_rate": 1.8867658889782786e-05,
      "loss": 0.961,
      "step": 11260
    },
    {
      "epoch": 0.5666733708769107,
      "grad_norm": 18.75,
      "learning_rate": 1.8866653258246178e-05,
      "loss": 1.2204,
      "step": 11270
    },
    {
      "epoch": 0.5671761866452132,
      "grad_norm": 13.5,
      "learning_rate": 1.8865647626709574e-05,
      "loss": 1.2005,
      "step": 11280
    },
    {
      "epoch": 0.5676790024135157,
      "grad_norm": 14.0,
      "learning_rate": 1.886464199517297e-05,
      "loss": 0.7524,
      "step": 11290
    },
    {
      "epoch": 0.5681818181818182,
      "grad_norm": 6.90625,
      "learning_rate": 1.8863636363636366e-05,
      "loss": 0.8488,
      "step": 11300
    },
    {
      "epoch": 0.5686846339501207,
      "grad_norm": 39.5,
      "learning_rate": 1.886263073209976e-05,
      "loss": 1.0165,
      "step": 11310
    },
    {
      "epoch": 0.5691874497184232,
      "grad_norm": 9.8125,
      "learning_rate": 1.8861625100563154e-05,
      "loss": 0.7513,
      "step": 11320
    },
    {
      "epoch": 0.5696902654867256,
      "grad_norm": 24.0,
      "learning_rate": 1.886061946902655e-05,
      "loss": 0.7737,
      "step": 11330
    },
    {
      "epoch": 0.5701930812550282,
      "grad_norm": 15.1875,
      "learning_rate": 1.8859613837489946e-05,
      "loss": 1.0234,
      "step": 11340
    },
    {
      "epoch": 0.5706958970233307,
      "grad_norm": 41.75,
      "learning_rate": 1.8858608205953338e-05,
      "loss": 0.9288,
      "step": 11350
    },
    {
      "epoch": 0.5711987127916331,
      "grad_norm": 25.375,
      "learning_rate": 1.8857602574416734e-05,
      "loss": 1.0888,
      "step": 11360
    },
    {
      "epoch": 0.5717015285599356,
      "grad_norm": 14.75,
      "learning_rate": 1.885659694288013e-05,
      "loss": 0.9278,
      "step": 11370
    },
    {
      "epoch": 0.5722043443282381,
      "grad_norm": 36.25,
      "learning_rate": 1.8855591311343526e-05,
      "loss": 1.1609,
      "step": 11380
    },
    {
      "epoch": 0.5727071600965407,
      "grad_norm": 19.0,
      "learning_rate": 1.8854585679806922e-05,
      "loss": 1.3852,
      "step": 11390
    },
    {
      "epoch": 0.5732099758648431,
      "grad_norm": 6.65625,
      "learning_rate": 1.8853580048270314e-05,
      "loss": 0.68,
      "step": 11400
    },
    {
      "epoch": 0.5737127916331456,
      "grad_norm": 46.75,
      "learning_rate": 1.885257441673371e-05,
      "loss": 1.2676,
      "step": 11410
    },
    {
      "epoch": 0.5742156074014481,
      "grad_norm": 16.625,
      "learning_rate": 1.8851568785197106e-05,
      "loss": 0.8167,
      "step": 11420
    },
    {
      "epoch": 0.5747184231697506,
      "grad_norm": 30.0,
      "learning_rate": 1.88505631536605e-05,
      "loss": 0.8557,
      "step": 11430
    },
    {
      "epoch": 0.5752212389380531,
      "grad_norm": 43.75,
      "learning_rate": 1.8849557522123894e-05,
      "loss": 0.9238,
      "step": 11440
    },
    {
      "epoch": 0.5757240547063556,
      "grad_norm": 21.625,
      "learning_rate": 1.884855189058729e-05,
      "loss": 1.3541,
      "step": 11450
    },
    {
      "epoch": 0.576226870474658,
      "grad_norm": 42.0,
      "learning_rate": 1.8847546259050686e-05,
      "loss": 1.3038,
      "step": 11460
    },
    {
      "epoch": 0.5767296862429606,
      "grad_norm": 35.25,
      "learning_rate": 1.8846540627514082e-05,
      "loss": 0.7772,
      "step": 11470
    },
    {
      "epoch": 0.5772325020112631,
      "grad_norm": 6.0625,
      "learning_rate": 1.8845534995977475e-05,
      "loss": 0.9913,
      "step": 11480
    },
    {
      "epoch": 0.5777353177795655,
      "grad_norm": 51.75,
      "learning_rate": 1.884452936444087e-05,
      "loss": 1.3605,
      "step": 11490
    },
    {
      "epoch": 0.578238133547868,
      "grad_norm": 14.25,
      "learning_rate": 1.8843523732904266e-05,
      "loss": 1.1211,
      "step": 11500
    },
    {
      "epoch": 0.578238133547868,
      "eval_accuracy": 0.5115270203272186,
      "eval_loss": 1.036556601524353,
      "eval_runtime": 465.9401,
      "eval_samples_per_second": 86.578,
      "eval_steps_per_second": 86.578,
      "step": 11500
    },
    {
      "epoch": 0.5787409493161706,
      "grad_norm": 9.25,
      "learning_rate": 1.884251810136766e-05,
      "loss": 1.1897,
      "step": 11510
    },
    {
      "epoch": 0.5792437650844731,
      "grad_norm": 6.03125,
      "learning_rate": 1.8841512469831055e-05,
      "loss": 0.9827,
      "step": 11520
    },
    {
      "epoch": 0.5797465808527755,
      "grad_norm": 6.59375,
      "learning_rate": 1.884050683829445e-05,
      "loss": 0.8371,
      "step": 11530
    },
    {
      "epoch": 0.580249396621078,
      "grad_norm": 29.75,
      "learning_rate": 1.8839501206757846e-05,
      "loss": 1.1436,
      "step": 11540
    },
    {
      "epoch": 0.5807522123893806,
      "grad_norm": 24.5,
      "learning_rate": 1.883849557522124e-05,
      "loss": 0.9333,
      "step": 11550
    },
    {
      "epoch": 0.581255028157683,
      "grad_norm": 17.0,
      "learning_rate": 1.8837489943684635e-05,
      "loss": 1.0105,
      "step": 11560
    },
    {
      "epoch": 0.5817578439259855,
      "grad_norm": 13.3125,
      "learning_rate": 1.883648431214803e-05,
      "loss": 1.0159,
      "step": 11570
    },
    {
      "epoch": 0.582260659694288,
      "grad_norm": 52.5,
      "learning_rate": 1.8835478680611427e-05,
      "loss": 1.0718,
      "step": 11580
    },
    {
      "epoch": 0.5827634754625906,
      "grad_norm": 30.125,
      "learning_rate": 1.883447304907482e-05,
      "loss": 0.8349,
      "step": 11590
    },
    {
      "epoch": 0.583266291230893,
      "grad_norm": 11.0625,
      "learning_rate": 1.8833467417538215e-05,
      "loss": 0.8935,
      "step": 11600
    },
    {
      "epoch": 0.5837691069991955,
      "grad_norm": 25.875,
      "learning_rate": 1.883246178600161e-05,
      "loss": 1.0757,
      "step": 11610
    },
    {
      "epoch": 0.584271922767498,
      "grad_norm": 13.4375,
      "learning_rate": 1.8831456154465007e-05,
      "loss": 0.7796,
      "step": 11620
    },
    {
      "epoch": 0.5847747385358005,
      "grad_norm": 20.875,
      "learning_rate": 1.88304505229284e-05,
      "loss": 0.935,
      "step": 11630
    },
    {
      "epoch": 0.585277554304103,
      "grad_norm": 43.5,
      "learning_rate": 1.8829444891391795e-05,
      "loss": 0.9787,
      "step": 11640
    },
    {
      "epoch": 0.5857803700724055,
      "grad_norm": 12.125,
      "learning_rate": 1.882843925985519e-05,
      "loss": 0.8575,
      "step": 11650
    },
    {
      "epoch": 0.5862831858407079,
      "grad_norm": 17.375,
      "learning_rate": 1.8827433628318587e-05,
      "loss": 0.7759,
      "step": 11660
    },
    {
      "epoch": 0.5867860016090105,
      "grad_norm": 39.5,
      "learning_rate": 1.882642799678198e-05,
      "loss": 1.0498,
      "step": 11670
    },
    {
      "epoch": 0.587288817377313,
      "grad_norm": 21.0,
      "learning_rate": 1.8825422365245375e-05,
      "loss": 1.0365,
      "step": 11680
    },
    {
      "epoch": 0.5877916331456154,
      "grad_norm": 14.5,
      "learning_rate": 1.882441673370877e-05,
      "loss": 0.995,
      "step": 11690
    },
    {
      "epoch": 0.5882944489139179,
      "grad_norm": 97.0,
      "learning_rate": 1.8823411102172167e-05,
      "loss": 1.3053,
      "step": 11700
    },
    {
      "epoch": 0.5887972646822205,
      "grad_norm": 15.75,
      "learning_rate": 1.882240547063556e-05,
      "loss": 0.7906,
      "step": 11710
    },
    {
      "epoch": 0.589300080450523,
      "grad_norm": 37.75,
      "learning_rate": 1.8821399839098955e-05,
      "loss": 1.1526,
      "step": 11720
    },
    {
      "epoch": 0.5898028962188254,
      "grad_norm": 25.625,
      "learning_rate": 1.882039420756235e-05,
      "loss": 0.8825,
      "step": 11730
    },
    {
      "epoch": 0.5903057119871279,
      "grad_norm": 15.5,
      "learning_rate": 1.8819388576025747e-05,
      "loss": 1.1241,
      "step": 11740
    },
    {
      "epoch": 0.5908085277554304,
      "grad_norm": 17.375,
      "learning_rate": 1.881838294448914e-05,
      "loss": 1.0366,
      "step": 11750
    },
    {
      "epoch": 0.5913113435237329,
      "grad_norm": 29.5,
      "learning_rate": 1.8817377312952535e-05,
      "loss": 1.1958,
      "step": 11760
    },
    {
      "epoch": 0.5918141592920354,
      "grad_norm": 19.125,
      "learning_rate": 1.881637168141593e-05,
      "loss": 1.2505,
      "step": 11770
    },
    {
      "epoch": 0.5923169750603379,
      "grad_norm": 31.75,
      "learning_rate": 1.8815366049879327e-05,
      "loss": 0.9719,
      "step": 11780
    },
    {
      "epoch": 0.5928197908286403,
      "grad_norm": 9.3125,
      "learning_rate": 1.881436041834272e-05,
      "loss": 0.9268,
      "step": 11790
    },
    {
      "epoch": 0.5933226065969429,
      "grad_norm": 8.5,
      "learning_rate": 1.8813354786806116e-05,
      "loss": 0.8984,
      "step": 11800
    },
    {
      "epoch": 0.5938254223652454,
      "grad_norm": 17.875,
      "learning_rate": 1.881234915526951e-05,
      "loss": 1.0292,
      "step": 11810
    },
    {
      "epoch": 0.5943282381335478,
      "grad_norm": 13.1875,
      "learning_rate": 1.8811343523732904e-05,
      "loss": 1.1356,
      "step": 11820
    },
    {
      "epoch": 0.5948310539018503,
      "grad_norm": 25.875,
      "learning_rate": 1.88103378921963e-05,
      "loss": 1.0693,
      "step": 11830
    },
    {
      "epoch": 0.5953338696701529,
      "grad_norm": 19.0,
      "learning_rate": 1.8809332260659696e-05,
      "loss": 1.1038,
      "step": 11840
    },
    {
      "epoch": 0.5958366854384554,
      "grad_norm": 31.5,
      "learning_rate": 1.880832662912309e-05,
      "loss": 0.7758,
      "step": 11850
    },
    {
      "epoch": 0.5963395012067578,
      "grad_norm": 15.125,
      "learning_rate": 1.8807320997586487e-05,
      "loss": 1.2754,
      "step": 11860
    },
    {
      "epoch": 0.5968423169750603,
      "grad_norm": 26.25,
      "learning_rate": 1.880631536604988e-05,
      "loss": 1.1808,
      "step": 11870
    },
    {
      "epoch": 0.5973451327433629,
      "grad_norm": 15.1875,
      "learning_rate": 1.8805309734513276e-05,
      "loss": 1.0024,
      "step": 11880
    },
    {
      "epoch": 0.5978479485116653,
      "grad_norm": 52.5,
      "learning_rate": 1.8804304102976672e-05,
      "loss": 0.8926,
      "step": 11890
    },
    {
      "epoch": 0.5983507642799678,
      "grad_norm": 13.75,
      "learning_rate": 1.8803298471440064e-05,
      "loss": 1.1999,
      "step": 11900
    },
    {
      "epoch": 0.5988535800482703,
      "grad_norm": 18.875,
      "learning_rate": 1.880229283990346e-05,
      "loss": 0.8841,
      "step": 11910
    },
    {
      "epoch": 0.5993563958165729,
      "grad_norm": 46.25,
      "learning_rate": 1.8801287208366856e-05,
      "loss": 1.0838,
      "step": 11920
    },
    {
      "epoch": 0.5998592115848753,
      "grad_norm": 15.8125,
      "learning_rate": 1.8800281576830252e-05,
      "loss": 0.9178,
      "step": 11930
    },
    {
      "epoch": 0.6003620273531778,
      "grad_norm": 44.75,
      "learning_rate": 1.8799275945293648e-05,
      "loss": 1.2286,
      "step": 11940
    },
    {
      "epoch": 0.6008648431214803,
      "grad_norm": 3.921875,
      "learning_rate": 1.879827031375704e-05,
      "loss": 0.9036,
      "step": 11950
    },
    {
      "epoch": 0.6013676588897828,
      "grad_norm": 3.953125,
      "learning_rate": 1.8797264682220436e-05,
      "loss": 1.0707,
      "step": 11960
    },
    {
      "epoch": 0.6018704746580853,
      "grad_norm": 12.75,
      "learning_rate": 1.8796259050683832e-05,
      "loss": 1.1303,
      "step": 11970
    },
    {
      "epoch": 0.6023732904263878,
      "grad_norm": 41.5,
      "learning_rate": 1.8795253419147224e-05,
      "loss": 0.815,
      "step": 11980
    },
    {
      "epoch": 0.6028761061946902,
      "grad_norm": 30.0,
      "learning_rate": 1.879424778761062e-05,
      "loss": 1.1546,
      "step": 11990
    },
    {
      "epoch": 0.6033789219629928,
      "grad_norm": 62.75,
      "learning_rate": 1.8793242156074016e-05,
      "loss": 0.9474,
      "step": 12000
    },
    {
      "epoch": 0.6033789219629928,
      "eval_accuracy": 0.5115518096182449,
      "eval_loss": 1.0344663858413696,
      "eval_runtime": 464.9744,
      "eval_samples_per_second": 86.757,
      "eval_steps_per_second": 86.757,
      "step": 12000
    },
    {
      "epoch": 0.6038817377312953,
      "grad_norm": 16.375,
      "learning_rate": 1.8792236524537412e-05,
      "loss": 0.9432,
      "step": 12010
    },
    {
      "epoch": 0.6043845534995977,
      "grad_norm": 8.625,
      "learning_rate": 1.8791230893000808e-05,
      "loss": 0.7113,
      "step": 12020
    },
    {
      "epoch": 0.6048873692679002,
      "grad_norm": 8.75,
      "learning_rate": 1.87902252614642e-05,
      "loss": 0.9072,
      "step": 12030
    },
    {
      "epoch": 0.6053901850362028,
      "grad_norm": 7.0,
      "learning_rate": 1.8789219629927596e-05,
      "loss": 0.7794,
      "step": 12040
    },
    {
      "epoch": 0.6058930008045053,
      "grad_norm": 41.0,
      "learning_rate": 1.8788213998390992e-05,
      "loss": 1.088,
      "step": 12050
    },
    {
      "epoch": 0.6063958165728077,
      "grad_norm": 13.0625,
      "learning_rate": 1.8787208366854385e-05,
      "loss": 1.1198,
      "step": 12060
    },
    {
      "epoch": 0.6068986323411102,
      "grad_norm": 29.25,
      "learning_rate": 1.878620273531778e-05,
      "loss": 1.0594,
      "step": 12070
    },
    {
      "epoch": 0.6074014481094127,
      "grad_norm": 9.6875,
      "learning_rate": 1.8785197103781176e-05,
      "loss": 1.0723,
      "step": 12080
    },
    {
      "epoch": 0.6079042638777152,
      "grad_norm": 32.25,
      "learning_rate": 1.878419147224457e-05,
      "loss": 1.0384,
      "step": 12090
    },
    {
      "epoch": 0.6084070796460177,
      "grad_norm": 5.09375,
      "learning_rate": 1.8783185840707968e-05,
      "loss": 0.9803,
      "step": 12100
    },
    {
      "epoch": 0.6089098954143202,
      "grad_norm": 18.625,
      "learning_rate": 1.878218020917136e-05,
      "loss": 1.1226,
      "step": 12110
    },
    {
      "epoch": 0.6094127111826226,
      "grad_norm": 11.375,
      "learning_rate": 1.8781174577634757e-05,
      "loss": 0.6218,
      "step": 12120
    },
    {
      "epoch": 0.6099155269509252,
      "grad_norm": 35.5,
      "learning_rate": 1.8780168946098152e-05,
      "loss": 0.934,
      "step": 12130
    },
    {
      "epoch": 0.6104183427192277,
      "grad_norm": 31.375,
      "learning_rate": 1.8779163314561545e-05,
      "loss": 1.2066,
      "step": 12140
    },
    {
      "epoch": 0.6109211584875301,
      "grad_norm": 19.0,
      "learning_rate": 1.877815768302494e-05,
      "loss": 1.0526,
      "step": 12150
    },
    {
      "epoch": 0.6114239742558326,
      "grad_norm": 10.8125,
      "learning_rate": 1.8777152051488337e-05,
      "loss": 0.8265,
      "step": 12160
    },
    {
      "epoch": 0.6119267900241352,
      "grad_norm": 60.25,
      "learning_rate": 1.877614641995173e-05,
      "loss": 1.4159,
      "step": 12170
    },
    {
      "epoch": 0.6124296057924377,
      "grad_norm": 42.5,
      "learning_rate": 1.877514078841513e-05,
      "loss": 0.9598,
      "step": 12180
    },
    {
      "epoch": 0.6129324215607401,
      "grad_norm": 31.0,
      "learning_rate": 1.877413515687852e-05,
      "loss": 0.9494,
      "step": 12190
    },
    {
      "epoch": 0.6134352373290426,
      "grad_norm": 21.875,
      "learning_rate": 1.8773129525341917e-05,
      "loss": 1.0357,
      "step": 12200
    },
    {
      "epoch": 0.6139380530973452,
      "grad_norm": 15.25,
      "learning_rate": 1.8772123893805313e-05,
      "loss": 0.728,
      "step": 12210
    },
    {
      "epoch": 0.6144408688656476,
      "grad_norm": 7.1875,
      "learning_rate": 1.8771118262268705e-05,
      "loss": 1.0115,
      "step": 12220
    },
    {
      "epoch": 0.6149436846339501,
      "grad_norm": 44.5,
      "learning_rate": 1.87701126307321e-05,
      "loss": 1.0542,
      "step": 12230
    },
    {
      "epoch": 0.6154465004022526,
      "grad_norm": 45.25,
      "learning_rate": 1.8769106999195497e-05,
      "loss": 1.0544,
      "step": 12240
    },
    {
      "epoch": 0.6159493161705552,
      "grad_norm": 12.0,
      "learning_rate": 1.876810136765889e-05,
      "loss": 1.2804,
      "step": 12250
    },
    {
      "epoch": 0.6164521319388576,
      "grad_norm": 7.28125,
      "learning_rate": 1.876709573612229e-05,
      "loss": 1.0787,
      "step": 12260
    },
    {
      "epoch": 0.6169549477071601,
      "grad_norm": 22.25,
      "learning_rate": 1.876609010458568e-05,
      "loss": 0.9921,
      "step": 12270
    },
    {
      "epoch": 0.6174577634754626,
      "grad_norm": 17.25,
      "learning_rate": 1.8765084473049077e-05,
      "loss": 0.9897,
      "step": 12280
    },
    {
      "epoch": 0.6179605792437651,
      "grad_norm": 24.25,
      "learning_rate": 1.8764078841512473e-05,
      "loss": 1.0232,
      "step": 12290
    },
    {
      "epoch": 0.6184633950120676,
      "grad_norm": 30.75,
      "learning_rate": 1.8763073209975865e-05,
      "loss": 0.999,
      "step": 12300
    },
    {
      "epoch": 0.6189662107803701,
      "grad_norm": 32.75,
      "learning_rate": 1.876206757843926e-05,
      "loss": 1.2868,
      "step": 12310
    },
    {
      "epoch": 0.6194690265486725,
      "grad_norm": 11.8125,
      "learning_rate": 1.8761061946902657e-05,
      "loss": 0.7433,
      "step": 12320
    },
    {
      "epoch": 0.6199718423169751,
      "grad_norm": 19.5,
      "learning_rate": 1.876005631536605e-05,
      "loss": 0.8507,
      "step": 12330
    },
    {
      "epoch": 0.6204746580852776,
      "grad_norm": 24.125,
      "learning_rate": 1.8759050683829446e-05,
      "loss": 1.0319,
      "step": 12340
    },
    {
      "epoch": 0.62097747385358,
      "grad_norm": 26.125,
      "learning_rate": 1.875804505229284e-05,
      "loss": 1.0654,
      "step": 12350
    },
    {
      "epoch": 0.6214802896218825,
      "grad_norm": 9.625,
      "learning_rate": 1.8757039420756234e-05,
      "loss": 0.8278,
      "step": 12360
    },
    {
      "epoch": 0.6219831053901851,
      "grad_norm": 18.5,
      "learning_rate": 1.8756033789219633e-05,
      "loss": 0.9691,
      "step": 12370
    },
    {
      "epoch": 0.6224859211584876,
      "grad_norm": 26.625,
      "learning_rate": 1.8755028157683026e-05,
      "loss": 1.111,
      "step": 12380
    },
    {
      "epoch": 0.62298873692679,
      "grad_norm": 19.5,
      "learning_rate": 1.875402252614642e-05,
      "loss": 0.8015,
      "step": 12390
    },
    {
      "epoch": 0.6234915526950925,
      "grad_norm": 86.0,
      "learning_rate": 1.8753016894609817e-05,
      "loss": 1.2869,
      "step": 12400
    },
    {
      "epoch": 0.6239943684633951,
      "grad_norm": 38.25,
      "learning_rate": 1.875201126307321e-05,
      "loss": 1.0032,
      "step": 12410
    },
    {
      "epoch": 0.6244971842316975,
      "grad_norm": 56.25,
      "learning_rate": 1.8751005631536606e-05,
      "loss": 1.1519,
      "step": 12420
    },
    {
      "epoch": 0.625,
      "grad_norm": 17.625,
      "learning_rate": 1.8750000000000002e-05,
      "loss": 1.2147,
      "step": 12430
    },
    {
      "epoch": 0.6255028157683025,
      "grad_norm": 65.0,
      "learning_rate": 1.8748994368463394e-05,
      "loss": 1.0602,
      "step": 12440
    },
    {
      "epoch": 0.6260056315366049,
      "grad_norm": 21.5,
      "learning_rate": 1.8747988736926794e-05,
      "loss": 1.0031,
      "step": 12450
    },
    {
      "epoch": 0.6265084473049075,
      "grad_norm": 41.25,
      "learning_rate": 1.8746983105390186e-05,
      "loss": 1.2617,
      "step": 12460
    },
    {
      "epoch": 0.62701126307321,
      "grad_norm": 30.875,
      "learning_rate": 1.8745977473853582e-05,
      "loss": 1.0568,
      "step": 12470
    },
    {
      "epoch": 0.6275140788415124,
      "grad_norm": 52.25,
      "learning_rate": 1.8744971842316978e-05,
      "loss": 1.2168,
      "step": 12480
    },
    {
      "epoch": 0.6280168946098149,
      "grad_norm": 6.875,
      "learning_rate": 1.874396621078037e-05,
      "loss": 0.9234,
      "step": 12490
    },
    {
      "epoch": 0.6285197103781175,
      "grad_norm": 5.3125,
      "learning_rate": 1.8742960579243766e-05,
      "loss": 0.6878,
      "step": 12500
    },
    {
      "epoch": 0.6285197103781175,
      "eval_accuracy": 0.5118740704015865,
      "eval_loss": 1.033707618713379,
      "eval_runtime": 462.8188,
      "eval_samples_per_second": 87.162,
      "eval_steps_per_second": 87.162,
      "step": 12500
    },
    {
      "epoch": 0.62902252614642,
      "grad_norm": 26.0,
      "learning_rate": 1.8741954947707162e-05,
      "loss": 1.2172,
      "step": 12510
    },
    {
      "epoch": 0.6295253419147224,
      "grad_norm": 19.5,
      "learning_rate": 1.8740949316170554e-05,
      "loss": 0.9748,
      "step": 12520
    },
    {
      "epoch": 0.6300281576830249,
      "grad_norm": 8.875,
      "learning_rate": 1.8739943684633954e-05,
      "loss": 0.826,
      "step": 12530
    },
    {
      "epoch": 0.6305309734513275,
      "grad_norm": 7.78125,
      "learning_rate": 1.8738938053097346e-05,
      "loss": 0.8234,
      "step": 12540
    },
    {
      "epoch": 0.6310337892196299,
      "grad_norm": 12.8125,
      "learning_rate": 1.8737932421560742e-05,
      "loss": 0.9227,
      "step": 12550
    },
    {
      "epoch": 0.6315366049879324,
      "grad_norm": 17.0,
      "learning_rate": 1.8736926790024138e-05,
      "loss": 1.1403,
      "step": 12560
    },
    {
      "epoch": 0.6320394207562349,
      "grad_norm": 19.125,
      "learning_rate": 1.873592115848753e-05,
      "loss": 0.8703,
      "step": 12570
    },
    {
      "epoch": 0.6325422365245374,
      "grad_norm": 50.5,
      "learning_rate": 1.8734915526950926e-05,
      "loss": 1.0712,
      "step": 12580
    },
    {
      "epoch": 0.6330450522928399,
      "grad_norm": 15.5625,
      "learning_rate": 1.8733909895414322e-05,
      "loss": 1.3061,
      "step": 12590
    },
    {
      "epoch": 0.6335478680611424,
      "grad_norm": 7.1875,
      "learning_rate": 1.8732904263877715e-05,
      "loss": 1.1146,
      "step": 12600
    },
    {
      "epoch": 0.6340506838294448,
      "grad_norm": 7.65625,
      "learning_rate": 1.873189863234111e-05,
      "loss": 1.1465,
      "step": 12610
    },
    {
      "epoch": 0.6345534995977474,
      "grad_norm": 6.65625,
      "learning_rate": 1.8730893000804507e-05,
      "loss": 0.9772,
      "step": 12620
    },
    {
      "epoch": 0.6350563153660499,
      "grad_norm": 19.0,
      "learning_rate": 1.8729887369267902e-05,
      "loss": 0.9507,
      "step": 12630
    },
    {
      "epoch": 0.6355591311343524,
      "grad_norm": 63.75,
      "learning_rate": 1.8728881737731298e-05,
      "loss": 1.1855,
      "step": 12640
    },
    {
      "epoch": 0.6360619469026548,
      "grad_norm": 32.75,
      "learning_rate": 1.872787610619469e-05,
      "loss": 0.9497,
      "step": 12650
    },
    {
      "epoch": 0.6365647626709574,
      "grad_norm": 7.9375,
      "learning_rate": 1.8726870474658087e-05,
      "loss": 0.9029,
      "step": 12660
    },
    {
      "epoch": 0.6370675784392599,
      "grad_norm": 37.75,
      "learning_rate": 1.8725864843121483e-05,
      "loss": 1.1439,
      "step": 12670
    },
    {
      "epoch": 0.6375703942075623,
      "grad_norm": 20.5,
      "learning_rate": 1.8724859211584875e-05,
      "loss": 0.8592,
      "step": 12680
    },
    {
      "epoch": 0.6380732099758648,
      "grad_norm": 17.125,
      "learning_rate": 1.872385358004827e-05,
      "loss": 0.7343,
      "step": 12690
    },
    {
      "epoch": 0.6385760257441674,
      "grad_norm": 5.25,
      "learning_rate": 1.8722847948511667e-05,
      "loss": 1.2031,
      "step": 12700
    },
    {
      "epoch": 0.6390788415124699,
      "grad_norm": 56.5,
      "learning_rate": 1.8721842316975063e-05,
      "loss": 1.0962,
      "step": 12710
    },
    {
      "epoch": 0.6395816572807723,
      "grad_norm": 15.4375,
      "learning_rate": 1.872083668543846e-05,
      "loss": 0.843,
      "step": 12720
    },
    {
      "epoch": 0.6400844730490748,
      "grad_norm": 16.0,
      "learning_rate": 1.871983105390185e-05,
      "loss": 0.8351,
      "step": 12730
    },
    {
      "epoch": 0.6405872888173774,
      "grad_norm": 17.125,
      "learning_rate": 1.8718825422365247e-05,
      "loss": 1.245,
      "step": 12740
    },
    {
      "epoch": 0.6410901045856798,
      "grad_norm": 43.5,
      "learning_rate": 1.8717819790828643e-05,
      "loss": 1.4917,
      "step": 12750
    },
    {
      "epoch": 0.6415929203539823,
      "grad_norm": 38.0,
      "learning_rate": 1.8716814159292035e-05,
      "loss": 1.0593,
      "step": 12760
    },
    {
      "epoch": 0.6420957361222848,
      "grad_norm": 41.0,
      "learning_rate": 1.871580852775543e-05,
      "loss": 1.1859,
      "step": 12770
    },
    {
      "epoch": 0.6425985518905873,
      "grad_norm": 15.75,
      "learning_rate": 1.8714802896218827e-05,
      "loss": 1.4027,
      "step": 12780
    },
    {
      "epoch": 0.6431013676588898,
      "grad_norm": 34.5,
      "learning_rate": 1.8713797264682223e-05,
      "loss": 1.0395,
      "step": 12790
    },
    {
      "epoch": 0.6436041834271923,
      "grad_norm": 4.59375,
      "learning_rate": 1.871279163314562e-05,
      "loss": 1.0036,
      "step": 12800
    },
    {
      "epoch": 0.6441069991954947,
      "grad_norm": 8.8125,
      "learning_rate": 1.871178600160901e-05,
      "loss": 1.1277,
      "step": 12810
    },
    {
      "epoch": 0.6446098149637972,
      "grad_norm": 18.0,
      "learning_rate": 1.8710780370072407e-05,
      "loss": 1.2169,
      "step": 12820
    },
    {
      "epoch": 0.6451126307320998,
      "grad_norm": 11.375,
      "learning_rate": 1.8709774738535803e-05,
      "loss": 1.0882,
      "step": 12830
    },
    {
      "epoch": 0.6456154465004023,
      "grad_norm": 34.75,
      "learning_rate": 1.8708769106999196e-05,
      "loss": 1.0605,
      "step": 12840
    },
    {
      "epoch": 0.6461182622687047,
      "grad_norm": 8.9375,
      "learning_rate": 1.870776347546259e-05,
      "loss": 1.3268,
      "step": 12850
    },
    {
      "epoch": 0.6466210780370072,
      "grad_norm": 38.25,
      "learning_rate": 1.8706757843925987e-05,
      "loss": 0.823,
      "step": 12860
    },
    {
      "epoch": 0.6471238938053098,
      "grad_norm": 31.125,
      "learning_rate": 1.8705752212389383e-05,
      "loss": 1.1289,
      "step": 12870
    },
    {
      "epoch": 0.6476267095736122,
      "grad_norm": 23.375,
      "learning_rate": 1.8704746580852776e-05,
      "loss": 1.1454,
      "step": 12880
    },
    {
      "epoch": 0.6481295253419147,
      "grad_norm": 12.1875,
      "learning_rate": 1.870374094931617e-05,
      "loss": 0.7924,
      "step": 12890
    },
    {
      "epoch": 0.6486323411102172,
      "grad_norm": 20.5,
      "learning_rate": 1.8702735317779567e-05,
      "loss": 1.272,
      "step": 12900
    },
    {
      "epoch": 0.6491351568785197,
      "grad_norm": 19.25,
      "learning_rate": 1.8701729686242963e-05,
      "loss": 0.981,
      "step": 12910
    },
    {
      "epoch": 0.6496379726468222,
      "grad_norm": 45.0,
      "learning_rate": 1.8700724054706356e-05,
      "loss": 1.0984,
      "step": 12920
    },
    {
      "epoch": 0.6501407884151247,
      "grad_norm": 34.25,
      "learning_rate": 1.869971842316975e-05,
      "loss": 0.986,
      "step": 12930
    },
    {
      "epoch": 0.6506436041834271,
      "grad_norm": 17.375,
      "learning_rate": 1.8698712791633148e-05,
      "loss": 0.7826,
      "step": 12940
    },
    {
      "epoch": 0.6511464199517297,
      "grad_norm": 64.0,
      "learning_rate": 1.8697707160096543e-05,
      "loss": 0.9625,
      "step": 12950
    },
    {
      "epoch": 0.6516492357200322,
      "grad_norm": 19.625,
      "learning_rate": 1.8696701528559936e-05,
      "loss": 1.0627,
      "step": 12960
    },
    {
      "epoch": 0.6521520514883347,
      "grad_norm": 4.71875,
      "learning_rate": 1.8695695897023332e-05,
      "loss": 0.9697,
      "step": 12970
    },
    {
      "epoch": 0.6526548672566371,
      "grad_norm": 7.09375,
      "learning_rate": 1.8694690265486728e-05,
      "loss": 0.7422,
      "step": 12980
    },
    {
      "epoch": 0.6531576830249397,
      "grad_norm": 11.75,
      "learning_rate": 1.8693684633950124e-05,
      "loss": 0.9628,
      "step": 12990
    },
    {
      "epoch": 0.6536604987932422,
      "grad_norm": 9.125,
      "learning_rate": 1.8692679002413516e-05,
      "loss": 0.9721,
      "step": 13000
    },
    {
      "epoch": 0.6536604987932422,
      "eval_accuracy": 0.5121963311849281,
      "eval_loss": 1.031802773475647,
      "eval_runtime": 462.7587,
      "eval_samples_per_second": 87.173,
      "eval_steps_per_second": 87.173,
      "step": 13000
    },
    {
      "epoch": 0.6541633145615446,
      "grad_norm": 22.625,
      "learning_rate": 1.8691673370876912e-05,
      "loss": 0.8199,
      "step": 13010
    },
    {
      "epoch": 0.6546661303298471,
      "grad_norm": 94.0,
      "learning_rate": 1.8690667739340308e-05,
      "loss": 1.2703,
      "step": 13020
    },
    {
      "epoch": 0.6551689460981497,
      "grad_norm": 16.25,
      "learning_rate": 1.8689662107803704e-05,
      "loss": 0.7051,
      "step": 13030
    },
    {
      "epoch": 0.6556717618664522,
      "grad_norm": 31.0,
      "learning_rate": 1.8688656476267096e-05,
      "loss": 0.9251,
      "step": 13040
    },
    {
      "epoch": 0.6561745776347546,
      "grad_norm": 27.625,
      "learning_rate": 1.8687650844730492e-05,
      "loss": 0.7904,
      "step": 13050
    },
    {
      "epoch": 0.6566773934030571,
      "grad_norm": 15.0,
      "learning_rate": 1.8686645213193888e-05,
      "loss": 0.7843,
      "step": 13060
    },
    {
      "epoch": 0.6571802091713597,
      "grad_norm": 18.875,
      "learning_rate": 1.8685639581657284e-05,
      "loss": 0.8107,
      "step": 13070
    },
    {
      "epoch": 0.6576830249396621,
      "grad_norm": 92.0,
      "learning_rate": 1.8684633950120676e-05,
      "loss": 1.1434,
      "step": 13080
    },
    {
      "epoch": 0.6581858407079646,
      "grad_norm": 39.5,
      "learning_rate": 1.8683628318584072e-05,
      "loss": 1.1716,
      "step": 13090
    },
    {
      "epoch": 0.6586886564762671,
      "grad_norm": 39.5,
      "learning_rate": 1.8682622687047468e-05,
      "loss": 0.805,
      "step": 13100
    },
    {
      "epoch": 0.6591914722445696,
      "grad_norm": 9.625,
      "learning_rate": 1.8681617055510864e-05,
      "loss": 1.1102,
      "step": 13110
    },
    {
      "epoch": 0.6596942880128721,
      "grad_norm": 24.625,
      "learning_rate": 1.8680611423974256e-05,
      "loss": 0.7388,
      "step": 13120
    },
    {
      "epoch": 0.6601971037811746,
      "grad_norm": 21.5,
      "learning_rate": 1.8679605792437652e-05,
      "loss": 1.1485,
      "step": 13130
    },
    {
      "epoch": 0.660699919549477,
      "grad_norm": 35.5,
      "learning_rate": 1.8678600160901048e-05,
      "loss": 1.0864,
      "step": 13140
    },
    {
      "epoch": 0.6612027353177795,
      "grad_norm": 19.625,
      "learning_rate": 1.867759452936444e-05,
      "loss": 1.1206,
      "step": 13150
    },
    {
      "epoch": 0.6617055510860821,
      "grad_norm": 32.75,
      "learning_rate": 1.8676588897827837e-05,
      "loss": 0.8133,
      "step": 13160
    },
    {
      "epoch": 0.6622083668543846,
      "grad_norm": 5.8125,
      "learning_rate": 1.8675583266291232e-05,
      "loss": 1.0973,
      "step": 13170
    },
    {
      "epoch": 0.662711182622687,
      "grad_norm": 52.0,
      "learning_rate": 1.8674577634754628e-05,
      "loss": 1.2342,
      "step": 13180
    },
    {
      "epoch": 0.6632139983909895,
      "grad_norm": 29.125,
      "learning_rate": 1.8673572003218024e-05,
      "loss": 1.0618,
      "step": 13190
    },
    {
      "epoch": 0.6637168141592921,
      "grad_norm": 23.5,
      "learning_rate": 1.8672566371681417e-05,
      "loss": 0.867,
      "step": 13200
    },
    {
      "epoch": 0.6642196299275945,
      "grad_norm": 9.1875,
      "learning_rate": 1.8671560740144813e-05,
      "loss": 0.9183,
      "step": 13210
    },
    {
      "epoch": 0.664722445695897,
      "grad_norm": 44.25,
      "learning_rate": 1.867055510860821e-05,
      "loss": 0.9544,
      "step": 13220
    },
    {
      "epoch": 0.6652252614641995,
      "grad_norm": 7.40625,
      "learning_rate": 1.86695494770716e-05,
      "loss": 0.6139,
      "step": 13230
    },
    {
      "epoch": 0.665728077232502,
      "grad_norm": 47.0,
      "learning_rate": 1.8668543845534997e-05,
      "loss": 1.2582,
      "step": 13240
    },
    {
      "epoch": 0.6662308930008045,
      "grad_norm": 33.25,
      "learning_rate": 1.8667538213998393e-05,
      "loss": 1.0179,
      "step": 13250
    },
    {
      "epoch": 0.666733708769107,
      "grad_norm": 62.0,
      "learning_rate": 1.866653258246179e-05,
      "loss": 1.1932,
      "step": 13260
    },
    {
      "epoch": 0.6672365245374094,
      "grad_norm": 8.125,
      "learning_rate": 1.8665526950925184e-05,
      "loss": 0.9243,
      "step": 13270
    },
    {
      "epoch": 0.667739340305712,
      "grad_norm": 7.125,
      "learning_rate": 1.8664521319388577e-05,
      "loss": 1.0498,
      "step": 13280
    },
    {
      "epoch": 0.6682421560740145,
      "grad_norm": 13.3125,
      "learning_rate": 1.8663515687851973e-05,
      "loss": 1.031,
      "step": 13290
    },
    {
      "epoch": 0.668744971842317,
      "grad_norm": 6.3125,
      "learning_rate": 1.866251005631537e-05,
      "loss": 0.9012,
      "step": 13300
    },
    {
      "epoch": 0.6692477876106194,
      "grad_norm": 6.0625,
      "learning_rate": 1.866150442477876e-05,
      "loss": 1.1456,
      "step": 13310
    },
    {
      "epoch": 0.669750603378922,
      "grad_norm": 24.125,
      "learning_rate": 1.8660498793242157e-05,
      "loss": 0.8877,
      "step": 13320
    },
    {
      "epoch": 0.6702534191472245,
      "grad_norm": 11.375,
      "learning_rate": 1.8659493161705553e-05,
      "loss": 0.8331,
      "step": 13330
    },
    {
      "epoch": 0.6707562349155269,
      "grad_norm": 37.25,
      "learning_rate": 1.865848753016895e-05,
      "loss": 0.9967,
      "step": 13340
    },
    {
      "epoch": 0.6712590506838294,
      "grad_norm": 62.75,
      "learning_rate": 1.8657481898632345e-05,
      "loss": 0.8972,
      "step": 13350
    },
    {
      "epoch": 0.671761866452132,
      "grad_norm": 25.5,
      "learning_rate": 1.8656476267095737e-05,
      "loss": 1.3336,
      "step": 13360
    },
    {
      "epoch": 0.6722646822204345,
      "grad_norm": 15.75,
      "learning_rate": 1.8655470635559133e-05,
      "loss": 0.9743,
      "step": 13370
    },
    {
      "epoch": 0.6727674979887369,
      "grad_norm": 23.625,
      "learning_rate": 1.865446500402253e-05,
      "loss": 1.0329,
      "step": 13380
    },
    {
      "epoch": 0.6732703137570394,
      "grad_norm": 39.5,
      "learning_rate": 1.865345937248592e-05,
      "loss": 1.1817,
      "step": 13390
    },
    {
      "epoch": 0.673773129525342,
      "grad_norm": 11.4375,
      "learning_rate": 1.8652453740949317e-05,
      "loss": 0.7289,
      "step": 13400
    },
    {
      "epoch": 0.6742759452936444,
      "grad_norm": 25.0,
      "learning_rate": 1.8651448109412713e-05,
      "loss": 1.2439,
      "step": 13410
    },
    {
      "epoch": 0.6747787610619469,
      "grad_norm": 15.375,
      "learning_rate": 1.8650442477876106e-05,
      "loss": 1.1813,
      "step": 13420
    },
    {
      "epoch": 0.6752815768302494,
      "grad_norm": 102.0,
      "learning_rate": 1.8649436846339505e-05,
      "loss": 0.9507,
      "step": 13430
    },
    {
      "epoch": 0.6757843925985519,
      "grad_norm": 7.65625,
      "learning_rate": 1.8648431214802897e-05,
      "loss": 1.2928,
      "step": 13440
    },
    {
      "epoch": 0.6762872083668544,
      "grad_norm": 52.25,
      "learning_rate": 1.8647425583266293e-05,
      "loss": 0.8708,
      "step": 13450
    },
    {
      "epoch": 0.6767900241351569,
      "grad_norm": 21.875,
      "learning_rate": 1.864641995172969e-05,
      "loss": 0.7471,
      "step": 13460
    },
    {
      "epoch": 0.6772928399034593,
      "grad_norm": 29.625,
      "learning_rate": 1.864541432019308e-05,
      "loss": 0.8003,
      "step": 13470
    },
    {
      "epoch": 0.6777956556717619,
      "grad_norm": 6.0625,
      "learning_rate": 1.8644408688656478e-05,
      "loss": 0.8372,
      "step": 13480
    },
    {
      "epoch": 0.6782984714400644,
      "grad_norm": 14.5625,
      "learning_rate": 1.8643403057119873e-05,
      "loss": 0.9016,
      "step": 13490
    },
    {
      "epoch": 0.6788012872083669,
      "grad_norm": 14.4375,
      "learning_rate": 1.8642397425583266e-05,
      "loss": 0.9227,
      "step": 13500
    },
    {
      "epoch": 0.6788012872083669,
      "eval_accuracy": 0.510932077342588,
      "eval_loss": 1.0323444604873657,
      "eval_runtime": 461.9217,
      "eval_samples_per_second": 87.331,
      "eval_steps_per_second": 87.331,
      "step": 13500
    },
    {
      "epoch": 0.6793041029766693,
      "grad_norm": 27.375,
      "learning_rate": 1.8641391794046665e-05,
      "loss": 0.9622,
      "step": 13510
    },
    {
      "epoch": 0.6798069187449718,
      "grad_norm": 9.125,
      "learning_rate": 1.8640386162510058e-05,
      "loss": 0.8834,
      "step": 13520
    },
    {
      "epoch": 0.6803097345132744,
      "grad_norm": 6.34375,
      "learning_rate": 1.8639380530973454e-05,
      "loss": 0.9499,
      "step": 13530
    },
    {
      "epoch": 0.6808125502815768,
      "grad_norm": 9.625,
      "learning_rate": 1.863837489943685e-05,
      "loss": 0.9415,
      "step": 13540
    },
    {
      "epoch": 0.6813153660498793,
      "grad_norm": 43.25,
      "learning_rate": 1.8637369267900242e-05,
      "loss": 1.1731,
      "step": 13550
    },
    {
      "epoch": 0.6818181818181818,
      "grad_norm": 15.6875,
      "learning_rate": 1.8636363636363638e-05,
      "loss": 1.2233,
      "step": 13560
    },
    {
      "epoch": 0.6823209975864843,
      "grad_norm": 10.5625,
      "learning_rate": 1.8635358004827034e-05,
      "loss": 1.0328,
      "step": 13570
    },
    {
      "epoch": 0.6828238133547868,
      "grad_norm": 10.8125,
      "learning_rate": 1.8634352373290426e-05,
      "loss": 1.1615,
      "step": 13580
    },
    {
      "epoch": 0.6833266291230893,
      "grad_norm": 20.0,
      "learning_rate": 1.8633346741753825e-05,
      "loss": 0.7219,
      "step": 13590
    },
    {
      "epoch": 0.6838294448913917,
      "grad_norm": 37.0,
      "learning_rate": 1.8632341110217218e-05,
      "loss": 1.289,
      "step": 13600
    },
    {
      "epoch": 0.6843322606596943,
      "grad_norm": 29.25,
      "learning_rate": 1.8631335478680614e-05,
      "loss": 0.871,
      "step": 13610
    },
    {
      "epoch": 0.6848350764279968,
      "grad_norm": 15.3125,
      "learning_rate": 1.863032984714401e-05,
      "loss": 1.0844,
      "step": 13620
    },
    {
      "epoch": 0.6853378921962993,
      "grad_norm": 7.78125,
      "learning_rate": 1.8629324215607402e-05,
      "loss": 0.801,
      "step": 13630
    },
    {
      "epoch": 0.6858407079646017,
      "grad_norm": 11.875,
      "learning_rate": 1.8628318584070798e-05,
      "loss": 0.7915,
      "step": 13640
    },
    {
      "epoch": 0.6863435237329043,
      "grad_norm": 31.25,
      "learning_rate": 1.8627312952534194e-05,
      "loss": 1.0958,
      "step": 13650
    },
    {
      "epoch": 0.6868463395012068,
      "grad_norm": 80.0,
      "learning_rate": 1.8626307320997586e-05,
      "loss": 1.0332,
      "step": 13660
    },
    {
      "epoch": 0.6873491552695092,
      "grad_norm": 34.75,
      "learning_rate": 1.8625301689460982e-05,
      "loss": 1.3167,
      "step": 13670
    },
    {
      "epoch": 0.6878519710378117,
      "grad_norm": 31.375,
      "learning_rate": 1.8624296057924378e-05,
      "loss": 1.2158,
      "step": 13680
    },
    {
      "epoch": 0.6883547868061143,
      "grad_norm": 25.75,
      "learning_rate": 1.862329042638777e-05,
      "loss": 0.9274,
      "step": 13690
    },
    {
      "epoch": 0.6888576025744167,
      "grad_norm": 9.8125,
      "learning_rate": 1.862228479485117e-05,
      "loss": 0.7301,
      "step": 13700
    },
    {
      "epoch": 0.6893604183427192,
      "grad_norm": 24.875,
      "learning_rate": 1.8621279163314562e-05,
      "loss": 1.0061,
      "step": 13710
    },
    {
      "epoch": 0.6898632341110217,
      "grad_norm": 25.75,
      "learning_rate": 1.862027353177796e-05,
      "loss": 1.0051,
      "step": 13720
    },
    {
      "epoch": 0.6903660498793243,
      "grad_norm": 12.125,
      "learning_rate": 1.8619267900241354e-05,
      "loss": 1.0593,
      "step": 13730
    },
    {
      "epoch": 0.6908688656476267,
      "grad_norm": 8.9375,
      "learning_rate": 1.8618262268704747e-05,
      "loss": 0.5666,
      "step": 13740
    },
    {
      "epoch": 0.6913716814159292,
      "grad_norm": 35.0,
      "learning_rate": 1.8617256637168143e-05,
      "loss": 1.0133,
      "step": 13750
    },
    {
      "epoch": 0.6918744971842317,
      "grad_norm": 5.90625,
      "learning_rate": 1.861625100563154e-05,
      "loss": 0.7495,
      "step": 13760
    },
    {
      "epoch": 0.6923773129525342,
      "grad_norm": 17.5,
      "learning_rate": 1.861524537409493e-05,
      "loss": 1.0428,
      "step": 13770
    },
    {
      "epoch": 0.6928801287208367,
      "grad_norm": 58.5,
      "learning_rate": 1.861423974255833e-05,
      "loss": 1.2406,
      "step": 13780
    },
    {
      "epoch": 0.6933829444891392,
      "grad_norm": 31.0,
      "learning_rate": 1.8613234111021723e-05,
      "loss": 1.0103,
      "step": 13790
    },
    {
      "epoch": 0.6938857602574416,
      "grad_norm": 5.9375,
      "learning_rate": 1.861222847948512e-05,
      "loss": 1.1257,
      "step": 13800
    },
    {
      "epoch": 0.6943885760257442,
      "grad_norm": 54.0,
      "learning_rate": 1.8611222847948514e-05,
      "loss": 0.9919,
      "step": 13810
    },
    {
      "epoch": 0.6948913917940467,
      "grad_norm": 44.0,
      "learning_rate": 1.8610217216411907e-05,
      "loss": 0.9218,
      "step": 13820
    },
    {
      "epoch": 0.6953942075623492,
      "grad_norm": 25.125,
      "learning_rate": 1.8609211584875303e-05,
      "loss": 0.9679,
      "step": 13830
    },
    {
      "epoch": 0.6958970233306516,
      "grad_norm": 17.125,
      "learning_rate": 1.86082059533387e-05,
      "loss": 1.0946,
      "step": 13840
    },
    {
      "epoch": 0.6963998390989542,
      "grad_norm": 19.125,
      "learning_rate": 1.860720032180209e-05,
      "loss": 0.8294,
      "step": 13850
    },
    {
      "epoch": 0.6969026548672567,
      "grad_norm": 34.5,
      "learning_rate": 1.860619469026549e-05,
      "loss": 1.1483,
      "step": 13860
    },
    {
      "epoch": 0.6974054706355591,
      "grad_norm": 18.25,
      "learning_rate": 1.8605189058728883e-05,
      "loss": 0.9998,
      "step": 13870
    },
    {
      "epoch": 0.6979082864038616,
      "grad_norm": 53.5,
      "learning_rate": 1.860418342719228e-05,
      "loss": 1.0154,
      "step": 13880
    },
    {
      "epoch": 0.6984111021721641,
      "grad_norm": 25.0,
      "learning_rate": 1.8603177795655675e-05,
      "loss": 1.1831,
      "step": 13890
    },
    {
      "epoch": 0.6989139179404666,
      "grad_norm": 23.125,
      "learning_rate": 1.8602172164119067e-05,
      "loss": 0.9966,
      "step": 13900
    },
    {
      "epoch": 0.6994167337087691,
      "grad_norm": 11.9375,
      "learning_rate": 1.8601166532582463e-05,
      "loss": 0.7847,
      "step": 13910
    },
    {
      "epoch": 0.6999195494770716,
      "grad_norm": 15.125,
      "learning_rate": 1.860016090104586e-05,
      "loss": 0.9223,
      "step": 13920
    },
    {
      "epoch": 0.700422365245374,
      "grad_norm": 12.5625,
      "learning_rate": 1.859915526950925e-05,
      "loss": 1.0408,
      "step": 13930
    },
    {
      "epoch": 0.7009251810136766,
      "grad_norm": 58.0,
      "learning_rate": 1.8598149637972647e-05,
      "loss": 0.9072,
      "step": 13940
    },
    {
      "epoch": 0.7014279967819791,
      "grad_norm": 28.125,
      "learning_rate": 1.8597144006436043e-05,
      "loss": 0.6444,
      "step": 13950
    },
    {
      "epoch": 0.7019308125502816,
      "grad_norm": 32.0,
      "learning_rate": 1.8596138374899436e-05,
      "loss": 0.8683,
      "step": 13960
    },
    {
      "epoch": 0.702433628318584,
      "grad_norm": 10.0,
      "learning_rate": 1.8595132743362835e-05,
      "loss": 0.9581,
      "step": 13970
    },
    {
      "epoch": 0.7029364440868866,
      "grad_norm": 18.0,
      "learning_rate": 1.8594127111826227e-05,
      "loss": 1.1096,
      "step": 13980
    },
    {
      "epoch": 0.7034392598551891,
      "grad_norm": 19.875,
      "learning_rate": 1.8593121480289623e-05,
      "loss": 0.9573,
      "step": 13990
    },
    {
      "epoch": 0.7039420756234915,
      "grad_norm": 7.5,
      "learning_rate": 1.859211584875302e-05,
      "loss": 1.1005,
      "step": 14000
    },
    {
      "epoch": 0.7039420756234915,
      "eval_accuracy": 0.5113534952900347,
      "eval_loss": 1.030100703239441,
      "eval_runtime": 463.2207,
      "eval_samples_per_second": 87.086,
      "eval_steps_per_second": 87.086,
      "step": 14000
    },
    {
      "epoch": 0.704444891391794,
      "grad_norm": 41.75,
      "learning_rate": 1.8591110217216412e-05,
      "loss": 1.1537,
      "step": 14010
    },
    {
      "epoch": 0.7049477071600966,
      "grad_norm": 5.8125,
      "learning_rate": 1.8590104585679808e-05,
      "loss": 1.0649,
      "step": 14020
    },
    {
      "epoch": 0.705450522928399,
      "grad_norm": 14.125,
      "learning_rate": 1.8589098954143203e-05,
      "loss": 0.9864,
      "step": 14030
    },
    {
      "epoch": 0.7059533386967015,
      "grad_norm": 42.5,
      "learning_rate": 1.8588093322606596e-05,
      "loss": 0.8748,
      "step": 14040
    },
    {
      "epoch": 0.706456154465004,
      "grad_norm": 27.25,
      "learning_rate": 1.8587087691069995e-05,
      "loss": 0.966,
      "step": 14050
    },
    {
      "epoch": 0.7069589702333066,
      "grad_norm": 30.0,
      "learning_rate": 1.8586082059533388e-05,
      "loss": 0.9762,
      "step": 14060
    },
    {
      "epoch": 0.707461786001609,
      "grad_norm": 57.5,
      "learning_rate": 1.8585076427996784e-05,
      "loss": 1.528,
      "step": 14070
    },
    {
      "epoch": 0.7079646017699115,
      "grad_norm": 62.25,
      "learning_rate": 1.858407079646018e-05,
      "loss": 0.8634,
      "step": 14080
    },
    {
      "epoch": 0.708467417538214,
      "grad_norm": 18.875,
      "learning_rate": 1.8583065164923572e-05,
      "loss": 0.808,
      "step": 14090
    },
    {
      "epoch": 0.7089702333065165,
      "grad_norm": 12.375,
      "learning_rate": 1.8582059533386968e-05,
      "loss": 0.95,
      "step": 14100
    },
    {
      "epoch": 0.709473049074819,
      "grad_norm": 24.875,
      "learning_rate": 1.8581053901850364e-05,
      "loss": 0.8798,
      "step": 14110
    },
    {
      "epoch": 0.7099758648431215,
      "grad_norm": 5.15625,
      "learning_rate": 1.8580048270313756e-05,
      "loss": 0.7978,
      "step": 14120
    },
    {
      "epoch": 0.7104786806114239,
      "grad_norm": 37.5,
      "learning_rate": 1.8579042638777155e-05,
      "loss": 1.123,
      "step": 14130
    },
    {
      "epoch": 0.7109814963797265,
      "grad_norm": 18.25,
      "learning_rate": 1.8578037007240548e-05,
      "loss": 1.1545,
      "step": 14140
    },
    {
      "epoch": 0.711484312148029,
      "grad_norm": 34.0,
      "learning_rate": 1.8577031375703944e-05,
      "loss": 1.0842,
      "step": 14150
    },
    {
      "epoch": 0.7119871279163315,
      "grad_norm": 10.0,
      "learning_rate": 1.857602574416734e-05,
      "loss": 1.0354,
      "step": 14160
    },
    {
      "epoch": 0.7124899436846339,
      "grad_norm": 12.0,
      "learning_rate": 1.8575020112630732e-05,
      "loss": 0.7939,
      "step": 14170
    },
    {
      "epoch": 0.7129927594529365,
      "grad_norm": 13.0625,
      "learning_rate": 1.8574014481094128e-05,
      "loss": 1.1055,
      "step": 14180
    },
    {
      "epoch": 0.713495575221239,
      "grad_norm": 32.25,
      "learning_rate": 1.8573008849557524e-05,
      "loss": 0.9832,
      "step": 14190
    },
    {
      "epoch": 0.7139983909895414,
      "grad_norm": 16.5,
      "learning_rate": 1.8572003218020916e-05,
      "loss": 0.8253,
      "step": 14200
    },
    {
      "epoch": 0.7145012067578439,
      "grad_norm": 7.65625,
      "learning_rate": 1.8570997586484312e-05,
      "loss": 1.021,
      "step": 14210
    },
    {
      "epoch": 0.7150040225261464,
      "grad_norm": 22.875,
      "learning_rate": 1.8569991954947708e-05,
      "loss": 0.8142,
      "step": 14220
    },
    {
      "epoch": 0.7155068382944489,
      "grad_norm": 9.0625,
      "learning_rate": 1.8568986323411104e-05,
      "loss": 1.4501,
      "step": 14230
    },
    {
      "epoch": 0.7160096540627514,
      "grad_norm": 19.375,
      "learning_rate": 1.85679806918745e-05,
      "loss": 0.8696,
      "step": 14240
    },
    {
      "epoch": 0.7165124698310539,
      "grad_norm": 31.75,
      "learning_rate": 1.8566975060337892e-05,
      "loss": 0.8813,
      "step": 14250
    },
    {
      "epoch": 0.7170152855993563,
      "grad_norm": 8.9375,
      "learning_rate": 1.856596942880129e-05,
      "loss": 0.9293,
      "step": 14260
    },
    {
      "epoch": 0.7175181013676589,
      "grad_norm": 16.75,
      "learning_rate": 1.8564963797264684e-05,
      "loss": 0.8417,
      "step": 14270
    },
    {
      "epoch": 0.7180209171359614,
      "grad_norm": 39.75,
      "learning_rate": 1.856395816572808e-05,
      "loss": 1.0728,
      "step": 14280
    },
    {
      "epoch": 0.7185237329042639,
      "grad_norm": 11.625,
      "learning_rate": 1.8562952534191473e-05,
      "loss": 1.0961,
      "step": 14290
    },
    {
      "epoch": 0.7190265486725663,
      "grad_norm": 35.75,
      "learning_rate": 1.856194690265487e-05,
      "loss": 0.8579,
      "step": 14300
    },
    {
      "epoch": 0.7195293644408689,
      "grad_norm": 21.875,
      "learning_rate": 1.8560941271118264e-05,
      "loss": 1.2716,
      "step": 14310
    },
    {
      "epoch": 0.7200321802091714,
      "grad_norm": 21.375,
      "learning_rate": 1.855993563958166e-05,
      "loss": 0.9268,
      "step": 14320
    },
    {
      "epoch": 0.7205349959774738,
      "grad_norm": 5.5,
      "learning_rate": 1.8558930008045053e-05,
      "loss": 0.933,
      "step": 14330
    },
    {
      "epoch": 0.7210378117457763,
      "grad_norm": 38.0,
      "learning_rate": 1.855792437650845e-05,
      "loss": 1.3161,
      "step": 14340
    },
    {
      "epoch": 0.7215406275140789,
      "grad_norm": 90.0,
      "learning_rate": 1.8556918744971844e-05,
      "loss": 1.1467,
      "step": 14350
    },
    {
      "epoch": 0.7220434432823813,
      "grad_norm": 12.125,
      "learning_rate": 1.855591311343524e-05,
      "loss": 1.2752,
      "step": 14360
    },
    {
      "epoch": 0.7225462590506838,
      "grad_norm": 5.78125,
      "learning_rate": 1.8554907481898633e-05,
      "loss": 0.8789,
      "step": 14370
    },
    {
      "epoch": 0.7230490748189863,
      "grad_norm": 24.0,
      "learning_rate": 1.855390185036203e-05,
      "loss": 1.0457,
      "step": 14380
    },
    {
      "epoch": 0.7235518905872889,
      "grad_norm": 11.375,
      "learning_rate": 1.8552896218825425e-05,
      "loss": 0.7788,
      "step": 14390
    },
    {
      "epoch": 0.7240547063555913,
      "grad_norm": 59.0,
      "learning_rate": 1.855189058728882e-05,
      "loss": 1.1851,
      "step": 14400
    },
    {
      "epoch": 0.7245575221238938,
      "grad_norm": 9.0,
      "learning_rate": 1.8550884955752213e-05,
      "loss": 0.8034,
      "step": 14410
    },
    {
      "epoch": 0.7250603378921963,
      "grad_norm": 8.625,
      "learning_rate": 1.854987932421561e-05,
      "loss": 1.301,
      "step": 14420
    },
    {
      "epoch": 0.7255631536604988,
      "grad_norm": 25.625,
      "learning_rate": 1.8548873692679005e-05,
      "loss": 1.2742,
      "step": 14430
    },
    {
      "epoch": 0.7260659694288013,
      "grad_norm": 7.5625,
      "learning_rate": 1.85478680611424e-05,
      "loss": 0.9219,
      "step": 14440
    },
    {
      "epoch": 0.7265687851971038,
      "grad_norm": 21.0,
      "learning_rate": 1.8546862429605793e-05,
      "loss": 0.9952,
      "step": 14450
    },
    {
      "epoch": 0.7270716009654062,
      "grad_norm": 17.125,
      "learning_rate": 1.854585679806919e-05,
      "loss": 0.9337,
      "step": 14460
    },
    {
      "epoch": 0.7275744167337088,
      "grad_norm": 20.875,
      "learning_rate": 1.8544851166532585e-05,
      "loss": 1.1636,
      "step": 14470
    },
    {
      "epoch": 0.7280772325020113,
      "grad_norm": 10.125,
      "learning_rate": 1.8543845534995977e-05,
      "loss": 0.8086,
      "step": 14480
    },
    {
      "epoch": 0.7285800482703138,
      "grad_norm": 22.25,
      "learning_rate": 1.8542839903459373e-05,
      "loss": 0.8197,
      "step": 14490
    },
    {
      "epoch": 0.7290828640386162,
      "grad_norm": 45.0,
      "learning_rate": 1.854183427192277e-05,
      "loss": 1.087,
      "step": 14500
    },
    {
      "epoch": 0.7290828640386162,
      "eval_accuracy": 0.5115518096182449,
      "eval_loss": 1.0304903984069824,
      "eval_runtime": 462.3806,
      "eval_samples_per_second": 87.244,
      "eval_steps_per_second": 87.244,
      "step": 14500
    },
    {
      "epoch": 0.7295856798069188,
      "grad_norm": 23.875,
      "learning_rate": 1.8540828640386165e-05,
      "loss": 0.9876,
      "step": 14510
    },
    {
      "epoch": 0.7300884955752213,
      "grad_norm": 10.5625,
      "learning_rate": 1.853982300884956e-05,
      "loss": 1.0546,
      "step": 14520
    },
    {
      "epoch": 0.7305913113435237,
      "grad_norm": 82.5,
      "learning_rate": 1.8538817377312953e-05,
      "loss": 1.1393,
      "step": 14530
    },
    {
      "epoch": 0.7310941271118262,
      "grad_norm": 33.5,
      "learning_rate": 1.853781174577635e-05,
      "loss": 1.0272,
      "step": 14540
    },
    {
      "epoch": 0.7315969428801288,
      "grad_norm": 21.125,
      "learning_rate": 1.8536806114239745e-05,
      "loss": 0.9209,
      "step": 14550
    },
    {
      "epoch": 0.7320997586484312,
      "grad_norm": 62.0,
      "learning_rate": 1.8535800482703138e-05,
      "loss": 1.0285,
      "step": 14560
    },
    {
      "epoch": 0.7326025744167337,
      "grad_norm": 51.75,
      "learning_rate": 1.8534794851166533e-05,
      "loss": 0.9036,
      "step": 14570
    },
    {
      "epoch": 0.7331053901850362,
      "grad_norm": 13.625,
      "learning_rate": 1.853378921962993e-05,
      "loss": 0.9901,
      "step": 14580
    },
    {
      "epoch": 0.7336082059533386,
      "grad_norm": 23.75,
      "learning_rate": 1.8532783588093325e-05,
      "loss": 1.007,
      "step": 14590
    },
    {
      "epoch": 0.7341110217216412,
      "grad_norm": 6.71875,
      "learning_rate": 1.853177795655672e-05,
      "loss": 1.4053,
      "step": 14600
    },
    {
      "epoch": 0.7346138374899437,
      "grad_norm": 11.625,
      "learning_rate": 1.8530772325020114e-05,
      "loss": 0.9813,
      "step": 14610
    },
    {
      "epoch": 0.7351166532582462,
      "grad_norm": 8.5,
      "learning_rate": 1.852976669348351e-05,
      "loss": 1.1478,
      "step": 14620
    },
    {
      "epoch": 0.7356194690265486,
      "grad_norm": 16.125,
      "learning_rate": 1.8528761061946905e-05,
      "loss": 1.1858,
      "step": 14630
    },
    {
      "epoch": 0.7361222847948512,
      "grad_norm": 17.875,
      "learning_rate": 1.8527755430410298e-05,
      "loss": 0.9545,
      "step": 14640
    },
    {
      "epoch": 0.7366251005631537,
      "grad_norm": 31.5,
      "learning_rate": 1.8526749798873694e-05,
      "loss": 0.8781,
      "step": 14650
    },
    {
      "epoch": 0.7371279163314561,
      "grad_norm": 44.5,
      "learning_rate": 1.852574416733709e-05,
      "loss": 0.9256,
      "step": 14660
    },
    {
      "epoch": 0.7376307320997586,
      "grad_norm": 19.25,
      "learning_rate": 1.8524738535800486e-05,
      "loss": 0.8726,
      "step": 14670
    },
    {
      "epoch": 0.7381335478680612,
      "grad_norm": 10.8125,
      "learning_rate": 1.852373290426388e-05,
      "loss": 1.0437,
      "step": 14680
    },
    {
      "epoch": 0.7386363636363636,
      "grad_norm": 6.84375,
      "learning_rate": 1.8522727272727274e-05,
      "loss": 0.8256,
      "step": 14690
    },
    {
      "epoch": 0.7391391794046661,
      "grad_norm": 28.875,
      "learning_rate": 1.852172164119067e-05,
      "loss": 0.9751,
      "step": 14700
    },
    {
      "epoch": 0.7396419951729686,
      "grad_norm": 18.25,
      "learning_rate": 1.8520716009654066e-05,
      "loss": 0.931,
      "step": 14710
    },
    {
      "epoch": 0.7401448109412712,
      "grad_norm": 15.125,
      "learning_rate": 1.8519710378117458e-05,
      "loss": 0.9793,
      "step": 14720
    },
    {
      "epoch": 0.7406476267095736,
      "grad_norm": 9.25,
      "learning_rate": 1.8518704746580854e-05,
      "loss": 1.0711,
      "step": 14730
    },
    {
      "epoch": 0.7411504424778761,
      "grad_norm": 60.5,
      "learning_rate": 1.851769911504425e-05,
      "loss": 1.0638,
      "step": 14740
    },
    {
      "epoch": 0.7416532582461786,
      "grad_norm": 17.75,
      "learning_rate": 1.8516693483507642e-05,
      "loss": 0.9586,
      "step": 14750
    },
    {
      "epoch": 0.7421560740144811,
      "grad_norm": 14.5625,
      "learning_rate": 1.851568785197104e-05,
      "loss": 1.0803,
      "step": 14760
    },
    {
      "epoch": 0.7426588897827836,
      "grad_norm": 13.125,
      "learning_rate": 1.8514682220434434e-05,
      "loss": 0.7279,
      "step": 14770
    },
    {
      "epoch": 0.7431617055510861,
      "grad_norm": 8.5625,
      "learning_rate": 1.851367658889783e-05,
      "loss": 1.303,
      "step": 14780
    },
    {
      "epoch": 0.7436645213193885,
      "grad_norm": 36.75,
      "learning_rate": 1.8512670957361226e-05,
      "loss": 0.731,
      "step": 14790
    },
    {
      "epoch": 0.7441673370876911,
      "grad_norm": 30.5,
      "learning_rate": 1.851166532582462e-05,
      "loss": 1.1614,
      "step": 14800
    },
    {
      "epoch": 0.7446701528559936,
      "grad_norm": 14.0625,
      "learning_rate": 1.8510659694288014e-05,
      "loss": 0.6634,
      "step": 14810
    },
    {
      "epoch": 0.745172968624296,
      "grad_norm": 14.4375,
      "learning_rate": 1.850965406275141e-05,
      "loss": 1.0988,
      "step": 14820
    },
    {
      "epoch": 0.7456757843925985,
      "grad_norm": 24.75,
      "learning_rate": 1.8508648431214803e-05,
      "loss": 0.9588,
      "step": 14830
    },
    {
      "epoch": 0.7461786001609011,
      "grad_norm": 6.34375,
      "learning_rate": 1.8507642799678202e-05,
      "loss": 1.0428,
      "step": 14840
    },
    {
      "epoch": 0.7466814159292036,
      "grad_norm": 75.0,
      "learning_rate": 1.8506637168141594e-05,
      "loss": 1.1366,
      "step": 14850
    },
    {
      "epoch": 0.747184231697506,
      "grad_norm": 12.75,
      "learning_rate": 1.850563153660499e-05,
      "loss": 0.8348,
      "step": 14860
    },
    {
      "epoch": 0.7476870474658085,
      "grad_norm": 9.4375,
      "learning_rate": 1.8504625905068386e-05,
      "loss": 1.0447,
      "step": 14870
    },
    {
      "epoch": 0.7481898632341111,
      "grad_norm": 29.375,
      "learning_rate": 1.850362027353178e-05,
      "loss": 1.116,
      "step": 14880
    },
    {
      "epoch": 0.7486926790024135,
      "grad_norm": 33.0,
      "learning_rate": 1.8502614641995175e-05,
      "loss": 0.9166,
      "step": 14890
    },
    {
      "epoch": 0.749195494770716,
      "grad_norm": 37.25,
      "learning_rate": 1.850160901045857e-05,
      "loss": 1.0141,
      "step": 14900
    },
    {
      "epoch": 0.7496983105390185,
      "grad_norm": 11.6875,
      "learning_rate": 1.8500603378921963e-05,
      "loss": 0.8626,
      "step": 14910
    },
    {
      "epoch": 0.7502011263073209,
      "grad_norm": 7.4375,
      "learning_rate": 1.8499597747385362e-05,
      "loss": 0.7202,
      "step": 14920
    },
    {
      "epoch": 0.7507039420756235,
      "grad_norm": 37.25,
      "learning_rate": 1.8498592115848755e-05,
      "loss": 1.3069,
      "step": 14930
    },
    {
      "epoch": 0.751206757843926,
      "grad_norm": 45.25,
      "learning_rate": 1.849758648431215e-05,
      "loss": 0.9123,
      "step": 14940
    },
    {
      "epoch": 0.7517095736122285,
      "grad_norm": 12.1875,
      "learning_rate": 1.8496580852775546e-05,
      "loss": 0.9235,
      "step": 14950
    },
    {
      "epoch": 0.7522123893805309,
      "grad_norm": 31.875,
      "learning_rate": 1.849557522123894e-05,
      "loss": 1.4413,
      "step": 14960
    },
    {
      "epoch": 0.7527152051488335,
      "grad_norm": 16.875,
      "learning_rate": 1.8494569589702335e-05,
      "loss": 1.074,
      "step": 14970
    },
    {
      "epoch": 0.753218020917136,
      "grad_norm": 7.375,
      "learning_rate": 1.849356395816573e-05,
      "loss": 0.9867,
      "step": 14980
    },
    {
      "epoch": 0.7537208366854384,
      "grad_norm": 6.5625,
      "learning_rate": 1.8492558326629123e-05,
      "loss": 0.7498,
      "step": 14990
    },
    {
      "epoch": 0.7542236524537409,
      "grad_norm": 53.75,
      "learning_rate": 1.849155269509252e-05,
      "loss": 1.045,
      "step": 15000
    },
    {
      "epoch": 0.7542236524537409,
      "eval_accuracy": 0.5123450669310857,
      "eval_loss": 1.0296765565872192,
      "eval_runtime": 462.1831,
      "eval_samples_per_second": 87.281,
      "eval_steps_per_second": 87.281,
      "step": 15000
    },
    {
      "epoch": 0.7547264682220435,
      "grad_norm": 34.25,
      "learning_rate": 1.8490547063555915e-05,
      "loss": 1.3088,
      "step": 15010
    },
    {
      "epoch": 0.755229283990346,
      "grad_norm": 61.0,
      "learning_rate": 1.8489541432019307e-05,
      "loss": 0.9974,
      "step": 15020
    },
    {
      "epoch": 0.7557320997586484,
      "grad_norm": 27.5,
      "learning_rate": 1.8488535800482707e-05,
      "loss": 0.7487,
      "step": 15030
    },
    {
      "epoch": 0.7562349155269509,
      "grad_norm": 19.75,
      "learning_rate": 1.84875301689461e-05,
      "loss": 0.7547,
      "step": 15040
    },
    {
      "epoch": 0.7567377312952535,
      "grad_norm": 16.0,
      "learning_rate": 1.8486524537409495e-05,
      "loss": 0.9791,
      "step": 15050
    },
    {
      "epoch": 0.7572405470635559,
      "grad_norm": 22.75,
      "learning_rate": 1.848551890587289e-05,
      "loss": 1.3102,
      "step": 15060
    },
    {
      "epoch": 0.7577433628318584,
      "grad_norm": 12.8125,
      "learning_rate": 1.8484513274336283e-05,
      "loss": 0.8489,
      "step": 15070
    },
    {
      "epoch": 0.7582461786001609,
      "grad_norm": 38.5,
      "learning_rate": 1.848350764279968e-05,
      "loss": 1.138,
      "step": 15080
    },
    {
      "epoch": 0.7587489943684634,
      "grad_norm": 8.1875,
      "learning_rate": 1.8482502011263075e-05,
      "loss": 0.9184,
      "step": 15090
    },
    {
      "epoch": 0.7592518101367659,
      "grad_norm": 27.5,
      "learning_rate": 1.8481496379726468e-05,
      "loss": 0.9464,
      "step": 15100
    },
    {
      "epoch": 0.7597546259050684,
      "grad_norm": 28.75,
      "learning_rate": 1.8480490748189867e-05,
      "loss": 1.0403,
      "step": 15110
    },
    {
      "epoch": 0.7602574416733708,
      "grad_norm": 23.625,
      "learning_rate": 1.847948511665326e-05,
      "loss": 0.9567,
      "step": 15120
    },
    {
      "epoch": 0.7607602574416734,
      "grad_norm": 6.65625,
      "learning_rate": 1.8478479485116655e-05,
      "loss": 0.6304,
      "step": 15130
    },
    {
      "epoch": 0.7612630732099759,
      "grad_norm": 73.0,
      "learning_rate": 1.847747385358005e-05,
      "loss": 1.1861,
      "step": 15140
    },
    {
      "epoch": 0.7617658889782783,
      "grad_norm": 12.125,
      "learning_rate": 1.8476468222043444e-05,
      "loss": 1.237,
      "step": 15150
    },
    {
      "epoch": 0.7622687047465808,
      "grad_norm": 9.0625,
      "learning_rate": 1.847546259050684e-05,
      "loss": 1.0577,
      "step": 15160
    },
    {
      "epoch": 0.7627715205148834,
      "grad_norm": 37.75,
      "learning_rate": 1.8474456958970235e-05,
      "loss": 0.951,
      "step": 15170
    },
    {
      "epoch": 0.7632743362831859,
      "grad_norm": 11.8125,
      "learning_rate": 1.8473451327433628e-05,
      "loss": 1.2675,
      "step": 15180
    },
    {
      "epoch": 0.7637771520514883,
      "grad_norm": 14.625,
      "learning_rate": 1.8472445695897027e-05,
      "loss": 0.8958,
      "step": 15190
    },
    {
      "epoch": 0.7642799678197908,
      "grad_norm": 19.0,
      "learning_rate": 1.847144006436042e-05,
      "loss": 1.01,
      "step": 15200
    },
    {
      "epoch": 0.7647827835880934,
      "grad_norm": 26.125,
      "learning_rate": 1.8470434432823816e-05,
      "loss": 1.1511,
      "step": 15210
    },
    {
      "epoch": 0.7652855993563958,
      "grad_norm": 52.0,
      "learning_rate": 1.846942880128721e-05,
      "loss": 0.8142,
      "step": 15220
    },
    {
      "epoch": 0.7657884151246983,
      "grad_norm": 19.625,
      "learning_rate": 1.8468423169750604e-05,
      "loss": 0.9245,
      "step": 15230
    },
    {
      "epoch": 0.7662912308930008,
      "grad_norm": 12.5625,
      "learning_rate": 1.8467417538214e-05,
      "loss": 0.983,
      "step": 15240
    },
    {
      "epoch": 0.7667940466613034,
      "grad_norm": 34.75,
      "learning_rate": 1.8466411906677396e-05,
      "loss": 0.7909,
      "step": 15250
    },
    {
      "epoch": 0.7672968624296058,
      "grad_norm": 19.75,
      "learning_rate": 1.8465406275140788e-05,
      "loss": 1.0142,
      "step": 15260
    },
    {
      "epoch": 0.7677996781979083,
      "grad_norm": 17.875,
      "learning_rate": 1.8464400643604184e-05,
      "loss": 1.0054,
      "step": 15270
    },
    {
      "epoch": 0.7683024939662108,
      "grad_norm": 5.09375,
      "learning_rate": 1.846339501206758e-05,
      "loss": 0.9838,
      "step": 15280
    },
    {
      "epoch": 0.7688053097345132,
      "grad_norm": 9.625,
      "learning_rate": 1.8462389380530972e-05,
      "loss": 1.2101,
      "step": 15290
    },
    {
      "epoch": 0.7693081255028158,
      "grad_norm": 40.5,
      "learning_rate": 1.846138374899437e-05,
      "loss": 0.8731,
      "step": 15300
    },
    {
      "epoch": 0.7698109412711183,
      "grad_norm": 8.125,
      "learning_rate": 1.8460378117457764e-05,
      "loss": 0.838,
      "step": 15310
    },
    {
      "epoch": 0.7703137570394207,
      "grad_norm": 9.9375,
      "learning_rate": 1.845937248592116e-05,
      "loss": 0.9281,
      "step": 15320
    },
    {
      "epoch": 0.7708165728077232,
      "grad_norm": 9.0,
      "learning_rate": 1.8458366854384556e-05,
      "loss": 0.7506,
      "step": 15330
    },
    {
      "epoch": 0.7713193885760258,
      "grad_norm": 30.125,
      "learning_rate": 1.845736122284795e-05,
      "loss": 1.3884,
      "step": 15340
    },
    {
      "epoch": 0.7718222043443282,
      "grad_norm": 45.75,
      "learning_rate": 1.8456355591311344e-05,
      "loss": 0.9122,
      "step": 15350
    },
    {
      "epoch": 0.7723250201126307,
      "grad_norm": 24.75,
      "learning_rate": 1.845534995977474e-05,
      "loss": 1.3035,
      "step": 15360
    },
    {
      "epoch": 0.7728278358809332,
      "grad_norm": 39.5,
      "learning_rate": 1.8454344328238133e-05,
      "loss": 0.9214,
      "step": 15370
    },
    {
      "epoch": 0.7733306516492358,
      "grad_norm": 25.25,
      "learning_rate": 1.8453338696701532e-05,
      "loss": 0.9201,
      "step": 15380
    },
    {
      "epoch": 0.7738334674175382,
      "grad_norm": 11.0625,
      "learning_rate": 1.8452333065164924e-05,
      "loss": 0.9759,
      "step": 15390
    },
    {
      "epoch": 0.7743362831858407,
      "grad_norm": 13.0625,
      "learning_rate": 1.845132743362832e-05,
      "loss": 1.1604,
      "step": 15400
    },
    {
      "epoch": 0.7748390989541432,
      "grad_norm": 30.625,
      "learning_rate": 1.8450321802091716e-05,
      "loss": 1.281,
      "step": 15410
    },
    {
      "epoch": 0.7753419147224457,
      "grad_norm": 5.9375,
      "learning_rate": 1.844931617055511e-05,
      "loss": 1.1496,
      "step": 15420
    },
    {
      "epoch": 0.7758447304907482,
      "grad_norm": 37.75,
      "learning_rate": 1.8448310539018505e-05,
      "loss": 0.9043,
      "step": 15430
    },
    {
      "epoch": 0.7763475462590507,
      "grad_norm": 5.84375,
      "learning_rate": 1.84473049074819e-05,
      "loss": 1.1902,
      "step": 15440
    },
    {
      "epoch": 0.7768503620273531,
      "grad_norm": 14.5,
      "learning_rate": 1.8446299275945293e-05,
      "loss": 1.0369,
      "step": 15450
    },
    {
      "epoch": 0.7773531777956557,
      "grad_norm": 9.0625,
      "learning_rate": 1.8445293644408692e-05,
      "loss": 0.7428,
      "step": 15460
    },
    {
      "epoch": 0.7778559935639582,
      "grad_norm": 27.125,
      "learning_rate": 1.8444288012872085e-05,
      "loss": 1.0334,
      "step": 15470
    },
    {
      "epoch": 0.7783588093322606,
      "grad_norm": 25.125,
      "learning_rate": 1.844328238133548e-05,
      "loss": 1.0755,
      "step": 15480
    },
    {
      "epoch": 0.7788616251005631,
      "grad_norm": 25.125,
      "learning_rate": 1.8442276749798876e-05,
      "loss": 0.9872,
      "step": 15490
    },
    {
      "epoch": 0.7793644408688657,
      "grad_norm": 10.6875,
      "learning_rate": 1.844127111826227e-05,
      "loss": 1.0176,
      "step": 15500
    },
    {
      "epoch": 0.7793644408688657,
      "eval_accuracy": 0.5121715418939018,
      "eval_loss": 1.0302051305770874,
      "eval_runtime": 461.3059,
      "eval_samples_per_second": 87.447,
      "eval_steps_per_second": 87.447,
      "step": 15500
    },
    {
      "epoch": 0.7798672566371682,
      "grad_norm": 13.125,
      "learning_rate": 1.8440265486725665e-05,
      "loss": 0.9188,
      "step": 15510
    },
    {
      "epoch": 0.7803700724054706,
      "grad_norm": 29.875,
      "learning_rate": 1.843925985518906e-05,
      "loss": 0.8956,
      "step": 15520
    },
    {
      "epoch": 0.7808728881737731,
      "grad_norm": 29.625,
      "learning_rate": 1.8438254223652453e-05,
      "loss": 1.0843,
      "step": 15530
    },
    {
      "epoch": 0.7813757039420757,
      "grad_norm": 9.0625,
      "learning_rate": 1.843724859211585e-05,
      "loss": 0.7549,
      "step": 15540
    },
    {
      "epoch": 0.7818785197103781,
      "grad_norm": 69.5,
      "learning_rate": 1.8436242960579245e-05,
      "loss": 1.3544,
      "step": 15550
    },
    {
      "epoch": 0.7823813354786806,
      "grad_norm": 5.90625,
      "learning_rate": 1.843523732904264e-05,
      "loss": 0.8588,
      "step": 15560
    },
    {
      "epoch": 0.7828841512469831,
      "grad_norm": 47.75,
      "learning_rate": 1.8434231697506037e-05,
      "loss": 1.0913,
      "step": 15570
    },
    {
      "epoch": 0.7833869670152857,
      "grad_norm": 15.4375,
      "learning_rate": 1.843322606596943e-05,
      "loss": 0.9026,
      "step": 15580
    },
    {
      "epoch": 0.7838897827835881,
      "grad_norm": 46.5,
      "learning_rate": 1.8432220434432825e-05,
      "loss": 0.791,
      "step": 15590
    },
    {
      "epoch": 0.7843925985518906,
      "grad_norm": 17.75,
      "learning_rate": 1.843121480289622e-05,
      "loss": 1.2057,
      "step": 15600
    },
    {
      "epoch": 0.784895414320193,
      "grad_norm": 31.625,
      "learning_rate": 1.8430209171359613e-05,
      "loss": 0.6408,
      "step": 15610
    },
    {
      "epoch": 0.7853982300884956,
      "grad_norm": 25.25,
      "learning_rate": 1.842920353982301e-05,
      "loss": 0.9845,
      "step": 15620
    },
    {
      "epoch": 0.7859010458567981,
      "grad_norm": 39.75,
      "learning_rate": 1.8428197908286405e-05,
      "loss": 1.3269,
      "step": 15630
    },
    {
      "epoch": 0.7864038616251006,
      "grad_norm": 6.90625,
      "learning_rate": 1.84271922767498e-05,
      "loss": 1.187,
      "step": 15640
    },
    {
      "epoch": 0.786906677393403,
      "grad_norm": 23.875,
      "learning_rate": 1.8426186645213197e-05,
      "loss": 1.2489,
      "step": 15650
    },
    {
      "epoch": 0.7874094931617055,
      "grad_norm": 13.6875,
      "learning_rate": 1.842518101367659e-05,
      "loss": 1.0186,
      "step": 15660
    },
    {
      "epoch": 0.7879123089300081,
      "grad_norm": 14.5625,
      "learning_rate": 1.8424175382139985e-05,
      "loss": 0.8269,
      "step": 15670
    },
    {
      "epoch": 0.7884151246983105,
      "grad_norm": 37.5,
      "learning_rate": 1.842316975060338e-05,
      "loss": 1.2449,
      "step": 15680
    },
    {
      "epoch": 0.788917940466613,
      "grad_norm": 8.25,
      "learning_rate": 1.8422164119066774e-05,
      "loss": 0.943,
      "step": 15690
    },
    {
      "epoch": 0.7894207562349155,
      "grad_norm": 19.25,
      "learning_rate": 1.842115848753017e-05,
      "loss": 1.4987,
      "step": 15700
    },
    {
      "epoch": 0.789923572003218,
      "grad_norm": 40.25,
      "learning_rate": 1.8420152855993565e-05,
      "loss": 1.0566,
      "step": 15710
    },
    {
      "epoch": 0.7904263877715205,
      "grad_norm": 11.3125,
      "learning_rate": 1.841914722445696e-05,
      "loss": 0.9932,
      "step": 15720
    },
    {
      "epoch": 0.790929203539823,
      "grad_norm": 13.875,
      "learning_rate": 1.8418141592920357e-05,
      "loss": 0.747,
      "step": 15730
    },
    {
      "epoch": 0.7914320193081255,
      "grad_norm": 52.25,
      "learning_rate": 1.841713596138375e-05,
      "loss": 1.1804,
      "step": 15740
    },
    {
      "epoch": 0.791934835076428,
      "grad_norm": 7.21875,
      "learning_rate": 1.8416130329847146e-05,
      "loss": 1.0235,
      "step": 15750
    },
    {
      "epoch": 0.7924376508447305,
      "grad_norm": 24.75,
      "learning_rate": 1.841512469831054e-05,
      "loss": 0.9489,
      "step": 15760
    },
    {
      "epoch": 0.792940466613033,
      "grad_norm": 67.5,
      "learning_rate": 1.8414119066773934e-05,
      "loss": 0.8536,
      "step": 15770
    },
    {
      "epoch": 0.7934432823813354,
      "grad_norm": 36.5,
      "learning_rate": 1.841311343523733e-05,
      "loss": 0.9196,
      "step": 15780
    },
    {
      "epoch": 0.793946098149638,
      "grad_norm": 59.5,
      "learning_rate": 1.8412107803700726e-05,
      "loss": 1.2421,
      "step": 15790
    },
    {
      "epoch": 0.7944489139179405,
      "grad_norm": 6.53125,
      "learning_rate": 1.841110217216412e-05,
      "loss": 0.9862,
      "step": 15800
    },
    {
      "epoch": 0.794951729686243,
      "grad_norm": 10.25,
      "learning_rate": 1.8410096540627514e-05,
      "loss": 0.8791,
      "step": 15810
    },
    {
      "epoch": 0.7954545454545454,
      "grad_norm": 30.875,
      "learning_rate": 1.840909090909091e-05,
      "loss": 0.9496,
      "step": 15820
    },
    {
      "epoch": 0.795957361222848,
      "grad_norm": 31.875,
      "learning_rate": 1.8408085277554306e-05,
      "loss": 1.3875,
      "step": 15830
    },
    {
      "epoch": 0.7964601769911505,
      "grad_norm": 5.84375,
      "learning_rate": 1.8407079646017702e-05,
      "loss": 1.1654,
      "step": 15840
    },
    {
      "epoch": 0.7969629927594529,
      "grad_norm": 26.75,
      "learning_rate": 1.8406074014481094e-05,
      "loss": 0.8994,
      "step": 15850
    },
    {
      "epoch": 0.7974658085277554,
      "grad_norm": 60.0,
      "learning_rate": 1.840506838294449e-05,
      "loss": 1.2769,
      "step": 15860
    },
    {
      "epoch": 0.797968624296058,
      "grad_norm": 22.625,
      "learning_rate": 1.8404062751407886e-05,
      "loss": 0.949,
      "step": 15870
    },
    {
      "epoch": 0.7984714400643604,
      "grad_norm": 14.5625,
      "learning_rate": 1.8403057119871282e-05,
      "loss": 1.2673,
      "step": 15880
    },
    {
      "epoch": 0.7989742558326629,
      "grad_norm": 9.4375,
      "learning_rate": 1.8402051488334674e-05,
      "loss": 0.7867,
      "step": 15890
    },
    {
      "epoch": 0.7994770716009654,
      "grad_norm": 88.0,
      "learning_rate": 1.840104585679807e-05,
      "loss": 1.1961,
      "step": 15900
    },
    {
      "epoch": 0.799979887369268,
      "grad_norm": 12.125,
      "learning_rate": 1.8400040225261466e-05,
      "loss": 0.9253,
      "step": 15910
    },
    {
      "epoch": 0.8004827031375704,
      "grad_norm": 83.0,
      "learning_rate": 1.8399034593724862e-05,
      "loss": 1.0468,
      "step": 15920
    },
    {
      "epoch": 0.8009855189058729,
      "grad_norm": 9.75,
      "learning_rate": 1.8398028962188254e-05,
      "loss": 0.9049,
      "step": 15930
    },
    {
      "epoch": 0.8014883346741754,
      "grad_norm": 11.4375,
      "learning_rate": 1.839702333065165e-05,
      "loss": 1.0809,
      "step": 15940
    },
    {
      "epoch": 0.8019911504424779,
      "grad_norm": 13.1875,
      "learning_rate": 1.8396017699115046e-05,
      "loss": 0.9479,
      "step": 15950
    },
    {
      "epoch": 0.8024939662107804,
      "grad_norm": 6.59375,
      "learning_rate": 1.8395012067578442e-05,
      "loss": 0.9082,
      "step": 15960
    },
    {
      "epoch": 0.8029967819790829,
      "grad_norm": 52.5,
      "learning_rate": 1.8394006436041835e-05,
      "loss": 1.0834,
      "step": 15970
    },
    {
      "epoch": 0.8034995977473853,
      "grad_norm": 17.5,
      "learning_rate": 1.839300080450523e-05,
      "loss": 0.9888,
      "step": 15980
    },
    {
      "epoch": 0.8040024135156878,
      "grad_norm": 14.9375,
      "learning_rate": 1.8391995172968626e-05,
      "loss": 0.8679,
      "step": 15990
    },
    {
      "epoch": 0.8045052292839904,
      "grad_norm": 56.75,
      "learning_rate": 1.8390989541432022e-05,
      "loss": 1.0969,
      "step": 16000
    },
    {
      "epoch": 0.8045052292839904,
      "eval_accuracy": 0.5121219633118492,
      "eval_loss": 1.0300852060317993,
      "eval_runtime": 463.0004,
      "eval_samples_per_second": 87.127,
      "eval_steps_per_second": 87.127,
      "step": 16000
    },
    {
      "epoch": 0.8050080450522928,
      "grad_norm": 25.125,
      "learning_rate": 1.8389983909895418e-05,
      "loss": 1.0316,
      "step": 16010
    },
    {
      "epoch": 0.8055108608205953,
      "grad_norm": 10.25,
      "learning_rate": 1.838897827835881e-05,
      "loss": 0.8322,
      "step": 16020
    },
    {
      "epoch": 0.8060136765888978,
      "grad_norm": 88.5,
      "learning_rate": 1.8387972646822206e-05,
      "loss": 1.2985,
      "step": 16030
    },
    {
      "epoch": 0.8065164923572004,
      "grad_norm": 52.5,
      "learning_rate": 1.8386967015285602e-05,
      "loss": 1.0409,
      "step": 16040
    },
    {
      "epoch": 0.8070193081255028,
      "grad_norm": 79.0,
      "learning_rate": 1.8385961383748995e-05,
      "loss": 0.9334,
      "step": 16050
    },
    {
      "epoch": 0.8075221238938053,
      "grad_norm": 13.5625,
      "learning_rate": 1.838495575221239e-05,
      "loss": 1.0365,
      "step": 16060
    },
    {
      "epoch": 0.8080249396621078,
      "grad_norm": 21.125,
      "learning_rate": 1.8383950120675787e-05,
      "loss": 1.1605,
      "step": 16070
    },
    {
      "epoch": 0.8085277554304103,
      "grad_norm": 19.625,
      "learning_rate": 1.838294448913918e-05,
      "loss": 1.0843,
      "step": 16080
    },
    {
      "epoch": 0.8090305711987128,
      "grad_norm": 25.125,
      "learning_rate": 1.838193885760258e-05,
      "loss": 1.1091,
      "step": 16090
    },
    {
      "epoch": 0.8095333869670153,
      "grad_norm": 45.5,
      "learning_rate": 1.838093322606597e-05,
      "loss": 1.0225,
      "step": 16100
    },
    {
      "epoch": 0.8100362027353177,
      "grad_norm": 4.46875,
      "learning_rate": 1.8379927594529367e-05,
      "loss": 1.1937,
      "step": 16110
    },
    {
      "epoch": 0.8105390185036203,
      "grad_norm": 20.75,
      "learning_rate": 1.8378921962992763e-05,
      "loss": 0.9881,
      "step": 16120
    },
    {
      "epoch": 0.8110418342719228,
      "grad_norm": 28.75,
      "learning_rate": 1.8377916331456155e-05,
      "loss": 1.2092,
      "step": 16130
    },
    {
      "epoch": 0.8115446500402252,
      "grad_norm": 14.25,
      "learning_rate": 1.837691069991955e-05,
      "loss": 0.8522,
      "step": 16140
    },
    {
      "epoch": 0.8120474658085277,
      "grad_norm": 48.5,
      "learning_rate": 1.8375905068382947e-05,
      "loss": 0.7155,
      "step": 16150
    },
    {
      "epoch": 0.8125502815768303,
      "grad_norm": 22.0,
      "learning_rate": 1.837489943684634e-05,
      "loss": 0.9676,
      "step": 16160
    },
    {
      "epoch": 0.8130530973451328,
      "grad_norm": 26.75,
      "learning_rate": 1.837389380530974e-05,
      "loss": 0.8515,
      "step": 16170
    },
    {
      "epoch": 0.8135559131134352,
      "grad_norm": 6.6875,
      "learning_rate": 1.837288817377313e-05,
      "loss": 0.8802,
      "step": 16180
    },
    {
      "epoch": 0.8140587288817377,
      "grad_norm": 45.5,
      "learning_rate": 1.8371882542236527e-05,
      "loss": 1.3041,
      "step": 16190
    },
    {
      "epoch": 0.8145615446500403,
      "grad_norm": 40.5,
      "learning_rate": 1.8370876910699923e-05,
      "loss": 0.9752,
      "step": 16200
    },
    {
      "epoch": 0.8150643604183427,
      "grad_norm": 15.1875,
      "learning_rate": 1.8369871279163315e-05,
      "loss": 0.8569,
      "step": 16210
    },
    {
      "epoch": 0.8155671761866452,
      "grad_norm": 50.5,
      "learning_rate": 1.836886564762671e-05,
      "loss": 1.1208,
      "step": 16220
    },
    {
      "epoch": 0.8160699919549477,
      "grad_norm": 11.0625,
      "learning_rate": 1.8367860016090107e-05,
      "loss": 1.0629,
      "step": 16230
    },
    {
      "epoch": 0.8165728077232502,
      "grad_norm": 64.5,
      "learning_rate": 1.83668543845535e-05,
      "loss": 1.1629,
      "step": 16240
    },
    {
      "epoch": 0.8170756234915527,
      "grad_norm": 19.0,
      "learning_rate": 1.83658487530169e-05,
      "loss": 0.7962,
      "step": 16250
    },
    {
      "epoch": 0.8175784392598552,
      "grad_norm": 23.75,
      "learning_rate": 1.836484312148029e-05,
      "loss": 1.5389,
      "step": 16260
    },
    {
      "epoch": 0.8180812550281577,
      "grad_norm": 15.3125,
      "learning_rate": 1.8363837489943687e-05,
      "loss": 0.838,
      "step": 16270
    },
    {
      "epoch": 0.8185840707964602,
      "grad_norm": 35.5,
      "learning_rate": 1.8362831858407083e-05,
      "loss": 0.9173,
      "step": 16280
    },
    {
      "epoch": 0.8190868865647627,
      "grad_norm": 23.0,
      "learning_rate": 1.8361826226870476e-05,
      "loss": 1.2171,
      "step": 16290
    },
    {
      "epoch": 0.8195897023330652,
      "grad_norm": 33.25,
      "learning_rate": 1.836082059533387e-05,
      "loss": 1.1236,
      "step": 16300
    },
    {
      "epoch": 0.8200925181013676,
      "grad_norm": 28.75,
      "learning_rate": 1.8359814963797267e-05,
      "loss": 0.9621,
      "step": 16310
    },
    {
      "epoch": 0.8205953338696702,
      "grad_norm": 3.640625,
      "learning_rate": 1.835880933226066e-05,
      "loss": 1.053,
      "step": 16320
    },
    {
      "epoch": 0.8210981496379727,
      "grad_norm": 41.75,
      "learning_rate": 1.8357803700724056e-05,
      "loss": 1.2255,
      "step": 16330
    },
    {
      "epoch": 0.8216009654062751,
      "grad_norm": 14.1875,
      "learning_rate": 1.835679806918745e-05,
      "loss": 1.0903,
      "step": 16340
    },
    {
      "epoch": 0.8221037811745776,
      "grad_norm": 46.25,
      "learning_rate": 1.8355792437650844e-05,
      "loss": 1.199,
      "step": 16350
    },
    {
      "epoch": 0.8226065969428801,
      "grad_norm": 18.375,
      "learning_rate": 1.8354786806114243e-05,
      "loss": 1.2123,
      "step": 16360
    },
    {
      "epoch": 0.8231094127111827,
      "grad_norm": 31.5,
      "learning_rate": 1.8353781174577636e-05,
      "loss": 1.1022,
      "step": 16370
    },
    {
      "epoch": 0.8236122284794851,
      "grad_norm": 13.0625,
      "learning_rate": 1.8352775543041032e-05,
      "loss": 1.0532,
      "step": 16380
    },
    {
      "epoch": 0.8241150442477876,
      "grad_norm": 27.125,
      "learning_rate": 1.8351769911504428e-05,
      "loss": 0.9988,
      "step": 16390
    },
    {
      "epoch": 0.82461786001609,
      "grad_norm": 18.625,
      "learning_rate": 1.835076427996782e-05,
      "loss": 0.9286,
      "step": 16400
    },
    {
      "epoch": 0.8251206757843926,
      "grad_norm": 5.28125,
      "learning_rate": 1.8349758648431216e-05,
      "loss": 1.0479,
      "step": 16410
    },
    {
      "epoch": 0.8256234915526951,
      "grad_norm": 26.75,
      "learning_rate": 1.8348753016894612e-05,
      "loss": 0.8992,
      "step": 16420
    },
    {
      "epoch": 0.8261263073209976,
      "grad_norm": 35.25,
      "learning_rate": 1.8347747385358004e-05,
      "loss": 0.8922,
      "step": 16430
    },
    {
      "epoch": 0.8266291230893,
      "grad_norm": 5.28125,
      "learning_rate": 1.8346741753821404e-05,
      "loss": 0.6916,
      "step": 16440
    },
    {
      "epoch": 0.8271319388576026,
      "grad_norm": 47.5,
      "learning_rate": 1.8345736122284796e-05,
      "loss": 0.9853,
      "step": 16450
    },
    {
      "epoch": 0.8276347546259051,
      "grad_norm": 25.375,
      "learning_rate": 1.8344730490748192e-05,
      "loss": 0.8593,
      "step": 16460
    },
    {
      "epoch": 0.8281375703942075,
      "grad_norm": 27.25,
      "learning_rate": 1.8343724859211588e-05,
      "loss": 1.0337,
      "step": 16470
    },
    {
      "epoch": 0.82864038616251,
      "grad_norm": 19.75,
      "learning_rate": 1.834271922767498e-05,
      "loss": 1.1097,
      "step": 16480
    },
    {
      "epoch": 0.8291432019308126,
      "grad_norm": 42.75,
      "learning_rate": 1.8341713596138376e-05,
      "loss": 1.3658,
      "step": 16490
    },
    {
      "epoch": 0.8296460176991151,
      "grad_norm": 25.25,
      "learning_rate": 1.8340707964601772e-05,
      "loss": 1.1933,
      "step": 16500
    },
    {
      "epoch": 0.8296460176991151,
      "eval_accuracy": 0.5119236489836391,
      "eval_loss": 1.0301820039749146,
      "eval_runtime": 461.9591,
      "eval_samples_per_second": 87.324,
      "eval_steps_per_second": 87.324,
      "step": 16500
    },
    {
      "epoch": 0.8301488334674175,
      "grad_norm": 48.5,
      "learning_rate": 1.8339702333065165e-05,
      "loss": 1.3353,
      "step": 16510
    },
    {
      "epoch": 0.83065164923572,
      "grad_norm": 16.125,
      "learning_rate": 1.8338696701528564e-05,
      "loss": 0.914,
      "step": 16520
    },
    {
      "epoch": 0.8311544650040226,
      "grad_norm": 16.125,
      "learning_rate": 1.8337691069991956e-05,
      "loss": 0.9255,
      "step": 16530
    },
    {
      "epoch": 0.831657280772325,
      "grad_norm": 26.0,
      "learning_rate": 1.833668543845535e-05,
      "loss": 0.8681,
      "step": 16540
    },
    {
      "epoch": 0.8321600965406275,
      "grad_norm": 16.375,
      "learning_rate": 1.8335679806918748e-05,
      "loss": 1.0199,
      "step": 16550
    },
    {
      "epoch": 0.83266291230893,
      "grad_norm": 60.5,
      "learning_rate": 1.833467417538214e-05,
      "loss": 1.3848,
      "step": 16560
    },
    {
      "epoch": 0.8331657280772325,
      "grad_norm": 63.75,
      "learning_rate": 1.8333668543845536e-05,
      "loss": 0.9827,
      "step": 16570
    },
    {
      "epoch": 0.833668543845535,
      "grad_norm": 11.125,
      "learning_rate": 1.8332662912308932e-05,
      "loss": 0.875,
      "step": 16580
    },
    {
      "epoch": 0.8341713596138375,
      "grad_norm": 5.9375,
      "learning_rate": 1.8331657280772325e-05,
      "loss": 1.0749,
      "step": 16590
    },
    {
      "epoch": 0.83467417538214,
      "grad_norm": 19.75,
      "learning_rate": 1.833065164923572e-05,
      "loss": 1.0424,
      "step": 16600
    },
    {
      "epoch": 0.8351769911504425,
      "grad_norm": 14.5625,
      "learning_rate": 1.8329646017699117e-05,
      "loss": 1.331,
      "step": 16610
    },
    {
      "epoch": 0.835679806918745,
      "grad_norm": 27.5,
      "learning_rate": 1.832864038616251e-05,
      "loss": 1.025,
      "step": 16620
    },
    {
      "epoch": 0.8361826226870475,
      "grad_norm": 15.8125,
      "learning_rate": 1.832763475462591e-05,
      "loss": 0.8597,
      "step": 16630
    },
    {
      "epoch": 0.8366854384553499,
      "grad_norm": 10.375,
      "learning_rate": 1.83266291230893e-05,
      "loss": 1.2479,
      "step": 16640
    },
    {
      "epoch": 0.8371882542236525,
      "grad_norm": 16.375,
      "learning_rate": 1.8325623491552697e-05,
      "loss": 1.3217,
      "step": 16650
    },
    {
      "epoch": 0.837691069991955,
      "grad_norm": 5.90625,
      "learning_rate": 1.8324617860016093e-05,
      "loss": 1.0049,
      "step": 16660
    },
    {
      "epoch": 0.8381938857602574,
      "grad_norm": 43.75,
      "learning_rate": 1.8323612228479485e-05,
      "loss": 1.0463,
      "step": 16670
    },
    {
      "epoch": 0.8386967015285599,
      "grad_norm": 25.5,
      "learning_rate": 1.832260659694288e-05,
      "loss": 1.3906,
      "step": 16680
    },
    {
      "epoch": 0.8391995172968625,
      "grad_norm": 19.375,
      "learning_rate": 1.8321600965406277e-05,
      "loss": 0.9686,
      "step": 16690
    },
    {
      "epoch": 0.839702333065165,
      "grad_norm": 8.4375,
      "learning_rate": 1.832059533386967e-05,
      "loss": 1.2019,
      "step": 16700
    },
    {
      "epoch": 0.8402051488334674,
      "grad_norm": 4.0625,
      "learning_rate": 1.831958970233307e-05,
      "loss": 0.9921,
      "step": 16710
    },
    {
      "epoch": 0.8407079646017699,
      "grad_norm": 13.125,
      "learning_rate": 1.831858407079646e-05,
      "loss": 0.9354,
      "step": 16720
    },
    {
      "epoch": 0.8412107803700724,
      "grad_norm": 15.3125,
      "learning_rate": 1.8317578439259857e-05,
      "loss": 1.0542,
      "step": 16730
    },
    {
      "epoch": 0.8417135961383749,
      "grad_norm": 46.75,
      "learning_rate": 1.8316572807723253e-05,
      "loss": 0.9702,
      "step": 16740
    },
    {
      "epoch": 0.8422164119066774,
      "grad_norm": 20.125,
      "learning_rate": 1.8315567176186645e-05,
      "loss": 0.9636,
      "step": 16750
    },
    {
      "epoch": 0.8427192276749799,
      "grad_norm": 26.875,
      "learning_rate": 1.831456154465004e-05,
      "loss": 0.9355,
      "step": 16760
    },
    {
      "epoch": 0.8432220434432823,
      "grad_norm": 17.625,
      "learning_rate": 1.8313555913113437e-05,
      "loss": 1.1685,
      "step": 16770
    },
    {
      "epoch": 0.8437248592115849,
      "grad_norm": 5.6875,
      "learning_rate": 1.831255028157683e-05,
      "loss": 0.9408,
      "step": 16780
    },
    {
      "epoch": 0.8442276749798874,
      "grad_norm": 31.0,
      "learning_rate": 1.8311544650040225e-05,
      "loss": 1.2311,
      "step": 16790
    },
    {
      "epoch": 0.8447304907481898,
      "grad_norm": 10.8125,
      "learning_rate": 1.831053901850362e-05,
      "loss": 1.2096,
      "step": 16800
    },
    {
      "epoch": 0.8452333065164923,
      "grad_norm": 9.6875,
      "learning_rate": 1.8309533386967017e-05,
      "loss": 1.1112,
      "step": 16810
    },
    {
      "epoch": 0.8457361222847949,
      "grad_norm": 26.625,
      "learning_rate": 1.8308527755430413e-05,
      "loss": 0.9405,
      "step": 16820
    },
    {
      "epoch": 0.8462389380530974,
      "grad_norm": 22.5,
      "learning_rate": 1.8307522123893806e-05,
      "loss": 0.9714,
      "step": 16830
    },
    {
      "epoch": 0.8467417538213998,
      "grad_norm": 22.125,
      "learning_rate": 1.83065164923572e-05,
      "loss": 0.9878,
      "step": 16840
    },
    {
      "epoch": 0.8472445695897023,
      "grad_norm": 3.65625,
      "learning_rate": 1.8305510860820597e-05,
      "loss": 0.9349,
      "step": 16850
    },
    {
      "epoch": 0.8477473853580049,
      "grad_norm": 7.375,
      "learning_rate": 1.830450522928399e-05,
      "loss": 0.968,
      "step": 16860
    },
    {
      "epoch": 0.8482502011263073,
      "grad_norm": 53.0,
      "learning_rate": 1.8303499597747386e-05,
      "loss": 1.1667,
      "step": 16870
    },
    {
      "epoch": 0.8487530168946098,
      "grad_norm": 53.25,
      "learning_rate": 1.830249396621078e-05,
      "loss": 1.1114,
      "step": 16880
    },
    {
      "epoch": 0.8492558326629123,
      "grad_norm": 23.5,
      "learning_rate": 1.8301488334674178e-05,
      "loss": 0.874,
      "step": 16890
    },
    {
      "epoch": 0.8497586484312148,
      "grad_norm": 16.875,
      "learning_rate": 1.8300482703137573e-05,
      "loss": 1.2056,
      "step": 16900
    },
    {
      "epoch": 0.8502614641995173,
      "grad_norm": 7.59375,
      "learning_rate": 1.8299477071600966e-05,
      "loss": 1.0254,
      "step": 16910
    },
    {
      "epoch": 0.8507642799678198,
      "grad_norm": 60.75,
      "learning_rate": 1.8298471440064362e-05,
      "loss": 1.2557,
      "step": 16920
    },
    {
      "epoch": 0.8512670957361222,
      "grad_norm": 10.875,
      "learning_rate": 1.8297465808527758e-05,
      "loss": 1.1131,
      "step": 16930
    },
    {
      "epoch": 0.8517699115044248,
      "grad_norm": 14.3125,
      "learning_rate": 1.829646017699115e-05,
      "loss": 0.8695,
      "step": 16940
    },
    {
      "epoch": 0.8522727272727273,
      "grad_norm": 59.25,
      "learning_rate": 1.8295454545454546e-05,
      "loss": 0.6936,
      "step": 16950
    },
    {
      "epoch": 0.8527755430410298,
      "grad_norm": 6.71875,
      "learning_rate": 1.8294448913917942e-05,
      "loss": 0.8377,
      "step": 16960
    },
    {
      "epoch": 0.8532783588093322,
      "grad_norm": 11.125,
      "learning_rate": 1.8293443282381338e-05,
      "loss": 1.0102,
      "step": 16970
    },
    {
      "epoch": 0.8537811745776348,
      "grad_norm": 14.0625,
      "learning_rate": 1.8292437650844734e-05,
      "loss": 0.9527,
      "step": 16980
    },
    {
      "epoch": 0.8542839903459373,
      "grad_norm": 15.8125,
      "learning_rate": 1.8291432019308126e-05,
      "loss": 0.9743,
      "step": 16990
    },
    {
      "epoch": 0.8547868061142397,
      "grad_norm": 21.375,
      "learning_rate": 1.8290426387771522e-05,
      "loss": 1.1504,
      "step": 17000
    },
    {
      "epoch": 0.8547868061142397,
      "eval_accuracy": 0.5123698562221121,
      "eval_loss": 1.0280652046203613,
      "eval_runtime": 462.4558,
      "eval_samples_per_second": 87.23,
      "eval_steps_per_second": 87.23,
      "step": 17000
    },
    {
      "epoch": 0.8552896218825422,
      "grad_norm": 24.0,
      "learning_rate": 1.8289420756234918e-05,
      "loss": 0.8505,
      "step": 17010
    },
    {
      "epoch": 0.8557924376508448,
      "grad_norm": 28.0,
      "learning_rate": 1.828841512469831e-05,
      "loss": 1.005,
      "step": 17020
    },
    {
      "epoch": 0.8562952534191473,
      "grad_norm": 6.5625,
      "learning_rate": 1.8287409493161706e-05,
      "loss": 0.8718,
      "step": 17030
    },
    {
      "epoch": 0.8567980691874497,
      "grad_norm": 10.5625,
      "learning_rate": 1.8286403861625102e-05,
      "loss": 1.1798,
      "step": 17040
    },
    {
      "epoch": 0.8573008849557522,
      "grad_norm": 20.125,
      "learning_rate": 1.8285398230088498e-05,
      "loss": 0.9529,
      "step": 17050
    },
    {
      "epoch": 0.8578037007240547,
      "grad_norm": 26.5,
      "learning_rate": 1.828439259855189e-05,
      "loss": 1.2492,
      "step": 17060
    },
    {
      "epoch": 0.8583065164923572,
      "grad_norm": 11.8125,
      "learning_rate": 1.8283386967015286e-05,
      "loss": 1.4582,
      "step": 17070
    },
    {
      "epoch": 0.8588093322606597,
      "grad_norm": 19.125,
      "learning_rate": 1.8282381335478682e-05,
      "loss": 0.6868,
      "step": 17080
    },
    {
      "epoch": 0.8593121480289622,
      "grad_norm": 13.125,
      "learning_rate": 1.8281375703942078e-05,
      "loss": 0.9855,
      "step": 17090
    },
    {
      "epoch": 0.8598149637972646,
      "grad_norm": 22.625,
      "learning_rate": 1.828037007240547e-05,
      "loss": 1.0106,
      "step": 17100
    },
    {
      "epoch": 0.8603177795655672,
      "grad_norm": 7.5625,
      "learning_rate": 1.8279364440868867e-05,
      "loss": 1.0971,
      "step": 17110
    },
    {
      "epoch": 0.8608205953338697,
      "grad_norm": 25.25,
      "learning_rate": 1.8278358809332262e-05,
      "loss": 0.9079,
      "step": 17120
    },
    {
      "epoch": 0.8613234111021721,
      "grad_norm": 10.375,
      "learning_rate": 1.8277353177795658e-05,
      "loss": 1.0167,
      "step": 17130
    },
    {
      "epoch": 0.8618262268704746,
      "grad_norm": 12.0625,
      "learning_rate": 1.827634754625905e-05,
      "loss": 0.736,
      "step": 17140
    },
    {
      "epoch": 0.8623290426387772,
      "grad_norm": 5.75,
      "learning_rate": 1.8275341914722447e-05,
      "loss": 1.0892,
      "step": 17150
    },
    {
      "epoch": 0.8628318584070797,
      "grad_norm": 14.25,
      "learning_rate": 1.8274336283185843e-05,
      "loss": 1.3527,
      "step": 17160
    },
    {
      "epoch": 0.8633346741753821,
      "grad_norm": 4.6875,
      "learning_rate": 1.827333065164924e-05,
      "loss": 1.0316,
      "step": 17170
    },
    {
      "epoch": 0.8638374899436846,
      "grad_norm": 42.5,
      "learning_rate": 1.827232502011263e-05,
      "loss": 1.0999,
      "step": 17180
    },
    {
      "epoch": 0.8643403057119872,
      "grad_norm": 20.25,
      "learning_rate": 1.8271319388576027e-05,
      "loss": 1.0031,
      "step": 17190
    },
    {
      "epoch": 0.8648431214802896,
      "grad_norm": 8.5,
      "learning_rate": 1.8270313757039423e-05,
      "loss": 0.9769,
      "step": 17200
    },
    {
      "epoch": 0.8653459372485921,
      "grad_norm": 10.8125,
      "learning_rate": 1.826930812550282e-05,
      "loss": 1.2326,
      "step": 17210
    },
    {
      "epoch": 0.8658487530168946,
      "grad_norm": 22.375,
      "learning_rate": 1.826830249396621e-05,
      "loss": 1.0382,
      "step": 17220
    },
    {
      "epoch": 0.8663515687851971,
      "grad_norm": 12.625,
      "learning_rate": 1.8267296862429607e-05,
      "loss": 0.9285,
      "step": 17230
    },
    {
      "epoch": 0.8668543845534996,
      "grad_norm": 13.875,
      "learning_rate": 1.8266291230893003e-05,
      "loss": 1.0842,
      "step": 17240
    },
    {
      "epoch": 0.8673572003218021,
      "grad_norm": 68.0,
      "learning_rate": 1.82652855993564e-05,
      "loss": 1.1518,
      "step": 17250
    },
    {
      "epoch": 0.8678600160901045,
      "grad_norm": 31.375,
      "learning_rate": 1.826427996781979e-05,
      "loss": 1.4123,
      "step": 17260
    },
    {
      "epoch": 0.8683628318584071,
      "grad_norm": 17.75,
      "learning_rate": 1.8263274336283187e-05,
      "loss": 0.8724,
      "step": 17270
    },
    {
      "epoch": 0.8688656476267096,
      "grad_norm": 20.25,
      "learning_rate": 1.8262268704746583e-05,
      "loss": 1.1069,
      "step": 17280
    },
    {
      "epoch": 0.8693684633950121,
      "grad_norm": 36.5,
      "learning_rate": 1.826126307320998e-05,
      "loss": 0.9083,
      "step": 17290
    },
    {
      "epoch": 0.8698712791633145,
      "grad_norm": 11.6875,
      "learning_rate": 1.826025744167337e-05,
      "loss": 0.9378,
      "step": 17300
    },
    {
      "epoch": 0.8703740949316171,
      "grad_norm": 39.25,
      "learning_rate": 1.8259251810136767e-05,
      "loss": 1.1719,
      "step": 17310
    },
    {
      "epoch": 0.8708769106999196,
      "grad_norm": 11.25,
      "learning_rate": 1.8258246178600163e-05,
      "loss": 0.9659,
      "step": 17320
    },
    {
      "epoch": 0.871379726468222,
      "grad_norm": 11.75,
      "learning_rate": 1.8257240547063556e-05,
      "loss": 1.1138,
      "step": 17330
    },
    {
      "epoch": 0.8718825422365245,
      "grad_norm": 15.875,
      "learning_rate": 1.825623491552695e-05,
      "loss": 1.0494,
      "step": 17340
    },
    {
      "epoch": 0.8723853580048271,
      "grad_norm": 4.03125,
      "learning_rate": 1.8255229283990347e-05,
      "loss": 0.7788,
      "step": 17350
    },
    {
      "epoch": 0.8728881737731295,
      "grad_norm": 18.75,
      "learning_rate": 1.8254223652453743e-05,
      "loss": 1.1675,
      "step": 17360
    },
    {
      "epoch": 0.873390989541432,
      "grad_norm": 18.625,
      "learning_rate": 1.825321802091714e-05,
      "loss": 1.0899,
      "step": 17370
    },
    {
      "epoch": 0.8738938053097345,
      "grad_norm": 53.25,
      "learning_rate": 1.825221238938053e-05,
      "loss": 1.205,
      "step": 17380
    },
    {
      "epoch": 0.8743966210780371,
      "grad_norm": 15.875,
      "learning_rate": 1.8251206757843927e-05,
      "loss": 1.1858,
      "step": 17390
    },
    {
      "epoch": 0.8748994368463395,
      "grad_norm": 13.1875,
      "learning_rate": 1.8250201126307323e-05,
      "loss": 1.0256,
      "step": 17400
    },
    {
      "epoch": 0.875402252614642,
      "grad_norm": 13.75,
      "learning_rate": 1.8249195494770716e-05,
      "loss": 0.8975,
      "step": 17410
    },
    {
      "epoch": 0.8759050683829445,
      "grad_norm": 15.5,
      "learning_rate": 1.824818986323411e-05,
      "loss": 1.012,
      "step": 17420
    },
    {
      "epoch": 0.8764078841512469,
      "grad_norm": 32.75,
      "learning_rate": 1.8247184231697508e-05,
      "loss": 0.7981,
      "step": 17430
    },
    {
      "epoch": 0.8769106999195495,
      "grad_norm": 51.5,
      "learning_rate": 1.8246178600160903e-05,
      "loss": 1.0029,
      "step": 17440
    },
    {
      "epoch": 0.877413515687852,
      "grad_norm": 26.5,
      "learning_rate": 1.82451729686243e-05,
      "loss": 1.0312,
      "step": 17450
    },
    {
      "epoch": 0.8779163314561544,
      "grad_norm": 30.0,
      "learning_rate": 1.8244167337087692e-05,
      "loss": 1.1181,
      "step": 17460
    },
    {
      "epoch": 0.8784191472244569,
      "grad_norm": 52.0,
      "learning_rate": 1.8243161705551088e-05,
      "loss": 1.1908,
      "step": 17470
    },
    {
      "epoch": 0.8789219629927595,
      "grad_norm": 15.5625,
      "learning_rate": 1.8242156074014484e-05,
      "loss": 1.0125,
      "step": 17480
    },
    {
      "epoch": 0.879424778761062,
      "grad_norm": 14.25,
      "learning_rate": 1.8241150442477876e-05,
      "loss": 0.8435,
      "step": 17490
    },
    {
      "epoch": 0.8799275945293644,
      "grad_norm": 18.5,
      "learning_rate": 1.8240144810941272e-05,
      "loss": 1.1229,
      "step": 17500
    },
    {
      "epoch": 0.8799275945293644,
      "eval_accuracy": 0.5119236489836391,
      "eval_loss": 1.0284544229507446,
      "eval_runtime": 462.5487,
      "eval_samples_per_second": 87.212,
      "eval_steps_per_second": 87.212,
      "step": 17500
    },
    {
      "epoch": 0.8804304102976669,
      "grad_norm": 48.5,
      "learning_rate": 1.8239139179404668e-05,
      "loss": 0.91,
      "step": 17510
    },
    {
      "epoch": 0.8809332260659695,
      "grad_norm": 22.75,
      "learning_rate": 1.8238133547868064e-05,
      "loss": 0.9585,
      "step": 17520
    },
    {
      "epoch": 0.8814360418342719,
      "grad_norm": 25.25,
      "learning_rate": 1.823712791633146e-05,
      "loss": 1.0184,
      "step": 17530
    },
    {
      "epoch": 0.8819388576025744,
      "grad_norm": 26.0,
      "learning_rate": 1.8236122284794852e-05,
      "loss": 0.8615,
      "step": 17540
    },
    {
      "epoch": 0.8824416733708769,
      "grad_norm": 21.875,
      "learning_rate": 1.8235116653258248e-05,
      "loss": 1.1638,
      "step": 17550
    },
    {
      "epoch": 0.8829444891391794,
      "grad_norm": 15.375,
      "learning_rate": 1.8234111021721644e-05,
      "loss": 1.1136,
      "step": 17560
    },
    {
      "epoch": 0.8834473049074819,
      "grad_norm": 27.5,
      "learning_rate": 1.8233105390185036e-05,
      "loss": 0.9081,
      "step": 17570
    },
    {
      "epoch": 0.8839501206757844,
      "grad_norm": 35.75,
      "learning_rate": 1.8232099758648432e-05,
      "loss": 0.8577,
      "step": 17580
    },
    {
      "epoch": 0.8844529364440868,
      "grad_norm": 20.75,
      "learning_rate": 1.8231094127111828e-05,
      "loss": 1.1257,
      "step": 17590
    },
    {
      "epoch": 0.8849557522123894,
      "grad_norm": 11.0,
      "learning_rate": 1.823008849557522e-05,
      "loss": 1.0196,
      "step": 17600
    },
    {
      "epoch": 0.8854585679806919,
      "grad_norm": 53.0,
      "learning_rate": 1.822908286403862e-05,
      "loss": 0.9297,
      "step": 17610
    },
    {
      "epoch": 0.8859613837489944,
      "grad_norm": 26.375,
      "learning_rate": 1.8228077232502012e-05,
      "loss": 1.1288,
      "step": 17620
    },
    {
      "epoch": 0.8864641995172968,
      "grad_norm": 31.625,
      "learning_rate": 1.8227071600965408e-05,
      "loss": 1.2786,
      "step": 17630
    },
    {
      "epoch": 0.8869670152855994,
      "grad_norm": 21.5,
      "learning_rate": 1.8226065969428804e-05,
      "loss": 1.1805,
      "step": 17640
    },
    {
      "epoch": 0.8874698310539019,
      "grad_norm": 5.71875,
      "learning_rate": 1.8225060337892197e-05,
      "loss": 1.0016,
      "step": 17650
    },
    {
      "epoch": 0.8879726468222043,
      "grad_norm": 32.25,
      "learning_rate": 1.8224054706355592e-05,
      "loss": 1.0454,
      "step": 17660
    },
    {
      "epoch": 0.8884754625905068,
      "grad_norm": 17.0,
      "learning_rate": 1.822304907481899e-05,
      "loss": 0.8345,
      "step": 17670
    },
    {
      "epoch": 0.8889782783588094,
      "grad_norm": 24.25,
      "learning_rate": 1.822204344328238e-05,
      "loss": 1.0321,
      "step": 17680
    },
    {
      "epoch": 0.8894810941271118,
      "grad_norm": 17.125,
      "learning_rate": 1.822103781174578e-05,
      "loss": 0.8696,
      "step": 17690
    },
    {
      "epoch": 0.8899839098954143,
      "grad_norm": 23.625,
      "learning_rate": 1.8220032180209173e-05,
      "loss": 0.9015,
      "step": 17700
    },
    {
      "epoch": 0.8904867256637168,
      "grad_norm": 16.25,
      "learning_rate": 1.821902654867257e-05,
      "loss": 1.2547,
      "step": 17710
    },
    {
      "epoch": 0.8909895414320194,
      "grad_norm": 57.75,
      "learning_rate": 1.8218020917135964e-05,
      "loss": 1.5312,
      "step": 17720
    },
    {
      "epoch": 0.8914923572003218,
      "grad_norm": 15.4375,
      "learning_rate": 1.8217015285599357e-05,
      "loss": 1.0454,
      "step": 17730
    },
    {
      "epoch": 0.8919951729686243,
      "grad_norm": 55.75,
      "learning_rate": 1.8216009654062753e-05,
      "loss": 1.1093,
      "step": 17740
    },
    {
      "epoch": 0.8924979887369268,
      "grad_norm": 29.5,
      "learning_rate": 1.821500402252615e-05,
      "loss": 1.183,
      "step": 17750
    },
    {
      "epoch": 0.8930008045052292,
      "grad_norm": 31.25,
      "learning_rate": 1.821399839098954e-05,
      "loss": 1.0682,
      "step": 17760
    },
    {
      "epoch": 0.8935036202735318,
      "grad_norm": 30.75,
      "learning_rate": 1.821299275945294e-05,
      "loss": 1.3905,
      "step": 17770
    },
    {
      "epoch": 0.8940064360418343,
      "grad_norm": 6.96875,
      "learning_rate": 1.8211987127916333e-05,
      "loss": 0.8714,
      "step": 17780
    },
    {
      "epoch": 0.8945092518101367,
      "grad_norm": 30.375,
      "learning_rate": 1.821098149637973e-05,
      "loss": 1.4155,
      "step": 17790
    },
    {
      "epoch": 0.8950120675784392,
      "grad_norm": 47.5,
      "learning_rate": 1.8209975864843125e-05,
      "loss": 1.0382,
      "step": 17800
    },
    {
      "epoch": 0.8955148833467418,
      "grad_norm": 14.9375,
      "learning_rate": 1.8208970233306517e-05,
      "loss": 1.1908,
      "step": 17810
    },
    {
      "epoch": 0.8960176991150443,
      "grad_norm": 18.875,
      "learning_rate": 1.8207964601769913e-05,
      "loss": 1.1677,
      "step": 17820
    },
    {
      "epoch": 0.8965205148833467,
      "grad_norm": 12.875,
      "learning_rate": 1.820695897023331e-05,
      "loss": 0.9572,
      "step": 17830
    },
    {
      "epoch": 0.8970233306516492,
      "grad_norm": 59.5,
      "learning_rate": 1.82059533386967e-05,
      "loss": 1.2224,
      "step": 17840
    },
    {
      "epoch": 0.8975261464199518,
      "grad_norm": 24.5,
      "learning_rate": 1.8204947707160097e-05,
      "loss": 1.0429,
      "step": 17850
    },
    {
      "epoch": 0.8980289621882542,
      "grad_norm": 9.875,
      "learning_rate": 1.8203942075623493e-05,
      "loss": 1.0545,
      "step": 17860
    },
    {
      "epoch": 0.8985317779565567,
      "grad_norm": 18.0,
      "learning_rate": 1.8202936444086886e-05,
      "loss": 1.1099,
      "step": 17870
    },
    {
      "epoch": 0.8990345937248592,
      "grad_norm": 29.75,
      "learning_rate": 1.8201930812550285e-05,
      "loss": 1.2311,
      "step": 17880
    },
    {
      "epoch": 0.8995374094931617,
      "grad_norm": 14.625,
      "learning_rate": 1.8200925181013677e-05,
      "loss": 1.1047,
      "step": 17890
    },
    {
      "epoch": 0.9000402252614642,
      "grad_norm": 21.75,
      "learning_rate": 1.8199919549477073e-05,
      "loss": 1.2194,
      "step": 17900
    },
    {
      "epoch": 0.9005430410297667,
      "grad_norm": 11.5,
      "learning_rate": 1.819891391794047e-05,
      "loss": 0.8571,
      "step": 17910
    },
    {
      "epoch": 0.9010458567980691,
      "grad_norm": 50.5,
      "learning_rate": 1.819790828640386e-05,
      "loss": 1.098,
      "step": 17920
    },
    {
      "epoch": 0.9015486725663717,
      "grad_norm": 11.6875,
      "learning_rate": 1.8196902654867257e-05,
      "loss": 0.9747,
      "step": 17930
    },
    {
      "epoch": 0.9020514883346742,
      "grad_norm": 11.6875,
      "learning_rate": 1.8195897023330653e-05,
      "loss": 0.8156,
      "step": 17940
    },
    {
      "epoch": 0.9025543041029767,
      "grad_norm": 15.4375,
      "learning_rate": 1.8194891391794046e-05,
      "loss": 1.0256,
      "step": 17950
    },
    {
      "epoch": 0.9030571198712791,
      "grad_norm": 17.125,
      "learning_rate": 1.8193885760257445e-05,
      "loss": 0.9636,
      "step": 17960
    },
    {
      "epoch": 0.9035599356395817,
      "grad_norm": 11.4375,
      "learning_rate": 1.8192880128720838e-05,
      "loss": 0.7943,
      "step": 17970
    },
    {
      "epoch": 0.9040627514078842,
      "grad_norm": 9.1875,
      "learning_rate": 1.8191874497184233e-05,
      "loss": 0.8051,
      "step": 17980
    },
    {
      "epoch": 0.9045655671761866,
      "grad_norm": 17.25,
      "learning_rate": 1.819086886564763e-05,
      "loss": 0.8141,
      "step": 17990
    },
    {
      "epoch": 0.9050683829444891,
      "grad_norm": 57.25,
      "learning_rate": 1.8189863234111022e-05,
      "loss": 1.0643,
      "step": 18000
    },
    {
      "epoch": 0.9050683829444891,
      "eval_accuracy": 0.5120475954387704,
      "eval_loss": 1.0288959741592407,
      "eval_runtime": 462.556,
      "eval_samples_per_second": 87.211,
      "eval_steps_per_second": 87.211,
      "step": 18000
    },
    {
      "epoch": 0.9055711987127917,
      "grad_norm": 7.90625,
      "learning_rate": 1.8188857602574418e-05,
      "loss": 1.0145,
      "step": 18010
    },
    {
      "epoch": 0.9060740144810941,
      "grad_norm": 5.90625,
      "learning_rate": 1.8187851971037814e-05,
      "loss": 1.0173,
      "step": 18020
    },
    {
      "epoch": 0.9065768302493966,
      "grad_norm": 31.125,
      "learning_rate": 1.8186846339501206e-05,
      "loss": 1.1096,
      "step": 18030
    },
    {
      "epoch": 0.9070796460176991,
      "grad_norm": 10.8125,
      "learning_rate": 1.8185840707964605e-05,
      "loss": 0.7536,
      "step": 18040
    },
    {
      "epoch": 0.9075824617860017,
      "grad_norm": 12.75,
      "learning_rate": 1.8184835076427998e-05,
      "loss": 1.2761,
      "step": 18050
    },
    {
      "epoch": 0.9080852775543041,
      "grad_norm": 19.375,
      "learning_rate": 1.8183829444891394e-05,
      "loss": 0.6576,
      "step": 18060
    },
    {
      "epoch": 0.9085880933226066,
      "grad_norm": 15.75,
      "learning_rate": 1.818282381335479e-05,
      "loss": 0.8616,
      "step": 18070
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 6.5,
      "learning_rate": 1.8181818181818182e-05,
      "loss": 1.0466,
      "step": 18080
    },
    {
      "epoch": 0.9095937248592116,
      "grad_norm": 15.25,
      "learning_rate": 1.8180812550281578e-05,
      "loss": 0.9504,
      "step": 18090
    },
    {
      "epoch": 0.9100965406275141,
      "grad_norm": 27.0,
      "learning_rate": 1.8179806918744974e-05,
      "loss": 1.0633,
      "step": 18100
    },
    {
      "epoch": 0.9105993563958166,
      "grad_norm": 11.125,
      "learning_rate": 1.8178801287208366e-05,
      "loss": 0.9367,
      "step": 18110
    },
    {
      "epoch": 0.911102172164119,
      "grad_norm": 12.8125,
      "learning_rate": 1.8177795655671762e-05,
      "loss": 1.1117,
      "step": 18120
    },
    {
      "epoch": 0.9116049879324215,
      "grad_norm": 18.5,
      "learning_rate": 1.8176790024135158e-05,
      "loss": 0.9848,
      "step": 18130
    },
    {
      "epoch": 0.9121078037007241,
      "grad_norm": 28.625,
      "learning_rate": 1.8175784392598554e-05,
      "loss": 1.1417,
      "step": 18140
    },
    {
      "epoch": 0.9126106194690266,
      "grad_norm": 17.75,
      "learning_rate": 1.817477876106195e-05,
      "loss": 1.1899,
      "step": 18150
    },
    {
      "epoch": 0.913113435237329,
      "grad_norm": 6.40625,
      "learning_rate": 1.8173773129525342e-05,
      "loss": 0.8624,
      "step": 18160
    },
    {
      "epoch": 0.9136162510056315,
      "grad_norm": 26.5,
      "learning_rate": 1.8172767497988738e-05,
      "loss": 1.0761,
      "step": 18170
    },
    {
      "epoch": 0.9141190667739341,
      "grad_norm": 24.75,
      "learning_rate": 1.8171761866452134e-05,
      "loss": 0.9982,
      "step": 18180
    },
    {
      "epoch": 0.9146218825422365,
      "grad_norm": 6.90625,
      "learning_rate": 1.8170756234915527e-05,
      "loss": 0.8917,
      "step": 18190
    },
    {
      "epoch": 0.915124698310539,
      "grad_norm": 17.875,
      "learning_rate": 1.8169750603378922e-05,
      "loss": 0.7385,
      "step": 18200
    },
    {
      "epoch": 0.9156275140788415,
      "grad_norm": 14.9375,
      "learning_rate": 1.816874497184232e-05,
      "loss": 1.0182,
      "step": 18210
    },
    {
      "epoch": 0.916130329847144,
      "grad_norm": 65.5,
      "learning_rate": 1.8167739340305714e-05,
      "loss": 1.3689,
      "step": 18220
    },
    {
      "epoch": 0.9166331456154465,
      "grad_norm": 27.25,
      "learning_rate": 1.816673370876911e-05,
      "loss": 1.3457,
      "step": 18230
    },
    {
      "epoch": 0.917135961383749,
      "grad_norm": 27.5,
      "learning_rate": 1.8165728077232503e-05,
      "loss": 0.8542,
      "step": 18240
    },
    {
      "epoch": 0.9176387771520514,
      "grad_norm": 10.0625,
      "learning_rate": 1.81647224456959e-05,
      "loss": 0.7912,
      "step": 18250
    },
    {
      "epoch": 0.918141592920354,
      "grad_norm": 3.65625,
      "learning_rate": 1.8163716814159294e-05,
      "loss": 1.1645,
      "step": 18260
    },
    {
      "epoch": 0.9186444086886565,
      "grad_norm": 16.125,
      "learning_rate": 1.8162711182622687e-05,
      "loss": 0.7908,
      "step": 18270
    },
    {
      "epoch": 0.919147224456959,
      "grad_norm": 54.25,
      "learning_rate": 1.8161705551086083e-05,
      "loss": 0.9057,
      "step": 18280
    },
    {
      "epoch": 0.9196500402252614,
      "grad_norm": 22.5,
      "learning_rate": 1.816069991954948e-05,
      "loss": 1.2262,
      "step": 18290
    },
    {
      "epoch": 0.920152855993564,
      "grad_norm": 19.25,
      "learning_rate": 1.8159694288012874e-05,
      "loss": 0.9699,
      "step": 18300
    },
    {
      "epoch": 0.9206556717618665,
      "grad_norm": 20.5,
      "learning_rate": 1.815868865647627e-05,
      "loss": 1.0174,
      "step": 18310
    },
    {
      "epoch": 0.9211584875301689,
      "grad_norm": 49.0,
      "learning_rate": 1.8157683024939663e-05,
      "loss": 1.2709,
      "step": 18320
    },
    {
      "epoch": 0.9216613032984714,
      "grad_norm": 10.4375,
      "learning_rate": 1.815667739340306e-05,
      "loss": 0.9239,
      "step": 18330
    },
    {
      "epoch": 0.922164119066774,
      "grad_norm": 37.5,
      "learning_rate": 1.8155671761866455e-05,
      "loss": 1.1873,
      "step": 18340
    },
    {
      "epoch": 0.9226669348350764,
      "grad_norm": 27.75,
      "learning_rate": 1.8154666130329847e-05,
      "loss": 1.3248,
      "step": 18350
    },
    {
      "epoch": 0.9231697506033789,
      "grad_norm": 22.5,
      "learning_rate": 1.8153660498793243e-05,
      "loss": 0.7443,
      "step": 18360
    },
    {
      "epoch": 0.9236725663716814,
      "grad_norm": 3.359375,
      "learning_rate": 1.815265486725664e-05,
      "loss": 0.889,
      "step": 18370
    },
    {
      "epoch": 0.924175382139984,
      "grad_norm": 19.5,
      "learning_rate": 1.8151649235720035e-05,
      "loss": 1.0062,
      "step": 18380
    },
    {
      "epoch": 0.9246781979082864,
      "grad_norm": 31.875,
      "learning_rate": 1.8150643604183427e-05,
      "loss": 1.1468,
      "step": 18390
    },
    {
      "epoch": 0.9251810136765889,
      "grad_norm": 12.875,
      "learning_rate": 1.8149637972646823e-05,
      "loss": 1.0726,
      "step": 18400
    },
    {
      "epoch": 0.9256838294448914,
      "grad_norm": 46.75,
      "learning_rate": 1.814863234111022e-05,
      "loss": 0.909,
      "step": 18410
    },
    {
      "epoch": 0.9261866452131939,
      "grad_norm": 14.0,
      "learning_rate": 1.8147626709573615e-05,
      "loss": 0.8678,
      "step": 18420
    },
    {
      "epoch": 0.9266894609814964,
      "grad_norm": 5.65625,
      "learning_rate": 1.8146621078037007e-05,
      "loss": 0.8827,
      "step": 18430
    },
    {
      "epoch": 0.9271922767497989,
      "grad_norm": 17.5,
      "learning_rate": 1.8145615446500403e-05,
      "loss": 1.2191,
      "step": 18440
    },
    {
      "epoch": 0.9276950925181013,
      "grad_norm": 23.375,
      "learning_rate": 1.81446098149638e-05,
      "loss": 0.8359,
      "step": 18450
    },
    {
      "epoch": 0.9281979082864039,
      "grad_norm": 4.46875,
      "learning_rate": 1.8143604183427195e-05,
      "loss": 1.0543,
      "step": 18460
    },
    {
      "epoch": 0.9287007240547064,
      "grad_norm": 10.3125,
      "learning_rate": 1.8142598551890587e-05,
      "loss": 1.0665,
      "step": 18470
    },
    {
      "epoch": 0.9292035398230089,
      "grad_norm": 9.4375,
      "learning_rate": 1.8141592920353983e-05,
      "loss": 0.9558,
      "step": 18480
    },
    {
      "epoch": 0.9297063555913113,
      "grad_norm": 51.25,
      "learning_rate": 1.814058728881738e-05,
      "loss": 1.1789,
      "step": 18490
    },
    {
      "epoch": 0.9302091713596138,
      "grad_norm": 22.75,
      "learning_rate": 1.8139581657280775e-05,
      "loss": 0.9987,
      "step": 18500
    },
    {
      "epoch": 0.9302091713596138,
      "eval_accuracy": 0.5117997025285077,
      "eval_loss": 1.0290436744689941,
      "eval_runtime": 464.0292,
      "eval_samples_per_second": 86.934,
      "eval_steps_per_second": 86.934,
      "step": 18500
    },
    {
      "epoch": 0.9307119871279164,
      "grad_norm": 8.1875,
      "learning_rate": 1.8138576025744168e-05,
      "loss": 1.0059,
      "step": 18510
    },
    {
      "epoch": 0.9312148028962188,
      "grad_norm": 26.875,
      "learning_rate": 1.8137570394207563e-05,
      "loss": 1.0094,
      "step": 18520
    },
    {
      "epoch": 0.9317176186645213,
      "grad_norm": 69.0,
      "learning_rate": 1.813656476267096e-05,
      "loss": 1.2449,
      "step": 18530
    },
    {
      "epoch": 0.9322204344328238,
      "grad_norm": 44.5,
      "learning_rate": 1.8135559131134355e-05,
      "loss": 1.097,
      "step": 18540
    },
    {
      "epoch": 0.9327232502011263,
      "grad_norm": 29.875,
      "learning_rate": 1.8134553499597748e-05,
      "loss": 1.1332,
      "step": 18550
    },
    {
      "epoch": 0.9332260659694288,
      "grad_norm": 17.625,
      "learning_rate": 1.8133547868061144e-05,
      "loss": 1.2941,
      "step": 18560
    },
    {
      "epoch": 0.9337288817377313,
      "grad_norm": 17.125,
      "learning_rate": 1.813254223652454e-05,
      "loss": 0.959,
      "step": 18570
    },
    {
      "epoch": 0.9342316975060337,
      "grad_norm": 82.0,
      "learning_rate": 1.8131536604987935e-05,
      "loss": 1.1269,
      "step": 18580
    },
    {
      "epoch": 0.9347345132743363,
      "grad_norm": 21.375,
      "learning_rate": 1.8130530973451328e-05,
      "loss": 1.1298,
      "step": 18590
    },
    {
      "epoch": 0.9352373290426388,
      "grad_norm": 32.75,
      "learning_rate": 1.8129525341914724e-05,
      "loss": 0.8219,
      "step": 18600
    },
    {
      "epoch": 0.9357401448109413,
      "grad_norm": 19.5,
      "learning_rate": 1.812851971037812e-05,
      "loss": 1.1532,
      "step": 18610
    },
    {
      "epoch": 0.9362429605792437,
      "grad_norm": 13.8125,
      "learning_rate": 1.8127514078841515e-05,
      "loss": 0.7704,
      "step": 18620
    },
    {
      "epoch": 0.9367457763475463,
      "grad_norm": 12.75,
      "learning_rate": 1.8126508447304908e-05,
      "loss": 1.1767,
      "step": 18630
    },
    {
      "epoch": 0.9372485921158488,
      "grad_norm": 39.5,
      "learning_rate": 1.8125502815768304e-05,
      "loss": 1.1902,
      "step": 18640
    },
    {
      "epoch": 0.9377514078841512,
      "grad_norm": 61.5,
      "learning_rate": 1.81244971842317e-05,
      "loss": 1.3738,
      "step": 18650
    },
    {
      "epoch": 0.9382542236524537,
      "grad_norm": 15.25,
      "learning_rate": 1.8123491552695092e-05,
      "loss": 1.0483,
      "step": 18660
    },
    {
      "epoch": 0.9387570394207563,
      "grad_norm": 14.5625,
      "learning_rate": 1.8122485921158488e-05,
      "loss": 0.7977,
      "step": 18670
    },
    {
      "epoch": 0.9392598551890587,
      "grad_norm": 9.25,
      "learning_rate": 1.8121480289621884e-05,
      "loss": 1.0632,
      "step": 18680
    },
    {
      "epoch": 0.9397626709573612,
      "grad_norm": 26.125,
      "learning_rate": 1.812047465808528e-05,
      "loss": 0.8323,
      "step": 18690
    },
    {
      "epoch": 0.9402654867256637,
      "grad_norm": 18.25,
      "learning_rate": 1.8119469026548676e-05,
      "loss": 0.9594,
      "step": 18700
    },
    {
      "epoch": 0.9407683024939663,
      "grad_norm": 47.25,
      "learning_rate": 1.8118463395012068e-05,
      "loss": 1.0462,
      "step": 18710
    },
    {
      "epoch": 0.9412711182622687,
      "grad_norm": 18.75,
      "learning_rate": 1.8117457763475464e-05,
      "loss": 1.0559,
      "step": 18720
    },
    {
      "epoch": 0.9417739340305712,
      "grad_norm": 18.875,
      "learning_rate": 1.811645213193886e-05,
      "loss": 0.9539,
      "step": 18730
    },
    {
      "epoch": 0.9422767497988737,
      "grad_norm": 9.1875,
      "learning_rate": 1.8115446500402252e-05,
      "loss": 0.9342,
      "step": 18740
    },
    {
      "epoch": 0.9427795655671762,
      "grad_norm": 17.375,
      "learning_rate": 1.811444086886565e-05,
      "loss": 1.3474,
      "step": 18750
    },
    {
      "epoch": 0.9432823813354787,
      "grad_norm": 16.0,
      "learning_rate": 1.8113435237329044e-05,
      "loss": 1.074,
      "step": 18760
    },
    {
      "epoch": 0.9437851971037812,
      "grad_norm": 8.6875,
      "learning_rate": 1.811242960579244e-05,
      "loss": 1.2219,
      "step": 18770
    },
    {
      "epoch": 0.9442880128720836,
      "grad_norm": 19.125,
      "learning_rate": 1.8111423974255836e-05,
      "loss": 1.0072,
      "step": 18780
    },
    {
      "epoch": 0.9447908286403862,
      "grad_norm": 51.75,
      "learning_rate": 1.811041834271923e-05,
      "loss": 1.13,
      "step": 18790
    },
    {
      "epoch": 0.9452936444086887,
      "grad_norm": 23.0,
      "learning_rate": 1.8109412711182624e-05,
      "loss": 0.6863,
      "step": 18800
    },
    {
      "epoch": 0.9457964601769911,
      "grad_norm": 35.25,
      "learning_rate": 1.810840707964602e-05,
      "loss": 1.0483,
      "step": 18810
    },
    {
      "epoch": 0.9462992759452936,
      "grad_norm": 41.75,
      "learning_rate": 1.8107401448109413e-05,
      "loss": 0.9284,
      "step": 18820
    },
    {
      "epoch": 0.9468020917135961,
      "grad_norm": 51.0,
      "learning_rate": 1.810639581657281e-05,
      "loss": 1.1328,
      "step": 18830
    },
    {
      "epoch": 0.9473049074818987,
      "grad_norm": 40.0,
      "learning_rate": 1.8105390185036204e-05,
      "loss": 1.1017,
      "step": 18840
    },
    {
      "epoch": 0.9478077232502011,
      "grad_norm": 5.8125,
      "learning_rate": 1.81043845534996e-05,
      "loss": 0.956,
      "step": 18850
    },
    {
      "epoch": 0.9483105390185036,
      "grad_norm": 16.5,
      "learning_rate": 1.8103378921962996e-05,
      "loss": 0.7215,
      "step": 18860
    },
    {
      "epoch": 0.9488133547868061,
      "grad_norm": 9.25,
      "learning_rate": 1.810237329042639e-05,
      "loss": 0.9559,
      "step": 18870
    },
    {
      "epoch": 0.9493161705551086,
      "grad_norm": 21.625,
      "learning_rate": 1.8101367658889785e-05,
      "loss": 1.0588,
      "step": 18880
    },
    {
      "epoch": 0.9498189863234111,
      "grad_norm": 6.0,
      "learning_rate": 1.810036202735318e-05,
      "loss": 1.5373,
      "step": 18890
    },
    {
      "epoch": 0.9503218020917136,
      "grad_norm": 10.4375,
      "learning_rate": 1.8099356395816573e-05,
      "loss": 0.9312,
      "step": 18900
    },
    {
      "epoch": 0.950824617860016,
      "grad_norm": 20.125,
      "learning_rate": 1.809835076427997e-05,
      "loss": 0.9668,
      "step": 18910
    },
    {
      "epoch": 0.9513274336283186,
      "grad_norm": 15.375,
      "learning_rate": 1.8097345132743365e-05,
      "loss": 1.019,
      "step": 18920
    },
    {
      "epoch": 0.9518302493966211,
      "grad_norm": 44.0,
      "learning_rate": 1.8096339501206757e-05,
      "loss": 1.0835,
      "step": 18930
    },
    {
      "epoch": 0.9523330651649236,
      "grad_norm": 10.0,
      "learning_rate": 1.8095333869670157e-05,
      "loss": 0.7352,
      "step": 18940
    },
    {
      "epoch": 0.952835880933226,
      "grad_norm": 42.25,
      "learning_rate": 1.809432823813355e-05,
      "loss": 1.0419,
      "step": 18950
    },
    {
      "epoch": 0.9533386967015286,
      "grad_norm": 9.5625,
      "learning_rate": 1.8093322606596945e-05,
      "loss": 0.9516,
      "step": 18960
    },
    {
      "epoch": 0.9538415124698311,
      "grad_norm": 56.75,
      "learning_rate": 1.809231697506034e-05,
      "loss": 1.3043,
      "step": 18970
    },
    {
      "epoch": 0.9543443282381335,
      "grad_norm": 8.3125,
      "learning_rate": 1.8091311343523733e-05,
      "loss": 1.2066,
      "step": 18980
    },
    {
      "epoch": 0.954847144006436,
      "grad_norm": 13.75,
      "learning_rate": 1.809030571198713e-05,
      "loss": 1.1852,
      "step": 18990
    },
    {
      "epoch": 0.9553499597747386,
      "grad_norm": 5.75,
      "learning_rate": 1.8089300080450525e-05,
      "loss": 0.6543,
      "step": 19000
    },
    {
      "epoch": 0.9553499597747386,
      "eval_accuracy": 0.5119980168567179,
      "eval_loss": 1.0287611484527588,
      "eval_runtime": 463.8661,
      "eval_samples_per_second": 86.965,
      "eval_steps_per_second": 86.965,
      "step": 19000
    },
    {
      "epoch": 0.955852775543041,
      "grad_norm": 7.15625,
      "learning_rate": 1.8088294448913918e-05,
      "loss": 0.7083,
      "step": 19010
    },
    {
      "epoch": 0.9563555913113435,
      "grad_norm": 6.40625,
      "learning_rate": 1.8087288817377317e-05,
      "loss": 0.9441,
      "step": 19020
    },
    {
      "epoch": 0.956858407079646,
      "grad_norm": 72.5,
      "learning_rate": 1.808628318584071e-05,
      "loss": 1.0899,
      "step": 19030
    },
    {
      "epoch": 0.9573612228479486,
      "grad_norm": 8.375,
      "learning_rate": 1.8085277554304105e-05,
      "loss": 0.9151,
      "step": 19040
    },
    {
      "epoch": 0.957864038616251,
      "grad_norm": 11.9375,
      "learning_rate": 1.80842719227675e-05,
      "loss": 1.2156,
      "step": 19050
    },
    {
      "epoch": 0.9583668543845535,
      "grad_norm": 24.75,
      "learning_rate": 1.8083266291230894e-05,
      "loss": 0.8549,
      "step": 19060
    },
    {
      "epoch": 0.958869670152856,
      "grad_norm": 24.5,
      "learning_rate": 1.808226065969429e-05,
      "loss": 0.8033,
      "step": 19070
    },
    {
      "epoch": 0.9593724859211585,
      "grad_norm": 28.375,
      "learning_rate": 1.8081255028157685e-05,
      "loss": 0.9534,
      "step": 19080
    },
    {
      "epoch": 0.959875301689461,
      "grad_norm": 37.25,
      "learning_rate": 1.8080249396621078e-05,
      "loss": 0.9813,
      "step": 19090
    },
    {
      "epoch": 0.9603781174577635,
      "grad_norm": 17.625,
      "learning_rate": 1.8079243765084477e-05,
      "loss": 1.0775,
      "step": 19100
    },
    {
      "epoch": 0.9608809332260659,
      "grad_norm": 36.0,
      "learning_rate": 1.807823813354787e-05,
      "loss": 0.8017,
      "step": 19110
    },
    {
      "epoch": 0.9613837489943685,
      "grad_norm": 32.5,
      "learning_rate": 1.8077232502011265e-05,
      "loss": 1.029,
      "step": 19120
    },
    {
      "epoch": 0.961886564762671,
      "grad_norm": 13.125,
      "learning_rate": 1.807622687047466e-05,
      "loss": 1.0136,
      "step": 19130
    },
    {
      "epoch": 0.9623893805309734,
      "grad_norm": 22.875,
      "learning_rate": 1.8075221238938054e-05,
      "loss": 1.0102,
      "step": 19140
    },
    {
      "epoch": 0.9628921962992759,
      "grad_norm": 15.3125,
      "learning_rate": 1.807421560740145e-05,
      "loss": 0.9161,
      "step": 19150
    },
    {
      "epoch": 0.9633950120675785,
      "grad_norm": 6.625,
      "learning_rate": 1.8073209975864846e-05,
      "loss": 0.9422,
      "step": 19160
    },
    {
      "epoch": 0.963897827835881,
      "grad_norm": 39.0,
      "learning_rate": 1.8072204344328238e-05,
      "loss": 0.936,
      "step": 19170
    },
    {
      "epoch": 0.9644006436041834,
      "grad_norm": 17.125,
      "learning_rate": 1.8071198712791634e-05,
      "loss": 0.9629,
      "step": 19180
    },
    {
      "epoch": 0.9649034593724859,
      "grad_norm": 24.875,
      "learning_rate": 1.807019308125503e-05,
      "loss": 1.1532,
      "step": 19190
    },
    {
      "epoch": 0.9654062751407884,
      "grad_norm": 6.90625,
      "learning_rate": 1.8069187449718422e-05,
      "loss": 1.2381,
      "step": 19200
    },
    {
      "epoch": 0.9659090909090909,
      "grad_norm": 6.0625,
      "learning_rate": 1.806818181818182e-05,
      "loss": 0.878,
      "step": 19210
    },
    {
      "epoch": 0.9664119066773934,
      "grad_norm": 11.75,
      "learning_rate": 1.8067176186645214e-05,
      "loss": 0.9768,
      "step": 19220
    },
    {
      "epoch": 0.9669147224456959,
      "grad_norm": 39.5,
      "learning_rate": 1.806617055510861e-05,
      "loss": 1.3144,
      "step": 19230
    },
    {
      "epoch": 0.9674175382139983,
      "grad_norm": 19.125,
      "learning_rate": 1.8065164923572006e-05,
      "loss": 0.8789,
      "step": 19240
    },
    {
      "epoch": 0.9679203539823009,
      "grad_norm": 25.0,
      "learning_rate": 1.8064159292035398e-05,
      "loss": 1.0344,
      "step": 19250
    },
    {
      "epoch": 0.9684231697506034,
      "grad_norm": 25.0,
      "learning_rate": 1.8063153660498794e-05,
      "loss": 1.0752,
      "step": 19260
    },
    {
      "epoch": 0.9689259855189059,
      "grad_norm": 10.8125,
      "learning_rate": 1.806214802896219e-05,
      "loss": 1.0643,
      "step": 19270
    },
    {
      "epoch": 0.9694288012872083,
      "grad_norm": 19.125,
      "learning_rate": 1.8061142397425583e-05,
      "loss": 0.9669,
      "step": 19280
    },
    {
      "epoch": 0.9699316170555109,
      "grad_norm": 15.5625,
      "learning_rate": 1.8060136765888982e-05,
      "loss": 0.7741,
      "step": 19290
    },
    {
      "epoch": 0.9704344328238134,
      "grad_norm": 25.125,
      "learning_rate": 1.8059131134352374e-05,
      "loss": 1.0341,
      "step": 19300
    },
    {
      "epoch": 0.9709372485921158,
      "grad_norm": 28.25,
      "learning_rate": 1.805812550281577e-05,
      "loss": 1.0745,
      "step": 19310
    },
    {
      "epoch": 0.9714400643604183,
      "grad_norm": 8.25,
      "learning_rate": 1.8057119871279166e-05,
      "loss": 0.9194,
      "step": 19320
    },
    {
      "epoch": 0.9719428801287209,
      "grad_norm": 29.0,
      "learning_rate": 1.805611423974256e-05,
      "loss": 1.2579,
      "step": 19330
    },
    {
      "epoch": 0.9724456958970233,
      "grad_norm": 16.75,
      "learning_rate": 1.8055108608205954e-05,
      "loss": 1.0866,
      "step": 19340
    },
    {
      "epoch": 0.9729485116653258,
      "grad_norm": 9.5625,
      "learning_rate": 1.805410297666935e-05,
      "loss": 0.8343,
      "step": 19350
    },
    {
      "epoch": 0.9734513274336283,
      "grad_norm": 20.125,
      "learning_rate": 1.8053097345132743e-05,
      "loss": 1.0838,
      "step": 19360
    },
    {
      "epoch": 0.9739541432019309,
      "grad_norm": 46.0,
      "learning_rate": 1.8052091713596142e-05,
      "loss": 1.2646,
      "step": 19370
    },
    {
      "epoch": 0.9744569589702333,
      "grad_norm": 15.125,
      "learning_rate": 1.8051086082059535e-05,
      "loss": 0.9618,
      "step": 19380
    },
    {
      "epoch": 0.9749597747385358,
      "grad_norm": 41.5,
      "learning_rate": 1.805008045052293e-05,
      "loss": 0.9206,
      "step": 19390
    },
    {
      "epoch": 0.9754625905068383,
      "grad_norm": 22.75,
      "learning_rate": 1.8049074818986326e-05,
      "loss": 1.0321,
      "step": 19400
    },
    {
      "epoch": 0.9759654062751408,
      "grad_norm": 19.25,
      "learning_rate": 1.804806918744972e-05,
      "loss": 1.0122,
      "step": 19410
    },
    {
      "epoch": 0.9764682220434433,
      "grad_norm": 30.625,
      "learning_rate": 1.8047063555913115e-05,
      "loss": 1.0318,
      "step": 19420
    },
    {
      "epoch": 0.9769710378117458,
      "grad_norm": 38.5,
      "learning_rate": 1.804605792437651e-05,
      "loss": 1.0234,
      "step": 19430
    },
    {
      "epoch": 0.9774738535800482,
      "grad_norm": 7.1875,
      "learning_rate": 1.8045052292839903e-05,
      "loss": 0.7192,
      "step": 19440
    },
    {
      "epoch": 0.9779766693483508,
      "grad_norm": 62.5,
      "learning_rate": 1.80440466613033e-05,
      "loss": 1.355,
      "step": 19450
    },
    {
      "epoch": 0.9784794851166533,
      "grad_norm": 37.0,
      "learning_rate": 1.8043041029766695e-05,
      "loss": 1.1513,
      "step": 19460
    },
    {
      "epoch": 0.9789823008849557,
      "grad_norm": 20.375,
      "learning_rate": 1.8042035398230087e-05,
      "loss": 0.907,
      "step": 19470
    },
    {
      "epoch": 0.9794851166532582,
      "grad_norm": 40.25,
      "learning_rate": 1.8041029766693487e-05,
      "loss": 0.9807,
      "step": 19480
    },
    {
      "epoch": 0.9799879324215608,
      "grad_norm": 27.375,
      "learning_rate": 1.804002413515688e-05,
      "loss": 0.8325,
      "step": 19490
    },
    {
      "epoch": 0.9804907481898633,
      "grad_norm": 9.375,
      "learning_rate": 1.8039018503620275e-05,
      "loss": 0.9498,
      "step": 19500
    },
    {
      "epoch": 0.9804907481898633,
      "eval_accuracy": 0.5127169062964799,
      "eval_loss": 1.0279489755630493,
      "eval_runtime": 464.7726,
      "eval_samples_per_second": 86.795,
      "eval_steps_per_second": 86.795,
      "step": 19500
    },
    {
      "epoch": 0.9809935639581657,
      "grad_norm": 16.125,
      "learning_rate": 1.803801287208367e-05,
      "loss": 0.8838,
      "step": 19510
    },
    {
      "epoch": 0.9814963797264682,
      "grad_norm": 62.5,
      "learning_rate": 1.8037007240547063e-05,
      "loss": 1.3944,
      "step": 19520
    },
    {
      "epoch": 0.9819991954947708,
      "grad_norm": 16.875,
      "learning_rate": 1.803600160901046e-05,
      "loss": 0.6796,
      "step": 19530
    },
    {
      "epoch": 0.9825020112630732,
      "grad_norm": 15.0,
      "learning_rate": 1.8034995977473855e-05,
      "loss": 1.0838,
      "step": 19540
    },
    {
      "epoch": 0.9830048270313757,
      "grad_norm": 14.6875,
      "learning_rate": 1.803399034593725e-05,
      "loss": 1.2927,
      "step": 19550
    },
    {
      "epoch": 0.9835076427996782,
      "grad_norm": 19.0,
      "learning_rate": 1.8032984714400647e-05,
      "loss": 0.9657,
      "step": 19560
    },
    {
      "epoch": 0.9840104585679806,
      "grad_norm": 33.0,
      "learning_rate": 1.803197908286404e-05,
      "loss": 0.8857,
      "step": 19570
    },
    {
      "epoch": 0.9845132743362832,
      "grad_norm": 25.875,
      "learning_rate": 1.8030973451327435e-05,
      "loss": 1.0286,
      "step": 19580
    },
    {
      "epoch": 0.9850160901045857,
      "grad_norm": 26.625,
      "learning_rate": 1.802996781979083e-05,
      "loss": 1.124,
      "step": 19590
    },
    {
      "epoch": 0.9855189058728882,
      "grad_norm": 5.5625,
      "learning_rate": 1.8028962188254224e-05,
      "loss": 1.153,
      "step": 19600
    },
    {
      "epoch": 0.9860217216411906,
      "grad_norm": 21.625,
      "learning_rate": 1.802795655671762e-05,
      "loss": 0.9554,
      "step": 19610
    },
    {
      "epoch": 0.9865245374094932,
      "grad_norm": 11.75,
      "learning_rate": 1.8026950925181015e-05,
      "loss": 0.7725,
      "step": 19620
    },
    {
      "epoch": 0.9870273531777957,
      "grad_norm": 9.9375,
      "learning_rate": 1.802594529364441e-05,
      "loss": 0.9303,
      "step": 19630
    },
    {
      "epoch": 0.9875301689460981,
      "grad_norm": 26.75,
      "learning_rate": 1.8024939662107807e-05,
      "loss": 1.1122,
      "step": 19640
    },
    {
      "epoch": 0.9880329847144006,
      "grad_norm": 36.5,
      "learning_rate": 1.80239340305712e-05,
      "loss": 0.9611,
      "step": 19650
    },
    {
      "epoch": 0.9885358004827032,
      "grad_norm": 75.0,
      "learning_rate": 1.8022928399034595e-05,
      "loss": 0.9523,
      "step": 19660
    },
    {
      "epoch": 0.9890386162510056,
      "grad_norm": 12.375,
      "learning_rate": 1.802192276749799e-05,
      "loss": 0.8791,
      "step": 19670
    },
    {
      "epoch": 0.9895414320193081,
      "grad_norm": 8.3125,
      "learning_rate": 1.8020917135961384e-05,
      "loss": 0.7772,
      "step": 19680
    },
    {
      "epoch": 0.9900442477876106,
      "grad_norm": 11.0625,
      "learning_rate": 1.801991150442478e-05,
      "loss": 0.6931,
      "step": 19690
    },
    {
      "epoch": 0.9905470635559132,
      "grad_norm": 52.75,
      "learning_rate": 1.8018905872888176e-05,
      "loss": 1.2384,
      "step": 19700
    },
    {
      "epoch": 0.9910498793242156,
      "grad_norm": 29.125,
      "learning_rate": 1.801790024135157e-05,
      "loss": 1.0724,
      "step": 19710
    },
    {
      "epoch": 0.9915526950925181,
      "grad_norm": 22.625,
      "learning_rate": 1.8016894609814964e-05,
      "loss": 0.8798,
      "step": 19720
    },
    {
      "epoch": 0.9920555108608206,
      "grad_norm": 53.0,
      "learning_rate": 1.801588897827836e-05,
      "loss": 1.164,
      "step": 19730
    },
    {
      "epoch": 0.9925583266291231,
      "grad_norm": 11.0625,
      "learning_rate": 1.8014883346741756e-05,
      "loss": 0.9659,
      "step": 19740
    },
    {
      "epoch": 0.9930611423974256,
      "grad_norm": 9.9375,
      "learning_rate": 1.801387771520515e-05,
      "loss": 0.8944,
      "step": 19750
    },
    {
      "epoch": 0.9935639581657281,
      "grad_norm": 6.53125,
      "learning_rate": 1.8012872083668544e-05,
      "loss": 1.1573,
      "step": 19760
    },
    {
      "epoch": 0.9940667739340305,
      "grad_norm": 24.0,
      "learning_rate": 1.801186645213194e-05,
      "loss": 0.9801,
      "step": 19770
    },
    {
      "epoch": 0.9945695897023331,
      "grad_norm": 39.0,
      "learning_rate": 1.8010860820595336e-05,
      "loss": 0.988,
      "step": 19780
    },
    {
      "epoch": 0.9950724054706356,
      "grad_norm": 29.625,
      "learning_rate": 1.800985518905873e-05,
      "loss": 1.1283,
      "step": 19790
    },
    {
      "epoch": 0.995575221238938,
      "grad_norm": 8.4375,
      "learning_rate": 1.8008849557522124e-05,
      "loss": 0.9538,
      "step": 19800
    },
    {
      "epoch": 0.9960780370072405,
      "grad_norm": 23.875,
      "learning_rate": 1.800784392598552e-05,
      "loss": 1.1283,
      "step": 19810
    },
    {
      "epoch": 0.9965808527755431,
      "grad_norm": 27.75,
      "learning_rate": 1.8006838294448916e-05,
      "loss": 1.3719,
      "step": 19820
    },
    {
      "epoch": 0.9970836685438456,
      "grad_norm": 18.75,
      "learning_rate": 1.8005832662912312e-05,
      "loss": 1.0065,
      "step": 19830
    },
    {
      "epoch": 0.997586484312148,
      "grad_norm": 9.875,
      "learning_rate": 1.8004827031375704e-05,
      "loss": 0.8001,
      "step": 19840
    },
    {
      "epoch": 0.9980893000804505,
      "grad_norm": 23.625,
      "learning_rate": 1.80038213998391e-05,
      "loss": 0.838,
      "step": 19850
    },
    {
      "epoch": 0.9985921158487531,
      "grad_norm": 10.875,
      "learning_rate": 1.8002815768302496e-05,
      "loss": 0.9935,
      "step": 19860
    },
    {
      "epoch": 0.9990949316170555,
      "grad_norm": 21.5,
      "learning_rate": 1.8001810136765892e-05,
      "loss": 0.9495,
      "step": 19870
    },
    {
      "epoch": 0.999597747385358,
      "grad_norm": 21.0,
      "learning_rate": 1.8000804505229284e-05,
      "loss": 1.3054,
      "step": 19880
    },
    {
      "epoch": 1.0001005631536606,
      "grad_norm": 12.8125,
      "learning_rate": 1.799979887369268e-05,
      "loss": 0.9558,
      "step": 19890
    },
    {
      "epoch": 1.000603378921963,
      "grad_norm": 23.5,
      "learning_rate": 1.7998793242156076e-05,
      "loss": 0.9212,
      "step": 19900
    },
    {
      "epoch": 1.0011061946902655,
      "grad_norm": 36.5,
      "learning_rate": 1.7997787610619472e-05,
      "loss": 1.2904,
      "step": 19910
    },
    {
      "epoch": 1.001609010458568,
      "grad_norm": 17.625,
      "learning_rate": 1.7996781979082865e-05,
      "loss": 0.8919,
      "step": 19920
    },
    {
      "epoch": 1.0021118262268705,
      "grad_norm": 52.0,
      "learning_rate": 1.799577634754626e-05,
      "loss": 1.0397,
      "step": 19930
    },
    {
      "epoch": 1.002614641995173,
      "grad_norm": 12.4375,
      "learning_rate": 1.7994770716009656e-05,
      "loss": 0.9062,
      "step": 19940
    },
    {
      "epoch": 1.0031174577634754,
      "grad_norm": 20.5,
      "learning_rate": 1.7993765084473052e-05,
      "loss": 1.1182,
      "step": 19950
    },
    {
      "epoch": 1.0036202735317779,
      "grad_norm": 12.8125,
      "learning_rate": 1.7992759452936445e-05,
      "loss": 1.035,
      "step": 19960
    },
    {
      "epoch": 1.0041230893000805,
      "grad_norm": 26.5,
      "learning_rate": 1.799175382139984e-05,
      "loss": 0.8186,
      "step": 19970
    },
    {
      "epoch": 1.004625905068383,
      "grad_norm": 6.03125,
      "learning_rate": 1.7990748189863236e-05,
      "loss": 0.7744,
      "step": 19980
    },
    {
      "epoch": 1.0051287208366855,
      "grad_norm": 30.25,
      "learning_rate": 1.798974255832663e-05,
      "loss": 0.9524,
      "step": 19990
    },
    {
      "epoch": 1.005631536604988,
      "grad_norm": 41.0,
      "learning_rate": 1.7988736926790025e-05,
      "loss": 1.1021,
      "step": 20000
    },
    {
      "epoch": 1.005631536604988,
      "eval_accuracy": 0.5122706990580069,
      "eval_loss": 1.0278122425079346,
      "eval_runtime": 464.7435,
      "eval_samples_per_second": 86.801,
      "eval_steps_per_second": 86.801,
      "step": 20000
    },
    {
      "epoch": 1.0061343523732904,
      "grad_norm": 16.125,
      "learning_rate": 1.798773129525342e-05,
      "loss": 0.8576,
      "step": 20010
    },
    {
      "epoch": 1.0066371681415929,
      "grad_norm": 16.625,
      "learning_rate": 1.7986725663716817e-05,
      "loss": 0.8863,
      "step": 20020
    },
    {
      "epoch": 1.0071399839098953,
      "grad_norm": 14.9375,
      "learning_rate": 1.7985720032180212e-05,
      "loss": 1.0501,
      "step": 20030
    },
    {
      "epoch": 1.0076427996781978,
      "grad_norm": 18.375,
      "learning_rate": 1.7984714400643605e-05,
      "loss": 1.1526,
      "step": 20040
    },
    {
      "epoch": 1.0081456154465005,
      "grad_norm": 14.0,
      "learning_rate": 1.7983708769107e-05,
      "loss": 1.2451,
      "step": 20050
    },
    {
      "epoch": 1.008648431214803,
      "grad_norm": 31.375,
      "learning_rate": 1.7982703137570397e-05,
      "loss": 0.9783,
      "step": 20060
    },
    {
      "epoch": 1.0091512469831054,
      "grad_norm": 27.375,
      "learning_rate": 1.798169750603379e-05,
      "loss": 0.6951,
      "step": 20070
    },
    {
      "epoch": 1.009654062751408,
      "grad_norm": 7.03125,
      "learning_rate": 1.7980691874497185e-05,
      "loss": 1.0121,
      "step": 20080
    },
    {
      "epoch": 1.0101568785197104,
      "grad_norm": 23.125,
      "learning_rate": 1.797968624296058e-05,
      "loss": 1.0822,
      "step": 20090
    },
    {
      "epoch": 1.0106596942880128,
      "grad_norm": 14.1875,
      "learning_rate": 1.7978680611423977e-05,
      "loss": 1.1156,
      "step": 20100
    },
    {
      "epoch": 1.0111625100563153,
      "grad_norm": 56.25,
      "learning_rate": 1.7977674979887373e-05,
      "loss": 1.0195,
      "step": 20110
    },
    {
      "epoch": 1.0116653258246178,
      "grad_norm": 9.1875,
      "learning_rate": 1.7976669348350765e-05,
      "loss": 0.891,
      "step": 20120
    },
    {
      "epoch": 1.0121681415929205,
      "grad_norm": 33.5,
      "learning_rate": 1.797566371681416e-05,
      "loss": 0.9067,
      "step": 20130
    },
    {
      "epoch": 1.012670957361223,
      "grad_norm": 29.75,
      "learning_rate": 1.7974658085277557e-05,
      "loss": 1.0456,
      "step": 20140
    },
    {
      "epoch": 1.0131737731295254,
      "grad_norm": 4.28125,
      "learning_rate": 1.797365245374095e-05,
      "loss": 0.9581,
      "step": 20150
    },
    {
      "epoch": 1.0136765888978279,
      "grad_norm": 12.875,
      "learning_rate": 1.7972646822204345e-05,
      "loss": 0.9097,
      "step": 20160
    },
    {
      "epoch": 1.0141794046661303,
      "grad_norm": 32.25,
      "learning_rate": 1.797164119066774e-05,
      "loss": 1.026,
      "step": 20170
    },
    {
      "epoch": 1.0146822204344328,
      "grad_norm": 11.1875,
      "learning_rate": 1.7970635559131137e-05,
      "loss": 0.8718,
      "step": 20180
    },
    {
      "epoch": 1.0151850362027353,
      "grad_norm": 61.5,
      "learning_rate": 1.7969629927594533e-05,
      "loss": 1.1184,
      "step": 20190
    },
    {
      "epoch": 1.0156878519710377,
      "grad_norm": 3.4375,
      "learning_rate": 1.7968624296057925e-05,
      "loss": 1.0073,
      "step": 20200
    },
    {
      "epoch": 1.0161906677393404,
      "grad_norm": 14.4375,
      "learning_rate": 1.796761866452132e-05,
      "loss": 0.9546,
      "step": 20210
    },
    {
      "epoch": 1.0166934835076429,
      "grad_norm": 30.75,
      "learning_rate": 1.7966613032984717e-05,
      "loss": 0.8237,
      "step": 20220
    },
    {
      "epoch": 1.0171962992759453,
      "grad_norm": 8.375,
      "learning_rate": 1.796560740144811e-05,
      "loss": 0.9486,
      "step": 20230
    },
    {
      "epoch": 1.0176991150442478,
      "grad_norm": 52.0,
      "learning_rate": 1.7964601769911506e-05,
      "loss": 1.0439,
      "step": 20240
    },
    {
      "epoch": 1.0182019308125503,
      "grad_norm": 13.4375,
      "learning_rate": 1.79635961383749e-05,
      "loss": 1.0821,
      "step": 20250
    },
    {
      "epoch": 1.0187047465808527,
      "grad_norm": 42.5,
      "learning_rate": 1.7962590506838294e-05,
      "loss": 1.1346,
      "step": 20260
    },
    {
      "epoch": 1.0192075623491552,
      "grad_norm": 80.0,
      "learning_rate": 1.7961584875301693e-05,
      "loss": 1.2817,
      "step": 20270
    },
    {
      "epoch": 1.0197103781174577,
      "grad_norm": 73.0,
      "learning_rate": 1.7960579243765086e-05,
      "loss": 0.9632,
      "step": 20280
    },
    {
      "epoch": 1.0202131938857602,
      "grad_norm": 20.375,
      "learning_rate": 1.795957361222848e-05,
      "loss": 0.7583,
      "step": 20290
    },
    {
      "epoch": 1.0207160096540628,
      "grad_norm": 32.5,
      "learning_rate": 1.7958567980691877e-05,
      "loss": 1.0943,
      "step": 20300
    },
    {
      "epoch": 1.0212188254223653,
      "grad_norm": 19.875,
      "learning_rate": 1.795756234915527e-05,
      "loss": 1.2049,
      "step": 20310
    },
    {
      "epoch": 1.0217216411906678,
      "grad_norm": 86.0,
      "learning_rate": 1.7956556717618666e-05,
      "loss": 0.9441,
      "step": 20320
    },
    {
      "epoch": 1.0222244569589702,
      "grad_norm": 16.875,
      "learning_rate": 1.7955551086082062e-05,
      "loss": 0.9337,
      "step": 20330
    },
    {
      "epoch": 1.0227272727272727,
      "grad_norm": 16.25,
      "learning_rate": 1.7954545454545454e-05,
      "loss": 1.2926,
      "step": 20340
    },
    {
      "epoch": 1.0232300884955752,
      "grad_norm": 54.0,
      "learning_rate": 1.7953539823008853e-05,
      "loss": 1.3617,
      "step": 20350
    },
    {
      "epoch": 1.0237329042638776,
      "grad_norm": 4.125,
      "learning_rate": 1.7952534191472246e-05,
      "loss": 0.8597,
      "step": 20360
    },
    {
      "epoch": 1.02423572003218,
      "grad_norm": 6.9375,
      "learning_rate": 1.7951528559935642e-05,
      "loss": 1.1577,
      "step": 20370
    },
    {
      "epoch": 1.0247385358004828,
      "grad_norm": 28.375,
      "learning_rate": 1.7950522928399038e-05,
      "loss": 1.203,
      "step": 20380
    },
    {
      "epoch": 1.0252413515687853,
      "grad_norm": 31.625,
      "learning_rate": 1.794951729686243e-05,
      "loss": 1.1522,
      "step": 20390
    },
    {
      "epoch": 1.0257441673370877,
      "grad_norm": 88.5,
      "learning_rate": 1.7948511665325826e-05,
      "loss": 1.1883,
      "step": 20400
    },
    {
      "epoch": 1.0262469831053902,
      "grad_norm": 27.125,
      "learning_rate": 1.7947506033789222e-05,
      "loss": 0.9953,
      "step": 20410
    },
    {
      "epoch": 1.0267497988736927,
      "grad_norm": 39.75,
      "learning_rate": 1.7946500402252614e-05,
      "loss": 1.2923,
      "step": 20420
    },
    {
      "epoch": 1.0272526146419951,
      "grad_norm": 11.9375,
      "learning_rate": 1.7945494770716014e-05,
      "loss": 0.7893,
      "step": 20430
    },
    {
      "epoch": 1.0277554304102976,
      "grad_norm": 13.5,
      "learning_rate": 1.7944489139179406e-05,
      "loss": 1.0324,
      "step": 20440
    },
    {
      "epoch": 1.0282582461786,
      "grad_norm": 24.0,
      "learning_rate": 1.7943483507642802e-05,
      "loss": 1.0502,
      "step": 20450
    },
    {
      "epoch": 1.0287610619469028,
      "grad_norm": 38.75,
      "learning_rate": 1.7942477876106198e-05,
      "loss": 0.8382,
      "step": 20460
    },
    {
      "epoch": 1.0292638777152052,
      "grad_norm": 17.875,
      "learning_rate": 1.794147224456959e-05,
      "loss": 1.1179,
      "step": 20470
    },
    {
      "epoch": 1.0297666934835077,
      "grad_norm": 14.4375,
      "learning_rate": 1.7940466613032986e-05,
      "loss": 1.0102,
      "step": 20480
    },
    {
      "epoch": 1.0302695092518102,
      "grad_norm": 6.84375,
      "learning_rate": 1.7939460981496382e-05,
      "loss": 0.8806,
      "step": 20490
    },
    {
      "epoch": 1.0307723250201126,
      "grad_norm": 28.125,
      "learning_rate": 1.7938455349959775e-05,
      "loss": 0.8708,
      "step": 20500
    },
    {
      "epoch": 1.0307723250201126,
      "eval_accuracy": 0.5117253346554289,
      "eval_loss": 1.0275936126708984,
      "eval_runtime": 465.1747,
      "eval_samples_per_second": 86.72,
      "eval_steps_per_second": 86.72,
      "step": 20500
    },
    {
      "epoch": 1.031275140788415,
      "grad_norm": 19.25,
      "learning_rate": 1.793744971842317e-05,
      "loss": 0.879,
      "step": 20510
    },
    {
      "epoch": 1.0317779565567176,
      "grad_norm": 52.5,
      "learning_rate": 1.7936444086886566e-05,
      "loss": 0.9911,
      "step": 20520
    },
    {
      "epoch": 1.03228077232502,
      "grad_norm": 13.125,
      "learning_rate": 1.793543845534996e-05,
      "loss": 1.0048,
      "step": 20530
    },
    {
      "epoch": 1.0327835880933227,
      "grad_norm": 13.75,
      "learning_rate": 1.7934432823813358e-05,
      "loss": 1.1395,
      "step": 20540
    },
    {
      "epoch": 1.0332864038616252,
      "grad_norm": 26.375,
      "learning_rate": 1.793342719227675e-05,
      "loss": 1.181,
      "step": 20550
    },
    {
      "epoch": 1.0337892196299276,
      "grad_norm": 25.125,
      "learning_rate": 1.7932421560740147e-05,
      "loss": 1.117,
      "step": 20560
    },
    {
      "epoch": 1.0342920353982301,
      "grad_norm": 12.1875,
      "learning_rate": 1.7931415929203542e-05,
      "loss": 0.848,
      "step": 20570
    },
    {
      "epoch": 1.0347948511665326,
      "grad_norm": 23.0,
      "learning_rate": 1.7930410297666935e-05,
      "loss": 0.7857,
      "step": 20580
    },
    {
      "epoch": 1.035297666934835,
      "grad_norm": 8.3125,
      "learning_rate": 1.792940466613033e-05,
      "loss": 1.0403,
      "step": 20590
    },
    {
      "epoch": 1.0358004827031375,
      "grad_norm": 7.21875,
      "learning_rate": 1.7928399034593727e-05,
      "loss": 1.1314,
      "step": 20600
    },
    {
      "epoch": 1.03630329847144,
      "grad_norm": 34.25,
      "learning_rate": 1.792739340305712e-05,
      "loss": 0.8839,
      "step": 20610
    },
    {
      "epoch": 1.0368061142397424,
      "grad_norm": 17.125,
      "learning_rate": 1.792638777152052e-05,
      "loss": 1.2083,
      "step": 20620
    },
    {
      "epoch": 1.0373089300080451,
      "grad_norm": 17.0,
      "learning_rate": 1.792538213998391e-05,
      "loss": 1.0208,
      "step": 20630
    },
    {
      "epoch": 1.0378117457763476,
      "grad_norm": 8.8125,
      "learning_rate": 1.7924376508447307e-05,
      "loss": 0.9831,
      "step": 20640
    },
    {
      "epoch": 1.03831456154465,
      "grad_norm": 9.375,
      "learning_rate": 1.7923370876910703e-05,
      "loss": 1.0773,
      "step": 20650
    },
    {
      "epoch": 1.0388173773129525,
      "grad_norm": 10.9375,
      "learning_rate": 1.7922365245374095e-05,
      "loss": 0.6951,
      "step": 20660
    },
    {
      "epoch": 1.039320193081255,
      "grad_norm": 38.75,
      "learning_rate": 1.792135961383749e-05,
      "loss": 1.0535,
      "step": 20670
    },
    {
      "epoch": 1.0398230088495575,
      "grad_norm": 18.625,
      "learning_rate": 1.7920353982300887e-05,
      "loss": 0.7994,
      "step": 20680
    },
    {
      "epoch": 1.04032582461786,
      "grad_norm": 95.5,
      "learning_rate": 1.791934835076428e-05,
      "loss": 1.2618,
      "step": 20690
    },
    {
      "epoch": 1.0408286403861624,
      "grad_norm": 18.75,
      "learning_rate": 1.791834271922768e-05,
      "loss": 1.2127,
      "step": 20700
    },
    {
      "epoch": 1.041331456154465,
      "grad_norm": 9.0625,
      "learning_rate": 1.791733708769107e-05,
      "loss": 0.9537,
      "step": 20710
    },
    {
      "epoch": 1.0418342719227676,
      "grad_norm": 34.75,
      "learning_rate": 1.7916331456154467e-05,
      "loss": 1.252,
      "step": 20720
    },
    {
      "epoch": 1.04233708769107,
      "grad_norm": 17.625,
      "learning_rate": 1.7915325824617863e-05,
      "loss": 1.0437,
      "step": 20730
    },
    {
      "epoch": 1.0428399034593725,
      "grad_norm": 12.4375,
      "learning_rate": 1.7914320193081255e-05,
      "loss": 1.047,
      "step": 20740
    },
    {
      "epoch": 1.043342719227675,
      "grad_norm": 11.5625,
      "learning_rate": 1.791331456154465e-05,
      "loss": 1.0491,
      "step": 20750
    },
    {
      "epoch": 1.0438455349959774,
      "grad_norm": 27.75,
      "learning_rate": 1.7912308930008047e-05,
      "loss": 0.8504,
      "step": 20760
    },
    {
      "epoch": 1.04434835076428,
      "grad_norm": 5.71875,
      "learning_rate": 1.791130329847144e-05,
      "loss": 0.8787,
      "step": 20770
    },
    {
      "epoch": 1.0448511665325824,
      "grad_norm": 20.25,
      "learning_rate": 1.7910297666934836e-05,
      "loss": 0.9655,
      "step": 20780
    },
    {
      "epoch": 1.045353982300885,
      "grad_norm": 21.625,
      "learning_rate": 1.790929203539823e-05,
      "loss": 1.0036,
      "step": 20790
    },
    {
      "epoch": 1.0458567980691875,
      "grad_norm": 34.25,
      "learning_rate": 1.7908286403861624e-05,
      "loss": 0.8298,
      "step": 20800
    },
    {
      "epoch": 1.04635961383749,
      "grad_norm": 58.25,
      "learning_rate": 1.7907280772325023e-05,
      "loss": 0.8104,
      "step": 20810
    },
    {
      "epoch": 1.0468624296057925,
      "grad_norm": 6.1875,
      "learning_rate": 1.7906275140788416e-05,
      "loss": 0.8737,
      "step": 20820
    },
    {
      "epoch": 1.047365245374095,
      "grad_norm": 46.75,
      "learning_rate": 1.790526950925181e-05,
      "loss": 0.8487,
      "step": 20830
    },
    {
      "epoch": 1.0478680611423974,
      "grad_norm": 5.59375,
      "learning_rate": 1.7904263877715207e-05,
      "loss": 1.2296,
      "step": 20840
    },
    {
      "epoch": 1.0483708769106999,
      "grad_norm": 56.5,
      "learning_rate": 1.79032582461786e-05,
      "loss": 1.2816,
      "step": 20850
    },
    {
      "epoch": 1.0488736926790023,
      "grad_norm": 64.5,
      "learning_rate": 1.7902252614641996e-05,
      "loss": 0.915,
      "step": 20860
    },
    {
      "epoch": 1.049376508447305,
      "grad_norm": 8.375,
      "learning_rate": 1.7901246983105392e-05,
      "loss": 0.7916,
      "step": 20870
    },
    {
      "epoch": 1.0498793242156075,
      "grad_norm": 10.3125,
      "learning_rate": 1.7900241351568784e-05,
      "loss": 0.8386,
      "step": 20880
    },
    {
      "epoch": 1.05038213998391,
      "grad_norm": 20.5,
      "learning_rate": 1.7899235720032184e-05,
      "loss": 1.1192,
      "step": 20890
    },
    {
      "epoch": 1.0508849557522124,
      "grad_norm": 39.5,
      "learning_rate": 1.7898230088495576e-05,
      "loss": 1.1573,
      "step": 20900
    },
    {
      "epoch": 1.0513877715205149,
      "grad_norm": 43.75,
      "learning_rate": 1.7897224456958972e-05,
      "loss": 1.0448,
      "step": 20910
    },
    {
      "epoch": 1.0518905872888173,
      "grad_norm": 80.0,
      "learning_rate": 1.7896218825422368e-05,
      "loss": 1.1349,
      "step": 20920
    },
    {
      "epoch": 1.0523934030571198,
      "grad_norm": 13.375,
      "learning_rate": 1.789521319388576e-05,
      "loss": 0.8388,
      "step": 20930
    },
    {
      "epoch": 1.0528962188254223,
      "grad_norm": 28.875,
      "learning_rate": 1.7894207562349156e-05,
      "loss": 1.6216,
      "step": 20940
    },
    {
      "epoch": 1.053399034593725,
      "grad_norm": 18.5,
      "learning_rate": 1.7893201930812552e-05,
      "loss": 0.9621,
      "step": 20950
    },
    {
      "epoch": 1.0539018503620274,
      "grad_norm": 4.3125,
      "learning_rate": 1.7892196299275944e-05,
      "loss": 0.7724,
      "step": 20960
    },
    {
      "epoch": 1.05440466613033,
      "grad_norm": 9.3125,
      "learning_rate": 1.7891190667739344e-05,
      "loss": 1.0626,
      "step": 20970
    },
    {
      "epoch": 1.0549074818986324,
      "grad_norm": 58.75,
      "learning_rate": 1.7890185036202736e-05,
      "loss": 0.951,
      "step": 20980
    },
    {
      "epoch": 1.0554102976669348,
      "grad_norm": 45.75,
      "learning_rate": 1.7889179404666132e-05,
      "loss": 0.9314,
      "step": 20990
    },
    {
      "epoch": 1.0559131134352373,
      "grad_norm": 15.625,
      "learning_rate": 1.7888173773129528e-05,
      "loss": 1.0461,
      "step": 21000
    },
    {
      "epoch": 1.0559131134352373,
      "eval_accuracy": 0.512097174020823,
      "eval_loss": 1.0282175540924072,
      "eval_runtime": 464.6404,
      "eval_samples_per_second": 86.82,
      "eval_steps_per_second": 86.82,
      "step": 21000
    },
    {
      "epoch": 1.0564159292035398,
      "grad_norm": 5.84375,
      "learning_rate": 1.788716814159292e-05,
      "loss": 1.0763,
      "step": 21010
    },
    {
      "epoch": 1.0569187449718422,
      "grad_norm": 13.4375,
      "learning_rate": 1.7886162510056316e-05,
      "loss": 1.0952,
      "step": 21020
    },
    {
      "epoch": 1.0574215607401447,
      "grad_norm": 35.25,
      "learning_rate": 1.7885156878519712e-05,
      "loss": 1.2184,
      "step": 21030
    },
    {
      "epoch": 1.0579243765084474,
      "grad_norm": 16.125,
      "learning_rate": 1.7884151246983105e-05,
      "loss": 0.6456,
      "step": 21040
    },
    {
      "epoch": 1.0584271922767499,
      "grad_norm": 11.9375,
      "learning_rate": 1.78831456154465e-05,
      "loss": 0.7569,
      "step": 21050
    },
    {
      "epoch": 1.0589300080450523,
      "grad_norm": 5.125,
      "learning_rate": 1.7882139983909897e-05,
      "loss": 0.8019,
      "step": 21060
    },
    {
      "epoch": 1.0594328238133548,
      "grad_norm": 7.84375,
      "learning_rate": 1.7881134352373292e-05,
      "loss": 1.0129,
      "step": 21070
    },
    {
      "epoch": 1.0599356395816573,
      "grad_norm": 14.6875,
      "learning_rate": 1.7880128720836688e-05,
      "loss": 0.7816,
      "step": 21080
    },
    {
      "epoch": 1.0604384553499597,
      "grad_norm": 22.625,
      "learning_rate": 1.787912308930008e-05,
      "loss": 1.0689,
      "step": 21090
    },
    {
      "epoch": 1.0609412711182622,
      "grad_norm": 24.375,
      "learning_rate": 1.7878117457763477e-05,
      "loss": 1.1388,
      "step": 21100
    },
    {
      "epoch": 1.0614440868865647,
      "grad_norm": 52.0,
      "learning_rate": 1.7877111826226873e-05,
      "loss": 1.4824,
      "step": 21110
    },
    {
      "epoch": 1.0619469026548674,
      "grad_norm": 32.25,
      "learning_rate": 1.7876106194690265e-05,
      "loss": 1.0978,
      "step": 21120
    },
    {
      "epoch": 1.0624497184231698,
      "grad_norm": 43.25,
      "learning_rate": 1.787510056315366e-05,
      "loss": 0.9738,
      "step": 21130
    },
    {
      "epoch": 1.0629525341914723,
      "grad_norm": 31.25,
      "learning_rate": 1.7874094931617057e-05,
      "loss": 1.2175,
      "step": 21140
    },
    {
      "epoch": 1.0634553499597748,
      "grad_norm": 14.4375,
      "learning_rate": 1.7873089300080453e-05,
      "loss": 0.8389,
      "step": 21150
    },
    {
      "epoch": 1.0639581657280772,
      "grad_norm": 5.875,
      "learning_rate": 1.787208366854385e-05,
      "loss": 0.94,
      "step": 21160
    },
    {
      "epoch": 1.0644609814963797,
      "grad_norm": 32.25,
      "learning_rate": 1.787107803700724e-05,
      "loss": 1.2419,
      "step": 21170
    },
    {
      "epoch": 1.0649637972646822,
      "grad_norm": 8.4375,
      "learning_rate": 1.7870072405470637e-05,
      "loss": 0.942,
      "step": 21180
    },
    {
      "epoch": 1.0654666130329846,
      "grad_norm": 24.75,
      "learning_rate": 1.7869066773934033e-05,
      "loss": 0.6711,
      "step": 21190
    },
    {
      "epoch": 1.0659694288012873,
      "grad_norm": 33.0,
      "learning_rate": 1.7868061142397425e-05,
      "loss": 0.7403,
      "step": 21200
    },
    {
      "epoch": 1.0664722445695898,
      "grad_norm": 6.90625,
      "learning_rate": 1.786705551086082e-05,
      "loss": 0.6906,
      "step": 21210
    },
    {
      "epoch": 1.0669750603378922,
      "grad_norm": 20.25,
      "learning_rate": 1.7866049879324217e-05,
      "loss": 1.0858,
      "step": 21220
    },
    {
      "epoch": 1.0674778761061947,
      "grad_norm": 13.5625,
      "learning_rate": 1.7865044247787613e-05,
      "loss": 1.061,
      "step": 21230
    },
    {
      "epoch": 1.0679806918744972,
      "grad_norm": 26.25,
      "learning_rate": 1.786403861625101e-05,
      "loss": 0.8693,
      "step": 21240
    },
    {
      "epoch": 1.0684835076427996,
      "grad_norm": 60.0,
      "learning_rate": 1.78630329847144e-05,
      "loss": 1.0064,
      "step": 21250
    },
    {
      "epoch": 1.0689863234111021,
      "grad_norm": 5.78125,
      "learning_rate": 1.7862027353177797e-05,
      "loss": 1.2933,
      "step": 21260
    },
    {
      "epoch": 1.0694891391794046,
      "grad_norm": 24.25,
      "learning_rate": 1.7861021721641193e-05,
      "loss": 1.3688,
      "step": 21270
    },
    {
      "epoch": 1.069991954947707,
      "grad_norm": 9.8125,
      "learning_rate": 1.7860016090104586e-05,
      "loss": 0.9372,
      "step": 21280
    },
    {
      "epoch": 1.0704947707160097,
      "grad_norm": 12.9375,
      "learning_rate": 1.785901045856798e-05,
      "loss": 0.7877,
      "step": 21290
    },
    {
      "epoch": 1.0709975864843122,
      "grad_norm": 9.5,
      "learning_rate": 1.7858004827031377e-05,
      "loss": 0.9761,
      "step": 21300
    },
    {
      "epoch": 1.0715004022526147,
      "grad_norm": 12.375,
      "learning_rate": 1.7856999195494773e-05,
      "loss": 0.9644,
      "step": 21310
    },
    {
      "epoch": 1.0720032180209171,
      "grad_norm": 43.25,
      "learning_rate": 1.7855993563958166e-05,
      "loss": 1.2843,
      "step": 21320
    },
    {
      "epoch": 1.0725060337892196,
      "grad_norm": 7.75,
      "learning_rate": 1.785498793242156e-05,
      "loss": 0.9162,
      "step": 21330
    },
    {
      "epoch": 1.073008849557522,
      "grad_norm": 17.0,
      "learning_rate": 1.7853982300884957e-05,
      "loss": 1.0219,
      "step": 21340
    },
    {
      "epoch": 1.0735116653258245,
      "grad_norm": 20.25,
      "learning_rate": 1.7852976669348353e-05,
      "loss": 1.0869,
      "step": 21350
    },
    {
      "epoch": 1.0740144810941272,
      "grad_norm": 39.75,
      "learning_rate": 1.785197103781175e-05,
      "loss": 1.3475,
      "step": 21360
    },
    {
      "epoch": 1.0745172968624297,
      "grad_norm": 12.0625,
      "learning_rate": 1.785096540627514e-05,
      "loss": 0.7761,
      "step": 21370
    },
    {
      "epoch": 1.0750201126307322,
      "grad_norm": 22.375,
      "learning_rate": 1.7849959774738538e-05,
      "loss": 1.1574,
      "step": 21380
    },
    {
      "epoch": 1.0755229283990346,
      "grad_norm": 37.0,
      "learning_rate": 1.7848954143201933e-05,
      "loss": 1.2209,
      "step": 21390
    },
    {
      "epoch": 1.076025744167337,
      "grad_norm": 8.9375,
      "learning_rate": 1.7847948511665326e-05,
      "loss": 0.8283,
      "step": 21400
    },
    {
      "epoch": 1.0765285599356396,
      "grad_norm": 66.5,
      "learning_rate": 1.7846942880128722e-05,
      "loss": 1.1853,
      "step": 21410
    },
    {
      "epoch": 1.077031375703942,
      "grad_norm": 12.5,
      "learning_rate": 1.7845937248592118e-05,
      "loss": 0.9759,
      "step": 21420
    },
    {
      "epoch": 1.0775341914722445,
      "grad_norm": 32.75,
      "learning_rate": 1.7844931617055514e-05,
      "loss": 1.1772,
      "step": 21430
    },
    {
      "epoch": 1.078037007240547,
      "grad_norm": 18.375,
      "learning_rate": 1.784392598551891e-05,
      "loss": 1.0038,
      "step": 21440
    },
    {
      "epoch": 1.0785398230088497,
      "grad_norm": 7.03125,
      "learning_rate": 1.7842920353982302e-05,
      "loss": 1.0123,
      "step": 21450
    },
    {
      "epoch": 1.0790426387771521,
      "grad_norm": 8.0625,
      "learning_rate": 1.7841914722445698e-05,
      "loss": 1.1355,
      "step": 21460
    },
    {
      "epoch": 1.0795454545454546,
      "grad_norm": 47.5,
      "learning_rate": 1.7840909090909094e-05,
      "loss": 1.0454,
      "step": 21470
    },
    {
      "epoch": 1.080048270313757,
      "grad_norm": 41.0,
      "learning_rate": 1.7839903459372486e-05,
      "loss": 0.9378,
      "step": 21480
    },
    {
      "epoch": 1.0805510860820595,
      "grad_norm": 27.875,
      "learning_rate": 1.7838897827835882e-05,
      "loss": 1.154,
      "step": 21490
    },
    {
      "epoch": 1.081053901850362,
      "grad_norm": 52.75,
      "learning_rate": 1.7837892196299278e-05,
      "loss": 1.2339,
      "step": 21500
    },
    {
      "epoch": 1.081053901850362,
      "eval_accuracy": 0.5117253346554289,
      "eval_loss": 1.0274715423583984,
      "eval_runtime": 463.7635,
      "eval_samples_per_second": 86.984,
      "eval_steps_per_second": 86.984,
      "step": 21500
    }
  ],
  "logging_steps": 10,
  "max_steps": 198880,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "total_flos": 0.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
