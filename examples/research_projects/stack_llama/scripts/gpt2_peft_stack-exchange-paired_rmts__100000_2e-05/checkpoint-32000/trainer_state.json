{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.6090104585679805,
  "eval_steps": 500,
  "global_step": 32000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.000502815768302494,
      "grad_norm": 17.25,
      "learning_rate": 1.9998994368463394e-05,
      "loss": 1.755,
      "step": 10
    },
    {
      "epoch": 0.001005631536604988,
      "grad_norm": 70.0,
      "learning_rate": 1.9997988736926793e-05,
      "loss": 2.3226,
      "step": 20
    },
    {
      "epoch": 0.0015084473049074819,
      "grad_norm": 196.0,
      "learning_rate": 1.9996983105390186e-05,
      "loss": 3.7719,
      "step": 30
    },
    {
      "epoch": 0.002011263073209976,
      "grad_norm": 40.0,
      "learning_rate": 1.9995977473853582e-05,
      "loss": 2.4239,
      "step": 40
    },
    {
      "epoch": 0.00251407884151247,
      "grad_norm": 31.125,
      "learning_rate": 1.9994971842316978e-05,
      "loss": 2.9258,
      "step": 50
    },
    {
      "epoch": 0.0030168946098149637,
      "grad_norm": 23.25,
      "learning_rate": 1.999396621078037e-05,
      "loss": 1.6977,
      "step": 60
    },
    {
      "epoch": 0.0035197103781174576,
      "grad_norm": 121.5,
      "learning_rate": 1.9992960579243766e-05,
      "loss": 1.8602,
      "step": 70
    },
    {
      "epoch": 0.004022526146419952,
      "grad_norm": 62.25,
      "learning_rate": 1.9991954947707162e-05,
      "loss": 1.9538,
      "step": 80
    },
    {
      "epoch": 0.004525341914722446,
      "grad_norm": 86.0,
      "learning_rate": 1.9990949316170554e-05,
      "loss": 1.708,
      "step": 90
    },
    {
      "epoch": 0.00502815768302494,
      "grad_norm": 20.5,
      "learning_rate": 1.9989943684633954e-05,
      "loss": 2.9422,
      "step": 100
    },
    {
      "epoch": 0.0055309734513274336,
      "grad_norm": 28.0,
      "learning_rate": 1.9988938053097346e-05,
      "loss": 2.1455,
      "step": 110
    },
    {
      "epoch": 0.006033789219629927,
      "grad_norm": 31.125,
      "learning_rate": 1.9987932421560742e-05,
      "loss": 2.0912,
      "step": 120
    },
    {
      "epoch": 0.006536604987932421,
      "grad_norm": 129.0,
      "learning_rate": 1.9986926790024138e-05,
      "loss": 2.2301,
      "step": 130
    },
    {
      "epoch": 0.007039420756234915,
      "grad_norm": 30.0,
      "learning_rate": 1.998592115848753e-05,
      "loss": 3.5905,
      "step": 140
    },
    {
      "epoch": 0.007542236524537409,
      "grad_norm": 48.5,
      "learning_rate": 1.9984915526950926e-05,
      "loss": 2.2544,
      "step": 150
    },
    {
      "epoch": 0.008045052292839904,
      "grad_norm": 26.375,
      "learning_rate": 1.9983909895414322e-05,
      "loss": 2.4787,
      "step": 160
    },
    {
      "epoch": 0.008547868061142397,
      "grad_norm": 44.5,
      "learning_rate": 1.9982904263877715e-05,
      "loss": 1.9704,
      "step": 170
    },
    {
      "epoch": 0.009050683829444892,
      "grad_norm": 92.5,
      "learning_rate": 1.9981898632341114e-05,
      "loss": 1.738,
      "step": 180
    },
    {
      "epoch": 0.009553499597747385,
      "grad_norm": 63.0,
      "learning_rate": 1.9980893000804506e-05,
      "loss": 2.3344,
      "step": 190
    },
    {
      "epoch": 0.01005631536604988,
      "grad_norm": 99.5,
      "learning_rate": 1.9979887369267902e-05,
      "loss": 1.3479,
      "step": 200
    },
    {
      "epoch": 0.010559131134352374,
      "grad_norm": 24.375,
      "learning_rate": 1.9978881737731298e-05,
      "loss": 1.9866,
      "step": 210
    },
    {
      "epoch": 0.011061946902654867,
      "grad_norm": 36.5,
      "learning_rate": 1.997787610619469e-05,
      "loss": 1.9526,
      "step": 220
    },
    {
      "epoch": 0.011564762670957362,
      "grad_norm": 68.0,
      "learning_rate": 1.9976870474658087e-05,
      "loss": 2.4033,
      "step": 230
    },
    {
      "epoch": 0.012067578439259855,
      "grad_norm": 96.0,
      "learning_rate": 1.9975864843121482e-05,
      "loss": 2.0509,
      "step": 240
    },
    {
      "epoch": 0.01257039420756235,
      "grad_norm": 1.796875,
      "learning_rate": 1.9974859211584875e-05,
      "loss": 2.1263,
      "step": 250
    },
    {
      "epoch": 0.013073209975864843,
      "grad_norm": 41.25,
      "learning_rate": 1.997385358004827e-05,
      "loss": 1.4645,
      "step": 260
    },
    {
      "epoch": 0.013576025744167337,
      "grad_norm": 4.25,
      "learning_rate": 1.9972847948511667e-05,
      "loss": 1.9332,
      "step": 270
    },
    {
      "epoch": 0.01407884151246983,
      "grad_norm": 53.0,
      "learning_rate": 1.9971842316975063e-05,
      "loss": 1.3064,
      "step": 280
    },
    {
      "epoch": 0.014581657280772325,
      "grad_norm": 25.0,
      "learning_rate": 1.997083668543846e-05,
      "loss": 1.4321,
      "step": 290
    },
    {
      "epoch": 0.015084473049074818,
      "grad_norm": 127.5,
      "learning_rate": 1.996983105390185e-05,
      "loss": 1.9563,
      "step": 300
    },
    {
      "epoch": 0.015587288817377313,
      "grad_norm": 12.625,
      "learning_rate": 1.9968825422365247e-05,
      "loss": 1.7906,
      "step": 310
    },
    {
      "epoch": 0.016090104585679808,
      "grad_norm": 150.0,
      "learning_rate": 1.9967819790828643e-05,
      "loss": 1.7745,
      "step": 320
    },
    {
      "epoch": 0.016592920353982302,
      "grad_norm": 86.5,
      "learning_rate": 1.9966814159292035e-05,
      "loss": 1.9699,
      "step": 330
    },
    {
      "epoch": 0.017095736122284794,
      "grad_norm": 17.5,
      "learning_rate": 1.996580852775543e-05,
      "loss": 1.6041,
      "step": 340
    },
    {
      "epoch": 0.01759855189058729,
      "grad_norm": 126.0,
      "learning_rate": 1.9964802896218827e-05,
      "loss": 1.3953,
      "step": 350
    },
    {
      "epoch": 0.018101367658889783,
      "grad_norm": 94.5,
      "learning_rate": 1.9963797264682223e-05,
      "loss": 1.5418,
      "step": 360
    },
    {
      "epoch": 0.018604183427192278,
      "grad_norm": 162.0,
      "learning_rate": 1.996279163314562e-05,
      "loss": 1.7346,
      "step": 370
    },
    {
      "epoch": 0.01910699919549477,
      "grad_norm": 11.625,
      "learning_rate": 1.996178600160901e-05,
      "loss": 1.8128,
      "step": 380
    },
    {
      "epoch": 0.019609814963797264,
      "grad_norm": 17.0,
      "learning_rate": 1.9960780370072407e-05,
      "loss": 1.4312,
      "step": 390
    },
    {
      "epoch": 0.02011263073209976,
      "grad_norm": 12.375,
      "learning_rate": 1.9959774738535803e-05,
      "loss": 1.3423,
      "step": 400
    },
    {
      "epoch": 0.020615446500402253,
      "grad_norm": 16.875,
      "learning_rate": 1.9958769106999195e-05,
      "loss": 1.0962,
      "step": 410
    },
    {
      "epoch": 0.021118262268704748,
      "grad_norm": 78.5,
      "learning_rate": 1.995776347546259e-05,
      "loss": 1.5727,
      "step": 420
    },
    {
      "epoch": 0.02162107803700724,
      "grad_norm": 21.375,
      "learning_rate": 1.9956757843925987e-05,
      "loss": 1.3668,
      "step": 430
    },
    {
      "epoch": 0.022123893805309734,
      "grad_norm": 25.875,
      "learning_rate": 1.9955752212389383e-05,
      "loss": 1.4074,
      "step": 440
    },
    {
      "epoch": 0.02262670957361223,
      "grad_norm": 16.875,
      "learning_rate": 1.995474658085278e-05,
      "loss": 1.1227,
      "step": 450
    },
    {
      "epoch": 0.023129525341914724,
      "grad_norm": 71.0,
      "learning_rate": 1.995374094931617e-05,
      "loss": 1.2776,
      "step": 460
    },
    {
      "epoch": 0.023632341110217215,
      "grad_norm": 24.875,
      "learning_rate": 1.9952735317779567e-05,
      "loss": 1.55,
      "step": 470
    },
    {
      "epoch": 0.02413515687851971,
      "grad_norm": 89.0,
      "learning_rate": 1.9951729686242963e-05,
      "loss": 1.364,
      "step": 480
    },
    {
      "epoch": 0.024637972646822204,
      "grad_norm": 17.625,
      "learning_rate": 1.9950724054706356e-05,
      "loss": 1.0853,
      "step": 490
    },
    {
      "epoch": 0.0251407884151247,
      "grad_norm": 39.25,
      "learning_rate": 1.994971842316975e-05,
      "loss": 1.1744,
      "step": 500
    },
    {
      "epoch": 0.0251407884151247,
      "eval_accuracy": 0.49266236985622214,
      "eval_loss": 2.2441346645355225,
      "eval_runtime": 463.561,
      "eval_samples_per_second": 87.022,
      "eval_steps_per_second": 87.022,
      "step": 500
    },
    {
      "epoch": 0.025643604183427194,
      "grad_norm": 32.75,
      "learning_rate": 1.9948712791633147e-05,
      "loss": 1.1669,
      "step": 510
    },
    {
      "epoch": 0.026146419951729685,
      "grad_norm": 27.375,
      "learning_rate": 1.9947707160096543e-05,
      "loss": 1.0588,
      "step": 520
    },
    {
      "epoch": 0.02664923572003218,
      "grad_norm": 6.875,
      "learning_rate": 1.9946701528559936e-05,
      "loss": 1.3489,
      "step": 530
    },
    {
      "epoch": 0.027152051488334675,
      "grad_norm": 18.5,
      "learning_rate": 1.994569589702333e-05,
      "loss": 1.0483,
      "step": 540
    },
    {
      "epoch": 0.02765486725663717,
      "grad_norm": 16.625,
      "learning_rate": 1.9944690265486728e-05,
      "loss": 1.063,
      "step": 550
    },
    {
      "epoch": 0.02815768302493966,
      "grad_norm": 138.0,
      "learning_rate": 1.9943684633950123e-05,
      "loss": 1.7311,
      "step": 560
    },
    {
      "epoch": 0.028660498793242156,
      "grad_norm": 26.5,
      "learning_rate": 1.9942679002413516e-05,
      "loss": 1.0805,
      "step": 570
    },
    {
      "epoch": 0.02916331456154465,
      "grad_norm": 50.0,
      "learning_rate": 1.9941673370876912e-05,
      "loss": 0.9571,
      "step": 580
    },
    {
      "epoch": 0.029666130329847145,
      "grad_norm": 41.25,
      "learning_rate": 1.9940667739340308e-05,
      "loss": 1.4236,
      "step": 590
    },
    {
      "epoch": 0.030168946098149636,
      "grad_norm": 65.0,
      "learning_rate": 1.9939662107803704e-05,
      "loss": 0.8414,
      "step": 600
    },
    {
      "epoch": 0.03067176186645213,
      "grad_norm": 7.1875,
      "learning_rate": 1.9938656476267096e-05,
      "loss": 0.7962,
      "step": 610
    },
    {
      "epoch": 0.031174577634754626,
      "grad_norm": 30.25,
      "learning_rate": 1.9937650844730492e-05,
      "loss": 1.4491,
      "step": 620
    },
    {
      "epoch": 0.03167739340305712,
      "grad_norm": 7.84375,
      "learning_rate": 1.9936645213193888e-05,
      "loss": 1.1464,
      "step": 630
    },
    {
      "epoch": 0.032180209171359615,
      "grad_norm": 9.875,
      "learning_rate": 1.9935639581657284e-05,
      "loss": 1.0437,
      "step": 640
    },
    {
      "epoch": 0.03268302493966211,
      "grad_norm": 21.0,
      "learning_rate": 1.9934633950120676e-05,
      "loss": 1.3236,
      "step": 650
    },
    {
      "epoch": 0.033185840707964605,
      "grad_norm": 15.8125,
      "learning_rate": 1.9933628318584072e-05,
      "loss": 1.1448,
      "step": 660
    },
    {
      "epoch": 0.03368865647626709,
      "grad_norm": 63.75,
      "learning_rate": 1.9932622687047468e-05,
      "loss": 1.1797,
      "step": 670
    },
    {
      "epoch": 0.03419147224456959,
      "grad_norm": 34.5,
      "learning_rate": 1.9931617055510864e-05,
      "loss": 1.8753,
      "step": 680
    },
    {
      "epoch": 0.03469428801287208,
      "grad_norm": 30.25,
      "learning_rate": 1.9930611423974256e-05,
      "loss": 1.5983,
      "step": 690
    },
    {
      "epoch": 0.03519710378117458,
      "grad_norm": 24.25,
      "learning_rate": 1.9929605792437652e-05,
      "loss": 1.2898,
      "step": 700
    },
    {
      "epoch": 0.03569991954947707,
      "grad_norm": 72.0,
      "learning_rate": 1.9928600160901048e-05,
      "loss": 1.5699,
      "step": 710
    },
    {
      "epoch": 0.036202735317779566,
      "grad_norm": 63.5,
      "learning_rate": 1.9927594529364444e-05,
      "loss": 1.2395,
      "step": 720
    },
    {
      "epoch": 0.03670555108608206,
      "grad_norm": 4.3125,
      "learning_rate": 1.9926588897827836e-05,
      "loss": 1.0974,
      "step": 730
    },
    {
      "epoch": 0.037208366854384556,
      "grad_norm": 6.46875,
      "learning_rate": 1.9925583266291232e-05,
      "loss": 1.0399,
      "step": 740
    },
    {
      "epoch": 0.03771118262268705,
      "grad_norm": 10.125,
      "learning_rate": 1.9924577634754628e-05,
      "loss": 1.2209,
      "step": 750
    },
    {
      "epoch": 0.03821399839098954,
      "grad_norm": 30.25,
      "learning_rate": 1.9923572003218024e-05,
      "loss": 1.2585,
      "step": 760
    },
    {
      "epoch": 0.03871681415929203,
      "grad_norm": 9.3125,
      "learning_rate": 1.9922566371681417e-05,
      "loss": 1.0356,
      "step": 770
    },
    {
      "epoch": 0.03921962992759453,
      "grad_norm": 87.0,
      "learning_rate": 1.9921560740144812e-05,
      "loss": 1.3323,
      "step": 780
    },
    {
      "epoch": 0.03972244569589702,
      "grad_norm": 5.21875,
      "learning_rate": 1.992055510860821e-05,
      "loss": 0.837,
      "step": 790
    },
    {
      "epoch": 0.04022526146419952,
      "grad_norm": 46.75,
      "learning_rate": 1.99195494770716e-05,
      "loss": 1.1189,
      "step": 800
    },
    {
      "epoch": 0.04072807723250201,
      "grad_norm": 19.625,
      "learning_rate": 1.9918543845534997e-05,
      "loss": 0.8339,
      "step": 810
    },
    {
      "epoch": 0.04123089300080451,
      "grad_norm": 22.875,
      "learning_rate": 1.9917538213998393e-05,
      "loss": 1.0547,
      "step": 820
    },
    {
      "epoch": 0.041733708769107,
      "grad_norm": 70.5,
      "learning_rate": 1.991653258246179e-05,
      "loss": 1.4902,
      "step": 830
    },
    {
      "epoch": 0.042236524537409496,
      "grad_norm": 27.375,
      "learning_rate": 1.9915526950925184e-05,
      "loss": 1.0908,
      "step": 840
    },
    {
      "epoch": 0.042739340305711984,
      "grad_norm": 41.75,
      "learning_rate": 1.9914521319388577e-05,
      "loss": 1.2615,
      "step": 850
    },
    {
      "epoch": 0.04324215607401448,
      "grad_norm": 10.0,
      "learning_rate": 1.9913515687851973e-05,
      "loss": 1.4291,
      "step": 860
    },
    {
      "epoch": 0.043744971842316974,
      "grad_norm": 33.5,
      "learning_rate": 1.991251005631537e-05,
      "loss": 1.3932,
      "step": 870
    },
    {
      "epoch": 0.04424778761061947,
      "grad_norm": 30.875,
      "learning_rate": 1.991150442477876e-05,
      "loss": 1.3297,
      "step": 880
    },
    {
      "epoch": 0.04475060337892196,
      "grad_norm": 27.75,
      "learning_rate": 1.9910498793242157e-05,
      "loss": 1.0875,
      "step": 890
    },
    {
      "epoch": 0.04525341914722446,
      "grad_norm": 43.75,
      "learning_rate": 1.9909493161705553e-05,
      "loss": 1.151,
      "step": 900
    },
    {
      "epoch": 0.04575623491552695,
      "grad_norm": 14.5,
      "learning_rate": 1.990848753016895e-05,
      "loss": 0.8733,
      "step": 910
    },
    {
      "epoch": 0.04625905068382945,
      "grad_norm": 43.5,
      "learning_rate": 1.9907481898632345e-05,
      "loss": 1.2638,
      "step": 920
    },
    {
      "epoch": 0.04676186645213194,
      "grad_norm": 52.75,
      "learning_rate": 1.9906476267095737e-05,
      "loss": 1.1483,
      "step": 930
    },
    {
      "epoch": 0.04726468222043443,
      "grad_norm": 52.75,
      "learning_rate": 1.9905470635559133e-05,
      "loss": 1.3098,
      "step": 940
    },
    {
      "epoch": 0.047767497988736925,
      "grad_norm": 40.5,
      "learning_rate": 1.990446500402253e-05,
      "loss": 1.3221,
      "step": 950
    },
    {
      "epoch": 0.04827031375703942,
      "grad_norm": 13.0,
      "learning_rate": 1.990345937248592e-05,
      "loss": 1.1198,
      "step": 960
    },
    {
      "epoch": 0.048773129525341914,
      "grad_norm": 58.0,
      "learning_rate": 1.9902453740949317e-05,
      "loss": 1.2282,
      "step": 970
    },
    {
      "epoch": 0.04927594529364441,
      "grad_norm": 61.5,
      "learning_rate": 1.9901448109412713e-05,
      "loss": 1.322,
      "step": 980
    },
    {
      "epoch": 0.049778761061946904,
      "grad_norm": 11.75,
      "learning_rate": 1.990044247787611e-05,
      "loss": 0.9185,
      "step": 990
    },
    {
      "epoch": 0.0502815768302494,
      "grad_norm": 13.25,
      "learning_rate": 1.9899436846339505e-05,
      "loss": 1.1724,
      "step": 1000
    },
    {
      "epoch": 0.0502815768302494,
      "eval_accuracy": 0.49848785324739714,
      "eval_loss": 1.5374782085418701,
      "eval_runtime": 463.8368,
      "eval_samples_per_second": 86.97,
      "eval_steps_per_second": 86.97,
      "step": 1000
    },
    {
      "epoch": 0.05078439259855189,
      "grad_norm": 40.0,
      "learning_rate": 1.9898431214802897e-05,
      "loss": 1.4855,
      "step": 1010
    },
    {
      "epoch": 0.05128720836685439,
      "grad_norm": 40.5,
      "learning_rate": 1.9897425583266293e-05,
      "loss": 1.0843,
      "step": 1020
    },
    {
      "epoch": 0.051790024135156876,
      "grad_norm": 8.0,
      "learning_rate": 1.989641995172969e-05,
      "loss": 1.3188,
      "step": 1030
    },
    {
      "epoch": 0.05229283990345937,
      "grad_norm": 64.5,
      "learning_rate": 1.989541432019308e-05,
      "loss": 1.2116,
      "step": 1040
    },
    {
      "epoch": 0.052795655671761865,
      "grad_norm": 8.125,
      "learning_rate": 1.9894408688656477e-05,
      "loss": 0.8206,
      "step": 1050
    },
    {
      "epoch": 0.05329847144006436,
      "grad_norm": 23.125,
      "learning_rate": 1.9893403057119873e-05,
      "loss": 0.896,
      "step": 1060
    },
    {
      "epoch": 0.053801287208366855,
      "grad_norm": 11.0625,
      "learning_rate": 1.9892397425583266e-05,
      "loss": 1.1027,
      "step": 1070
    },
    {
      "epoch": 0.05430410297666935,
      "grad_norm": 57.25,
      "learning_rate": 1.9891391794046665e-05,
      "loss": 1.4035,
      "step": 1080
    },
    {
      "epoch": 0.054806918744971844,
      "grad_norm": 10.3125,
      "learning_rate": 1.9890386162510058e-05,
      "loss": 1.3311,
      "step": 1090
    },
    {
      "epoch": 0.05530973451327434,
      "grad_norm": 34.25,
      "learning_rate": 1.9889380530973453e-05,
      "loss": 1.6882,
      "step": 1100
    },
    {
      "epoch": 0.05581255028157683,
      "grad_norm": 36.0,
      "learning_rate": 1.988837489943685e-05,
      "loss": 0.9455,
      "step": 1110
    },
    {
      "epoch": 0.05631536604987932,
      "grad_norm": 56.5,
      "learning_rate": 1.9887369267900242e-05,
      "loss": 1.5086,
      "step": 1120
    },
    {
      "epoch": 0.056818181818181816,
      "grad_norm": 9.6875,
      "learning_rate": 1.9886363636363638e-05,
      "loss": 1.2461,
      "step": 1130
    },
    {
      "epoch": 0.05732099758648431,
      "grad_norm": 61.5,
      "learning_rate": 1.9885358004827034e-05,
      "loss": 1.1075,
      "step": 1140
    },
    {
      "epoch": 0.057823813354786806,
      "grad_norm": 35.25,
      "learning_rate": 1.9884352373290426e-05,
      "loss": 1.0538,
      "step": 1150
    },
    {
      "epoch": 0.0583266291230893,
      "grad_norm": 7.8125,
      "learning_rate": 1.9883346741753825e-05,
      "loss": 1.0417,
      "step": 1160
    },
    {
      "epoch": 0.058829444891391795,
      "grad_norm": 10.5,
      "learning_rate": 1.9882341110217218e-05,
      "loss": 1.1981,
      "step": 1170
    },
    {
      "epoch": 0.05933226065969429,
      "grad_norm": 35.75,
      "learning_rate": 1.9881335478680614e-05,
      "loss": 1.2672,
      "step": 1180
    },
    {
      "epoch": 0.059835076427996785,
      "grad_norm": 39.5,
      "learning_rate": 1.988032984714401e-05,
      "loss": 1.1306,
      "step": 1190
    },
    {
      "epoch": 0.06033789219629927,
      "grad_norm": 65.5,
      "learning_rate": 1.9879324215607402e-05,
      "loss": 1.3169,
      "step": 1200
    },
    {
      "epoch": 0.06084070796460177,
      "grad_norm": 29.625,
      "learning_rate": 1.9878318584070798e-05,
      "loss": 0.8346,
      "step": 1210
    },
    {
      "epoch": 0.06134352373290426,
      "grad_norm": 14.625,
      "learning_rate": 1.9877312952534194e-05,
      "loss": 0.8522,
      "step": 1220
    },
    {
      "epoch": 0.06184633950120676,
      "grad_norm": 18.5,
      "learning_rate": 1.9876307320997586e-05,
      "loss": 1.443,
      "step": 1230
    },
    {
      "epoch": 0.06234915526950925,
      "grad_norm": 67.0,
      "learning_rate": 1.9875301689460986e-05,
      "loss": 1.1026,
      "step": 1240
    },
    {
      "epoch": 0.06285197103781175,
      "grad_norm": 18.75,
      "learning_rate": 1.9874296057924378e-05,
      "loss": 0.8279,
      "step": 1250
    },
    {
      "epoch": 0.06335478680611424,
      "grad_norm": 22.625,
      "learning_rate": 1.9873290426387774e-05,
      "loss": 1.4913,
      "step": 1260
    },
    {
      "epoch": 0.06385760257441674,
      "grad_norm": 12.4375,
      "learning_rate": 1.987228479485117e-05,
      "loss": 0.9585,
      "step": 1270
    },
    {
      "epoch": 0.06436041834271923,
      "grad_norm": 27.25,
      "learning_rate": 1.9871279163314562e-05,
      "loss": 0.8827,
      "step": 1280
    },
    {
      "epoch": 0.06486323411102173,
      "grad_norm": 29.125,
      "learning_rate": 1.9870273531777958e-05,
      "loss": 1.0718,
      "step": 1290
    },
    {
      "epoch": 0.06536604987932422,
      "grad_norm": 11.4375,
      "learning_rate": 1.9869267900241354e-05,
      "loss": 0.8838,
      "step": 1300
    },
    {
      "epoch": 0.06586886564762671,
      "grad_norm": 26.0,
      "learning_rate": 1.9868262268704747e-05,
      "loss": 1.2334,
      "step": 1310
    },
    {
      "epoch": 0.06637168141592921,
      "grad_norm": 33.0,
      "learning_rate": 1.9867256637168142e-05,
      "loss": 1.2418,
      "step": 1320
    },
    {
      "epoch": 0.0668744971842317,
      "grad_norm": 23.125,
      "learning_rate": 1.986625100563154e-05,
      "loss": 1.353,
      "step": 1330
    },
    {
      "epoch": 0.06737731295253419,
      "grad_norm": 32.75,
      "learning_rate": 1.986524537409493e-05,
      "loss": 1.3633,
      "step": 1340
    },
    {
      "epoch": 0.06788012872083668,
      "grad_norm": 28.25,
      "learning_rate": 1.986423974255833e-05,
      "loss": 1.2437,
      "step": 1350
    },
    {
      "epoch": 0.06838294448913917,
      "grad_norm": 12.4375,
      "learning_rate": 1.9863234111021723e-05,
      "loss": 0.8242,
      "step": 1360
    },
    {
      "epoch": 0.06888576025744167,
      "grad_norm": 29.625,
      "learning_rate": 1.986222847948512e-05,
      "loss": 1.03,
      "step": 1370
    },
    {
      "epoch": 0.06938857602574416,
      "grad_norm": 20.75,
      "learning_rate": 1.9861222847948514e-05,
      "loss": 1.0067,
      "step": 1380
    },
    {
      "epoch": 0.06989139179404666,
      "grad_norm": 22.625,
      "learning_rate": 1.9860217216411907e-05,
      "loss": 1.2769,
      "step": 1390
    },
    {
      "epoch": 0.07039420756234915,
      "grad_norm": 27.875,
      "learning_rate": 1.9859211584875303e-05,
      "loss": 1.0452,
      "step": 1400
    },
    {
      "epoch": 0.07089702333065165,
      "grad_norm": 32.75,
      "learning_rate": 1.98582059533387e-05,
      "loss": 1.1272,
      "step": 1410
    },
    {
      "epoch": 0.07139983909895414,
      "grad_norm": 40.0,
      "learning_rate": 1.985720032180209e-05,
      "loss": 1.0697,
      "step": 1420
    },
    {
      "epoch": 0.07190265486725664,
      "grad_norm": 21.125,
      "learning_rate": 1.985619469026549e-05,
      "loss": 0.9728,
      "step": 1430
    },
    {
      "epoch": 0.07240547063555913,
      "grad_norm": 38.25,
      "learning_rate": 1.9855189058728883e-05,
      "loss": 1.0221,
      "step": 1440
    },
    {
      "epoch": 0.07290828640386163,
      "grad_norm": 13.5,
      "learning_rate": 1.985418342719228e-05,
      "loss": 0.9772,
      "step": 1450
    },
    {
      "epoch": 0.07341110217216412,
      "grad_norm": 66.0,
      "learning_rate": 1.9853177795655675e-05,
      "loss": 0.8205,
      "step": 1460
    },
    {
      "epoch": 0.07391391794046662,
      "grad_norm": 25.875,
      "learning_rate": 1.9852172164119067e-05,
      "loss": 0.9795,
      "step": 1470
    },
    {
      "epoch": 0.07441673370876911,
      "grad_norm": 15.4375,
      "learning_rate": 1.9851166532582463e-05,
      "loss": 1.1292,
      "step": 1480
    },
    {
      "epoch": 0.0749195494770716,
      "grad_norm": 11.8125,
      "learning_rate": 1.985016090104586e-05,
      "loss": 1.1356,
      "step": 1490
    },
    {
      "epoch": 0.0754223652453741,
      "grad_norm": 23.5,
      "learning_rate": 1.984915526950925e-05,
      "loss": 1.1094,
      "step": 1500
    },
    {
      "epoch": 0.0754223652453741,
      "eval_accuracy": 0.5020575111551809,
      "eval_loss": 1.2778980731964111,
      "eval_runtime": 464.4728,
      "eval_samples_per_second": 86.851,
      "eval_steps_per_second": 86.851,
      "step": 1500
    },
    {
      "epoch": 0.07592518101367658,
      "grad_norm": 20.625,
      "learning_rate": 1.984814963797265e-05,
      "loss": 1.2984,
      "step": 1510
    },
    {
      "epoch": 0.07642799678197908,
      "grad_norm": 16.5,
      "learning_rate": 1.9847144006436043e-05,
      "loss": 0.9923,
      "step": 1520
    },
    {
      "epoch": 0.07693081255028157,
      "grad_norm": 54.25,
      "learning_rate": 1.984613837489944e-05,
      "loss": 1.1829,
      "step": 1530
    },
    {
      "epoch": 0.07743362831858407,
      "grad_norm": 12.5625,
      "learning_rate": 1.9845132743362835e-05,
      "loss": 0.9829,
      "step": 1540
    },
    {
      "epoch": 0.07793644408688656,
      "grad_norm": 15.125,
      "learning_rate": 1.9844127111826227e-05,
      "loss": 0.9298,
      "step": 1550
    },
    {
      "epoch": 0.07843925985518906,
      "grad_norm": 14.3125,
      "learning_rate": 1.9843121480289623e-05,
      "loss": 0.948,
      "step": 1560
    },
    {
      "epoch": 0.07894207562349155,
      "grad_norm": 23.5,
      "learning_rate": 1.984211584875302e-05,
      "loss": 1.2381,
      "step": 1570
    },
    {
      "epoch": 0.07944489139179405,
      "grad_norm": 21.25,
      "learning_rate": 1.984111021721641e-05,
      "loss": 1.0238,
      "step": 1580
    },
    {
      "epoch": 0.07994770716009654,
      "grad_norm": 18.875,
      "learning_rate": 1.9840104585679807e-05,
      "loss": 0.7515,
      "step": 1590
    },
    {
      "epoch": 0.08045052292839903,
      "grad_norm": 45.5,
      "learning_rate": 1.9839098954143203e-05,
      "loss": 0.9784,
      "step": 1600
    },
    {
      "epoch": 0.08095333869670153,
      "grad_norm": 15.6875,
      "learning_rate": 1.9838093322606596e-05,
      "loss": 0.9643,
      "step": 1610
    },
    {
      "epoch": 0.08145615446500402,
      "grad_norm": 14.375,
      "learning_rate": 1.9837087691069995e-05,
      "loss": 0.6581,
      "step": 1620
    },
    {
      "epoch": 0.08195897023330652,
      "grad_norm": 8.6875,
      "learning_rate": 1.9836082059533388e-05,
      "loss": 1.0244,
      "step": 1630
    },
    {
      "epoch": 0.08246178600160901,
      "grad_norm": 20.0,
      "learning_rate": 1.9835076427996783e-05,
      "loss": 1.1502,
      "step": 1640
    },
    {
      "epoch": 0.08296460176991151,
      "grad_norm": 45.0,
      "learning_rate": 1.983407079646018e-05,
      "loss": 1.0626,
      "step": 1650
    },
    {
      "epoch": 0.083467417538214,
      "grad_norm": 45.75,
      "learning_rate": 1.9833065164923572e-05,
      "loss": 1.0124,
      "step": 1660
    },
    {
      "epoch": 0.0839702333065165,
      "grad_norm": 27.125,
      "learning_rate": 1.9832059533386968e-05,
      "loss": 1.2806,
      "step": 1670
    },
    {
      "epoch": 0.08447304907481899,
      "grad_norm": 58.5,
      "learning_rate": 1.9831053901850364e-05,
      "loss": 1.5004,
      "step": 1680
    },
    {
      "epoch": 0.08497586484312147,
      "grad_norm": 33.75,
      "learning_rate": 1.9830048270313756e-05,
      "loss": 1.586,
      "step": 1690
    },
    {
      "epoch": 0.08547868061142397,
      "grad_norm": 14.375,
      "learning_rate": 1.9829042638777155e-05,
      "loss": 0.9594,
      "step": 1700
    },
    {
      "epoch": 0.08598149637972646,
      "grad_norm": 28.875,
      "learning_rate": 1.9828037007240548e-05,
      "loss": 1.0504,
      "step": 1710
    },
    {
      "epoch": 0.08648431214802896,
      "grad_norm": 32.5,
      "learning_rate": 1.9827031375703944e-05,
      "loss": 0.8961,
      "step": 1720
    },
    {
      "epoch": 0.08698712791633145,
      "grad_norm": 30.75,
      "learning_rate": 1.982602574416734e-05,
      "loss": 1.0909,
      "step": 1730
    },
    {
      "epoch": 0.08748994368463395,
      "grad_norm": 18.125,
      "learning_rate": 1.9825020112630732e-05,
      "loss": 1.0049,
      "step": 1740
    },
    {
      "epoch": 0.08799275945293644,
      "grad_norm": 8.0,
      "learning_rate": 1.9824014481094128e-05,
      "loss": 0.9059,
      "step": 1750
    },
    {
      "epoch": 0.08849557522123894,
      "grad_norm": 44.5,
      "learning_rate": 1.9823008849557524e-05,
      "loss": 0.8509,
      "step": 1760
    },
    {
      "epoch": 0.08899839098954143,
      "grad_norm": 7.8125,
      "learning_rate": 1.9822003218020916e-05,
      "loss": 0.9924,
      "step": 1770
    },
    {
      "epoch": 0.08950120675784393,
      "grad_norm": 15.9375,
      "learning_rate": 1.9820997586484316e-05,
      "loss": 1.0462,
      "step": 1780
    },
    {
      "epoch": 0.09000402252614642,
      "grad_norm": 8.375,
      "learning_rate": 1.9819991954947708e-05,
      "loss": 0.863,
      "step": 1790
    },
    {
      "epoch": 0.09050683829444892,
      "grad_norm": 17.875,
      "learning_rate": 1.9818986323411104e-05,
      "loss": 1.0664,
      "step": 1800
    },
    {
      "epoch": 0.09100965406275141,
      "grad_norm": 21.625,
      "learning_rate": 1.98179806918745e-05,
      "loss": 1.023,
      "step": 1810
    },
    {
      "epoch": 0.0915124698310539,
      "grad_norm": 8.4375,
      "learning_rate": 1.9816975060337892e-05,
      "loss": 1.2834,
      "step": 1820
    },
    {
      "epoch": 0.0920152855993564,
      "grad_norm": 69.5,
      "learning_rate": 1.9815969428801288e-05,
      "loss": 1.231,
      "step": 1830
    },
    {
      "epoch": 0.0925181013676589,
      "grad_norm": 22.375,
      "learning_rate": 1.9814963797264684e-05,
      "loss": 1.0259,
      "step": 1840
    },
    {
      "epoch": 0.09302091713596139,
      "grad_norm": 7.0625,
      "learning_rate": 1.981395816572808e-05,
      "loss": 1.2927,
      "step": 1850
    },
    {
      "epoch": 0.09352373290426388,
      "grad_norm": 53.0,
      "learning_rate": 1.9812952534191472e-05,
      "loss": 0.8671,
      "step": 1860
    },
    {
      "epoch": 0.09402654867256637,
      "grad_norm": 13.8125,
      "learning_rate": 1.981194690265487e-05,
      "loss": 1.1904,
      "step": 1870
    },
    {
      "epoch": 0.09452936444086886,
      "grad_norm": 24.375,
      "learning_rate": 1.9810941271118264e-05,
      "loss": 1.4408,
      "step": 1880
    },
    {
      "epoch": 0.09503218020917135,
      "grad_norm": 9.25,
      "learning_rate": 1.980993563958166e-05,
      "loss": 1.2783,
      "step": 1890
    },
    {
      "epoch": 0.09553499597747385,
      "grad_norm": 47.5,
      "learning_rate": 1.9808930008045053e-05,
      "loss": 1.1093,
      "step": 1900
    },
    {
      "epoch": 0.09603781174577634,
      "grad_norm": 58.5,
      "learning_rate": 1.980792437650845e-05,
      "loss": 1.2751,
      "step": 1910
    },
    {
      "epoch": 0.09654062751407884,
      "grad_norm": 3.03125,
      "learning_rate": 1.9806918744971844e-05,
      "loss": 0.8366,
      "step": 1920
    },
    {
      "epoch": 0.09704344328238133,
      "grad_norm": 27.125,
      "learning_rate": 1.980591311343524e-05,
      "loss": 1.0107,
      "step": 1930
    },
    {
      "epoch": 0.09754625905068383,
      "grad_norm": 20.0,
      "learning_rate": 1.9804907481898633e-05,
      "loss": 0.9459,
      "step": 1940
    },
    {
      "epoch": 0.09804907481898632,
      "grad_norm": 13.0,
      "learning_rate": 1.980390185036203e-05,
      "loss": 0.9708,
      "step": 1950
    },
    {
      "epoch": 0.09855189058728882,
      "grad_norm": 8.5,
      "learning_rate": 1.9802896218825425e-05,
      "loss": 0.7031,
      "step": 1960
    },
    {
      "epoch": 0.09905470635559131,
      "grad_norm": 6.75,
      "learning_rate": 1.980189058728882e-05,
      "loss": 1.0424,
      "step": 1970
    },
    {
      "epoch": 0.09955752212389381,
      "grad_norm": 20.875,
      "learning_rate": 1.9800884955752213e-05,
      "loss": 1.4547,
      "step": 1980
    },
    {
      "epoch": 0.1000603378921963,
      "grad_norm": 19.125,
      "learning_rate": 1.979987932421561e-05,
      "loss": 1.0723,
      "step": 1990
    },
    {
      "epoch": 0.1005631536604988,
      "grad_norm": 25.625,
      "learning_rate": 1.9798873692679005e-05,
      "loss": 0.7404,
      "step": 2000
    },
    {
      "epoch": 0.1005631536604988,
      "eval_accuracy": 0.5057759048091225,
      "eval_loss": 1.196290373802185,
      "eval_runtime": 463.4626,
      "eval_samples_per_second": 87.04,
      "eval_steps_per_second": 87.04,
      "step": 2000
    },
    {
      "epoch": 0.10106596942880129,
      "grad_norm": 6.09375,
      "learning_rate": 1.97978680611424e-05,
      "loss": 0.7338,
      "step": 2010
    },
    {
      "epoch": 0.10156878519710379,
      "grad_norm": 11.4375,
      "learning_rate": 1.9796862429605793e-05,
      "loss": 1.273,
      "step": 2020
    },
    {
      "epoch": 0.10207160096540628,
      "grad_norm": 17.75,
      "learning_rate": 1.979585679806919e-05,
      "loss": 0.9983,
      "step": 2030
    },
    {
      "epoch": 0.10257441673370878,
      "grad_norm": 20.375,
      "learning_rate": 1.9794851166532585e-05,
      "loss": 1.306,
      "step": 2040
    },
    {
      "epoch": 0.10307723250201126,
      "grad_norm": 5.21875,
      "learning_rate": 1.979384553499598e-05,
      "loss": 0.8712,
      "step": 2050
    },
    {
      "epoch": 0.10358004827031375,
      "grad_norm": 14.125,
      "learning_rate": 1.9792839903459373e-05,
      "loss": 0.9737,
      "step": 2060
    },
    {
      "epoch": 0.10408286403861625,
      "grad_norm": 10.6875,
      "learning_rate": 1.979183427192277e-05,
      "loss": 0.835,
      "step": 2070
    },
    {
      "epoch": 0.10458567980691874,
      "grad_norm": 15.5,
      "learning_rate": 1.9790828640386165e-05,
      "loss": 1.0255,
      "step": 2080
    },
    {
      "epoch": 0.10508849557522124,
      "grad_norm": 12.125,
      "learning_rate": 1.978982300884956e-05,
      "loss": 0.9322,
      "step": 2090
    },
    {
      "epoch": 0.10559131134352373,
      "grad_norm": 66.0,
      "learning_rate": 1.9788817377312953e-05,
      "loss": 1.3371,
      "step": 2100
    },
    {
      "epoch": 0.10609412711182623,
      "grad_norm": 15.3125,
      "learning_rate": 1.978781174577635e-05,
      "loss": 1.0585,
      "step": 2110
    },
    {
      "epoch": 0.10659694288012872,
      "grad_norm": 35.0,
      "learning_rate": 1.9786806114239745e-05,
      "loss": 1.0711,
      "step": 2120
    },
    {
      "epoch": 0.10709975864843121,
      "grad_norm": 24.75,
      "learning_rate": 1.9785800482703138e-05,
      "loss": 1.3138,
      "step": 2130
    },
    {
      "epoch": 0.10760257441673371,
      "grad_norm": 41.75,
      "learning_rate": 1.9784794851166533e-05,
      "loss": 1.34,
      "step": 2140
    },
    {
      "epoch": 0.1081053901850362,
      "grad_norm": 7.34375,
      "learning_rate": 1.978378921962993e-05,
      "loss": 0.9177,
      "step": 2150
    },
    {
      "epoch": 0.1086082059533387,
      "grad_norm": 37.75,
      "learning_rate": 1.9782783588093325e-05,
      "loss": 1.1133,
      "step": 2160
    },
    {
      "epoch": 0.1091110217216412,
      "grad_norm": 63.75,
      "learning_rate": 1.978177795655672e-05,
      "loss": 1.0031,
      "step": 2170
    },
    {
      "epoch": 0.10961383748994369,
      "grad_norm": 5.875,
      "learning_rate": 1.9780772325020114e-05,
      "loss": 0.898,
      "step": 2180
    },
    {
      "epoch": 0.11011665325824618,
      "grad_norm": 12.5,
      "learning_rate": 1.977976669348351e-05,
      "loss": 1.0838,
      "step": 2190
    },
    {
      "epoch": 0.11061946902654868,
      "grad_norm": 20.25,
      "learning_rate": 1.9778761061946905e-05,
      "loss": 1.614,
      "step": 2200
    },
    {
      "epoch": 0.11112228479485117,
      "grad_norm": 29.375,
      "learning_rate": 1.9777755430410298e-05,
      "loss": 1.0045,
      "step": 2210
    },
    {
      "epoch": 0.11162510056315365,
      "grad_norm": 29.625,
      "learning_rate": 1.9776749798873694e-05,
      "loss": 1.08,
      "step": 2220
    },
    {
      "epoch": 0.11212791633145615,
      "grad_norm": 8.9375,
      "learning_rate": 1.977574416733709e-05,
      "loss": 0.9126,
      "step": 2230
    },
    {
      "epoch": 0.11263073209975864,
      "grad_norm": 20.5,
      "learning_rate": 1.9774738535800485e-05,
      "loss": 0.868,
      "step": 2240
    },
    {
      "epoch": 0.11313354786806114,
      "grad_norm": 25.375,
      "learning_rate": 1.977373290426388e-05,
      "loss": 0.7441,
      "step": 2250
    },
    {
      "epoch": 0.11363636363636363,
      "grad_norm": 30.25,
      "learning_rate": 1.9772727272727274e-05,
      "loss": 1.2848,
      "step": 2260
    },
    {
      "epoch": 0.11413917940466613,
      "grad_norm": 5.625,
      "learning_rate": 1.977172164119067e-05,
      "loss": 1.0578,
      "step": 2270
    },
    {
      "epoch": 0.11464199517296862,
      "grad_norm": 22.625,
      "learning_rate": 1.9770716009654066e-05,
      "loss": 1.399,
      "step": 2280
    },
    {
      "epoch": 0.11514481094127112,
      "grad_norm": 7.96875,
      "learning_rate": 1.9769710378117458e-05,
      "loss": 0.9063,
      "step": 2290
    },
    {
      "epoch": 0.11564762670957361,
      "grad_norm": 6.5625,
      "learning_rate": 1.9768704746580854e-05,
      "loss": 1.0792,
      "step": 2300
    },
    {
      "epoch": 0.1161504424778761,
      "grad_norm": 9.9375,
      "learning_rate": 1.976769911504425e-05,
      "loss": 0.8213,
      "step": 2310
    },
    {
      "epoch": 0.1166532582461786,
      "grad_norm": 17.0,
      "learning_rate": 1.9766693483507646e-05,
      "loss": 1.2637,
      "step": 2320
    },
    {
      "epoch": 0.1171560740144811,
      "grad_norm": 23.875,
      "learning_rate": 1.976568785197104e-05,
      "loss": 0.8917,
      "step": 2330
    },
    {
      "epoch": 0.11765888978278359,
      "grad_norm": 18.0,
      "learning_rate": 1.9764682220434434e-05,
      "loss": 0.849,
      "step": 2340
    },
    {
      "epoch": 0.11816170555108609,
      "grad_norm": 9.5625,
      "learning_rate": 1.976367658889783e-05,
      "loss": 0.9853,
      "step": 2350
    },
    {
      "epoch": 0.11866452131938858,
      "grad_norm": 12.5625,
      "learning_rate": 1.9762670957361226e-05,
      "loss": 1.1028,
      "step": 2360
    },
    {
      "epoch": 0.11916733708769107,
      "grad_norm": 7.375,
      "learning_rate": 1.9761665325824618e-05,
      "loss": 1.2908,
      "step": 2370
    },
    {
      "epoch": 0.11967015285599357,
      "grad_norm": 70.0,
      "learning_rate": 1.9760659694288014e-05,
      "loss": 1.004,
      "step": 2380
    },
    {
      "epoch": 0.12017296862429606,
      "grad_norm": 6.25,
      "learning_rate": 1.975965406275141e-05,
      "loss": 1.0106,
      "step": 2390
    },
    {
      "epoch": 0.12067578439259855,
      "grad_norm": 30.125,
      "learning_rate": 1.9758648431214803e-05,
      "loss": 1.4111,
      "step": 2400
    },
    {
      "epoch": 0.12117860016090104,
      "grad_norm": 5.625,
      "learning_rate": 1.9757642799678202e-05,
      "loss": 1.2037,
      "step": 2410
    },
    {
      "epoch": 0.12168141592920353,
      "grad_norm": 37.0,
      "learning_rate": 1.9756637168141594e-05,
      "loss": 1.2363,
      "step": 2420
    },
    {
      "epoch": 0.12218423169750603,
      "grad_norm": 11.75,
      "learning_rate": 1.975563153660499e-05,
      "loss": 1.027,
      "step": 2430
    },
    {
      "epoch": 0.12268704746580852,
      "grad_norm": 24.5,
      "learning_rate": 1.9754625905068386e-05,
      "loss": 1.0427,
      "step": 2440
    },
    {
      "epoch": 0.12318986323411102,
      "grad_norm": 21.125,
      "learning_rate": 1.975362027353178e-05,
      "loss": 0.6839,
      "step": 2450
    },
    {
      "epoch": 0.12369267900241351,
      "grad_norm": 20.75,
      "learning_rate": 1.9752614641995174e-05,
      "loss": 1.0212,
      "step": 2460
    },
    {
      "epoch": 0.12419549477071601,
      "grad_norm": 4.625,
      "learning_rate": 1.975160901045857e-05,
      "loss": 0.7714,
      "step": 2470
    },
    {
      "epoch": 0.1246983105390185,
      "grad_norm": 24.125,
      "learning_rate": 1.9750603378921963e-05,
      "loss": 1.0904,
      "step": 2480
    },
    {
      "epoch": 0.125201126307321,
      "grad_norm": 29.0,
      "learning_rate": 1.9749597747385362e-05,
      "loss": 0.9094,
      "step": 2490
    },
    {
      "epoch": 0.1257039420756235,
      "grad_norm": 10.75,
      "learning_rate": 1.9748592115848755e-05,
      "loss": 0.9452,
      "step": 2500
    },
    {
      "epoch": 0.1257039420756235,
      "eval_accuracy": 0.5054784333168071,
      "eval_loss": 1.1597926616668701,
      "eval_runtime": 462.8178,
      "eval_samples_per_second": 87.162,
      "eval_steps_per_second": 87.162,
      "step": 2500
    },
    {
      "epoch": 0.126206757843926,
      "grad_norm": 51.5,
      "learning_rate": 1.974758648431215e-05,
      "loss": 1.0503,
      "step": 2510
    },
    {
      "epoch": 0.12670957361222848,
      "grad_norm": 38.75,
      "learning_rate": 1.9746580852775546e-05,
      "loss": 1.1281,
      "step": 2520
    },
    {
      "epoch": 0.12721238938053098,
      "grad_norm": 40.5,
      "learning_rate": 1.974557522123894e-05,
      "loss": 1.3772,
      "step": 2530
    },
    {
      "epoch": 0.12771520514883347,
      "grad_norm": 28.125,
      "learning_rate": 1.9744569589702335e-05,
      "loss": 1.1235,
      "step": 2540
    },
    {
      "epoch": 0.12821802091713597,
      "grad_norm": 14.4375,
      "learning_rate": 1.974356395816573e-05,
      "loss": 1.1356,
      "step": 2550
    },
    {
      "epoch": 0.12872083668543846,
      "grad_norm": 6.875,
      "learning_rate": 1.9742558326629123e-05,
      "loss": 0.6989,
      "step": 2560
    },
    {
      "epoch": 0.12922365245374096,
      "grad_norm": 24.375,
      "learning_rate": 1.9741552695092522e-05,
      "loss": 0.9068,
      "step": 2570
    },
    {
      "epoch": 0.12972646822204345,
      "grad_norm": 33.0,
      "learning_rate": 1.9740547063555915e-05,
      "loss": 0.6883,
      "step": 2580
    },
    {
      "epoch": 0.13022928399034595,
      "grad_norm": 9.5,
      "learning_rate": 1.973954143201931e-05,
      "loss": 0.8915,
      "step": 2590
    },
    {
      "epoch": 0.13073209975864844,
      "grad_norm": 10.6875,
      "learning_rate": 1.9738535800482707e-05,
      "loss": 1.1406,
      "step": 2600
    },
    {
      "epoch": 0.13123491552695093,
      "grad_norm": 9.5625,
      "learning_rate": 1.97375301689461e-05,
      "loss": 1.1171,
      "step": 2610
    },
    {
      "epoch": 0.13173773129525343,
      "grad_norm": 8.6875,
      "learning_rate": 1.9736524537409495e-05,
      "loss": 0.8735,
      "step": 2620
    },
    {
      "epoch": 0.13224054706355592,
      "grad_norm": 20.5,
      "learning_rate": 1.973551890587289e-05,
      "loss": 1.0478,
      "step": 2630
    },
    {
      "epoch": 0.13274336283185842,
      "grad_norm": 16.625,
      "learning_rate": 1.9734513274336283e-05,
      "loss": 0.9281,
      "step": 2640
    },
    {
      "epoch": 0.1332461786001609,
      "grad_norm": 25.75,
      "learning_rate": 1.973350764279968e-05,
      "loss": 1.0036,
      "step": 2650
    },
    {
      "epoch": 0.1337489943684634,
      "grad_norm": 42.25,
      "learning_rate": 1.9732502011263075e-05,
      "loss": 1.0214,
      "step": 2660
    },
    {
      "epoch": 0.13425181013676588,
      "grad_norm": 55.75,
      "learning_rate": 1.9731496379726468e-05,
      "loss": 1.173,
      "step": 2670
    },
    {
      "epoch": 0.13475462590506837,
      "grad_norm": 36.75,
      "learning_rate": 1.9730490748189867e-05,
      "loss": 1.1137,
      "step": 2680
    },
    {
      "epoch": 0.13525744167337087,
      "grad_norm": 19.375,
      "learning_rate": 1.972948511665326e-05,
      "loss": 0.8623,
      "step": 2690
    },
    {
      "epoch": 0.13576025744167336,
      "grad_norm": 11.4375,
      "learning_rate": 1.9728479485116655e-05,
      "loss": 0.8267,
      "step": 2700
    },
    {
      "epoch": 0.13626307320997585,
      "grad_norm": 8.25,
      "learning_rate": 1.972747385358005e-05,
      "loss": 1.0373,
      "step": 2710
    },
    {
      "epoch": 0.13676588897827835,
      "grad_norm": 47.25,
      "learning_rate": 1.9726468222043444e-05,
      "loss": 1.0575,
      "step": 2720
    },
    {
      "epoch": 0.13726870474658084,
      "grad_norm": 24.75,
      "learning_rate": 1.972546259050684e-05,
      "loss": 1.1627,
      "step": 2730
    },
    {
      "epoch": 0.13777152051488334,
      "grad_norm": 18.875,
      "learning_rate": 1.9724456958970235e-05,
      "loss": 1.1038,
      "step": 2740
    },
    {
      "epoch": 0.13827433628318583,
      "grad_norm": 58.5,
      "learning_rate": 1.9723451327433628e-05,
      "loss": 1.2842,
      "step": 2750
    },
    {
      "epoch": 0.13877715205148833,
      "grad_norm": 33.0,
      "learning_rate": 1.9722445695897027e-05,
      "loss": 1.2195,
      "step": 2760
    },
    {
      "epoch": 0.13927996781979082,
      "grad_norm": 39.0,
      "learning_rate": 1.972144006436042e-05,
      "loss": 1.0509,
      "step": 2770
    },
    {
      "epoch": 0.13978278358809332,
      "grad_norm": 4.90625,
      "learning_rate": 1.9720434432823815e-05,
      "loss": 0.9176,
      "step": 2780
    },
    {
      "epoch": 0.1402855993563958,
      "grad_norm": 43.75,
      "learning_rate": 1.971942880128721e-05,
      "loss": 1.0877,
      "step": 2790
    },
    {
      "epoch": 0.1407884151246983,
      "grad_norm": 15.75,
      "learning_rate": 1.9718423169750604e-05,
      "loss": 1.0382,
      "step": 2800
    },
    {
      "epoch": 0.1412912308930008,
      "grad_norm": 29.75,
      "learning_rate": 1.9717417538214e-05,
      "loss": 0.9011,
      "step": 2810
    },
    {
      "epoch": 0.1417940466613033,
      "grad_norm": 8.5,
      "learning_rate": 1.9716411906677396e-05,
      "loss": 0.8672,
      "step": 2820
    },
    {
      "epoch": 0.1422968624296058,
      "grad_norm": 17.375,
      "learning_rate": 1.9715406275140788e-05,
      "loss": 0.9634,
      "step": 2830
    },
    {
      "epoch": 0.1427996781979083,
      "grad_norm": 43.0,
      "learning_rate": 1.9714400643604187e-05,
      "loss": 1.1591,
      "step": 2840
    },
    {
      "epoch": 0.14330249396621078,
      "grad_norm": 11.5625,
      "learning_rate": 1.971339501206758e-05,
      "loss": 0.8279,
      "step": 2850
    },
    {
      "epoch": 0.14380530973451328,
      "grad_norm": 16.75,
      "learning_rate": 1.9712389380530976e-05,
      "loss": 1.342,
      "step": 2860
    },
    {
      "epoch": 0.14430812550281577,
      "grad_norm": 43.75,
      "learning_rate": 1.971138374899437e-05,
      "loss": 0.9795,
      "step": 2870
    },
    {
      "epoch": 0.14481094127111827,
      "grad_norm": 19.875,
      "learning_rate": 1.9710378117457764e-05,
      "loss": 1.1341,
      "step": 2880
    },
    {
      "epoch": 0.14531375703942076,
      "grad_norm": 12.3125,
      "learning_rate": 1.970937248592116e-05,
      "loss": 1.4276,
      "step": 2890
    },
    {
      "epoch": 0.14581657280772325,
      "grad_norm": 14.9375,
      "learning_rate": 1.9708366854384556e-05,
      "loss": 1.3269,
      "step": 2900
    },
    {
      "epoch": 0.14631938857602575,
      "grad_norm": 42.0,
      "learning_rate": 1.9707361222847948e-05,
      "loss": 1.1609,
      "step": 2910
    },
    {
      "epoch": 0.14682220434432824,
      "grad_norm": 4.59375,
      "learning_rate": 1.9706355591311344e-05,
      "loss": 0.7524,
      "step": 2920
    },
    {
      "epoch": 0.14732502011263074,
      "grad_norm": 10.6875,
      "learning_rate": 1.970534995977474e-05,
      "loss": 0.9016,
      "step": 2930
    },
    {
      "epoch": 0.14782783588093323,
      "grad_norm": 24.0,
      "learning_rate": 1.9704344328238133e-05,
      "loss": 0.8496,
      "step": 2940
    },
    {
      "epoch": 0.14833065164923573,
      "grad_norm": 5.28125,
      "learning_rate": 1.9703338696701532e-05,
      "loss": 1.0099,
      "step": 2950
    },
    {
      "epoch": 0.14883346741753822,
      "grad_norm": 23.75,
      "learning_rate": 1.9702333065164924e-05,
      "loss": 1.2633,
      "step": 2960
    },
    {
      "epoch": 0.14933628318584072,
      "grad_norm": 6.21875,
      "learning_rate": 1.970132743362832e-05,
      "loss": 1.1463,
      "step": 2970
    },
    {
      "epoch": 0.1498390989541432,
      "grad_norm": 20.375,
      "learning_rate": 1.9700321802091716e-05,
      "loss": 1.2526,
      "step": 2980
    },
    {
      "epoch": 0.1503419147224457,
      "grad_norm": 7.40625,
      "learning_rate": 1.969931617055511e-05,
      "loss": 1.258,
      "step": 2990
    },
    {
      "epoch": 0.1508447304907482,
      "grad_norm": 65.5,
      "learning_rate": 1.9698310539018504e-05,
      "loss": 1.0141,
      "step": 3000
    },
    {
      "epoch": 0.1508447304907482,
      "eval_accuracy": 0.5075359444719881,
      "eval_loss": 1.1231328248977661,
      "eval_runtime": 463.832,
      "eval_samples_per_second": 86.971,
      "eval_steps_per_second": 86.971,
      "step": 3000
    },
    {
      "epoch": 0.1513475462590507,
      "grad_norm": 31.875,
      "learning_rate": 1.96973049074819e-05,
      "loss": 1.0824,
      "step": 3010
    },
    {
      "epoch": 0.15185036202735316,
      "grad_norm": 16.75,
      "learning_rate": 1.9696299275945293e-05,
      "loss": 1.2176,
      "step": 3020
    },
    {
      "epoch": 0.15235317779565566,
      "grad_norm": 21.875,
      "learning_rate": 1.9695293644408692e-05,
      "loss": 0.918,
      "step": 3030
    },
    {
      "epoch": 0.15285599356395815,
      "grad_norm": 18.875,
      "learning_rate": 1.9694288012872085e-05,
      "loss": 0.9132,
      "step": 3040
    },
    {
      "epoch": 0.15335880933226065,
      "grad_norm": 19.375,
      "learning_rate": 1.969328238133548e-05,
      "loss": 1.0541,
      "step": 3050
    },
    {
      "epoch": 0.15386162510056314,
      "grad_norm": 31.0,
      "learning_rate": 1.9692276749798876e-05,
      "loss": 1.023,
      "step": 3060
    },
    {
      "epoch": 0.15436444086886564,
      "grad_norm": 24.625,
      "learning_rate": 1.969127111826227e-05,
      "loss": 1.2168,
      "step": 3070
    },
    {
      "epoch": 0.15486725663716813,
      "grad_norm": 19.5,
      "learning_rate": 1.9690265486725665e-05,
      "loss": 0.968,
      "step": 3080
    },
    {
      "epoch": 0.15537007240547063,
      "grad_norm": 64.5,
      "learning_rate": 1.968925985518906e-05,
      "loss": 1.3459,
      "step": 3090
    },
    {
      "epoch": 0.15587288817377312,
      "grad_norm": 9.0,
      "learning_rate": 1.9688254223652453e-05,
      "loss": 0.8097,
      "step": 3100
    },
    {
      "epoch": 0.15637570394207562,
      "grad_norm": 16.375,
      "learning_rate": 1.9687248592115852e-05,
      "loss": 0.6782,
      "step": 3110
    },
    {
      "epoch": 0.1568785197103781,
      "grad_norm": 9.375,
      "learning_rate": 1.9686242960579245e-05,
      "loss": 1.1597,
      "step": 3120
    },
    {
      "epoch": 0.1573813354786806,
      "grad_norm": 19.875,
      "learning_rate": 1.968523732904264e-05,
      "loss": 1.2039,
      "step": 3130
    },
    {
      "epoch": 0.1578841512469831,
      "grad_norm": 13.875,
      "learning_rate": 1.9684231697506037e-05,
      "loss": 0.9437,
      "step": 3140
    },
    {
      "epoch": 0.1583869670152856,
      "grad_norm": 125.5,
      "learning_rate": 1.968322606596943e-05,
      "loss": 1.0803,
      "step": 3150
    },
    {
      "epoch": 0.1588897827835881,
      "grad_norm": 11.875,
      "learning_rate": 1.9682220434432825e-05,
      "loss": 1.1613,
      "step": 3160
    },
    {
      "epoch": 0.15939259855189059,
      "grad_norm": 17.5,
      "learning_rate": 1.968121480289622e-05,
      "loss": 1.1287,
      "step": 3170
    },
    {
      "epoch": 0.15989541432019308,
      "grad_norm": 27.125,
      "learning_rate": 1.9680209171359613e-05,
      "loss": 1.0549,
      "step": 3180
    },
    {
      "epoch": 0.16039823008849557,
      "grad_norm": 16.0,
      "learning_rate": 1.967920353982301e-05,
      "loss": 1.2829,
      "step": 3190
    },
    {
      "epoch": 0.16090104585679807,
      "grad_norm": 33.5,
      "learning_rate": 1.9678197908286405e-05,
      "loss": 0.6152,
      "step": 3200
    },
    {
      "epoch": 0.16140386162510056,
      "grad_norm": 44.5,
      "learning_rate": 1.96771922767498e-05,
      "loss": 1.0514,
      "step": 3210
    },
    {
      "epoch": 0.16190667739340306,
      "grad_norm": 80.5,
      "learning_rate": 1.9676186645213197e-05,
      "loss": 1.0487,
      "step": 3220
    },
    {
      "epoch": 0.16240949316170555,
      "grad_norm": 48.25,
      "learning_rate": 1.967518101367659e-05,
      "loss": 1.0744,
      "step": 3230
    },
    {
      "epoch": 0.16291230893000805,
      "grad_norm": 24.875,
      "learning_rate": 1.9674175382139985e-05,
      "loss": 1.1655,
      "step": 3240
    },
    {
      "epoch": 0.16341512469831054,
      "grad_norm": 37.0,
      "learning_rate": 1.967316975060338e-05,
      "loss": 1.0573,
      "step": 3250
    },
    {
      "epoch": 0.16391794046661304,
      "grad_norm": 9.0,
      "learning_rate": 1.9672164119066774e-05,
      "loss": 1.0104,
      "step": 3260
    },
    {
      "epoch": 0.16442075623491553,
      "grad_norm": 3.90625,
      "learning_rate": 1.967115848753017e-05,
      "loss": 0.8098,
      "step": 3270
    },
    {
      "epoch": 0.16492357200321803,
      "grad_norm": 15.375,
      "learning_rate": 1.9670152855993565e-05,
      "loss": 0.8626,
      "step": 3280
    },
    {
      "epoch": 0.16542638777152052,
      "grad_norm": 40.5,
      "learning_rate": 1.966914722445696e-05,
      "loss": 0.9948,
      "step": 3290
    },
    {
      "epoch": 0.16592920353982302,
      "grad_norm": 22.375,
      "learning_rate": 1.9668141592920357e-05,
      "loss": 0.9389,
      "step": 3300
    },
    {
      "epoch": 0.1664320193081255,
      "grad_norm": 19.375,
      "learning_rate": 1.966713596138375e-05,
      "loss": 1.3179,
      "step": 3310
    },
    {
      "epoch": 0.166934835076428,
      "grad_norm": 8.9375,
      "learning_rate": 1.9666130329847145e-05,
      "loss": 0.8864,
      "step": 3320
    },
    {
      "epoch": 0.1674376508447305,
      "grad_norm": 17.5,
      "learning_rate": 1.966512469831054e-05,
      "loss": 1.1146,
      "step": 3330
    },
    {
      "epoch": 0.167940466613033,
      "grad_norm": 29.5,
      "learning_rate": 1.9664119066773934e-05,
      "loss": 1.1229,
      "step": 3340
    },
    {
      "epoch": 0.1684432823813355,
      "grad_norm": 54.25,
      "learning_rate": 1.966311343523733e-05,
      "loss": 1.0238,
      "step": 3350
    },
    {
      "epoch": 0.16894609814963799,
      "grad_norm": 33.0,
      "learning_rate": 1.9662107803700726e-05,
      "loss": 1.0284,
      "step": 3360
    },
    {
      "epoch": 0.16944891391794048,
      "grad_norm": 26.375,
      "learning_rate": 1.966110217216412e-05,
      "loss": 1.0578,
      "step": 3370
    },
    {
      "epoch": 0.16995172968624295,
      "grad_norm": 66.5,
      "learning_rate": 1.9660096540627517e-05,
      "loss": 1.3476,
      "step": 3380
    },
    {
      "epoch": 0.17045454545454544,
      "grad_norm": 20.625,
      "learning_rate": 1.965909090909091e-05,
      "loss": 1.038,
      "step": 3390
    },
    {
      "epoch": 0.17095736122284794,
      "grad_norm": 23.625,
      "learning_rate": 1.9658085277554306e-05,
      "loss": 0.8734,
      "step": 3400
    },
    {
      "epoch": 0.17146017699115043,
      "grad_norm": 25.75,
      "learning_rate": 1.96570796460177e-05,
      "loss": 0.9469,
      "step": 3410
    },
    {
      "epoch": 0.17196299275945293,
      "grad_norm": 39.0,
      "learning_rate": 1.9656074014481094e-05,
      "loss": 1.0126,
      "step": 3420
    },
    {
      "epoch": 0.17246580852775542,
      "grad_norm": 11.125,
      "learning_rate": 1.965506838294449e-05,
      "loss": 1.171,
      "step": 3430
    },
    {
      "epoch": 0.17296862429605792,
      "grad_norm": 14.9375,
      "learning_rate": 1.9654062751407886e-05,
      "loss": 0.8855,
      "step": 3440
    },
    {
      "epoch": 0.1734714400643604,
      "grad_norm": 18.375,
      "learning_rate": 1.9653057119871282e-05,
      "loss": 0.826,
      "step": 3450
    },
    {
      "epoch": 0.1739742558326629,
      "grad_norm": 34.25,
      "learning_rate": 1.9652051488334674e-05,
      "loss": 1.0488,
      "step": 3460
    },
    {
      "epoch": 0.1744770716009654,
      "grad_norm": 25.5,
      "learning_rate": 1.965104585679807e-05,
      "loss": 1.3076,
      "step": 3470
    },
    {
      "epoch": 0.1749798873692679,
      "grad_norm": 48.25,
      "learning_rate": 1.9650040225261466e-05,
      "loss": 1.0111,
      "step": 3480
    },
    {
      "epoch": 0.1754827031375704,
      "grad_norm": 12.625,
      "learning_rate": 1.9649034593724862e-05,
      "loss": 0.8922,
      "step": 3490
    },
    {
      "epoch": 0.17598551890587288,
      "grad_norm": 9.375,
      "learning_rate": 1.9648028962188254e-05,
      "loss": 0.8531,
      "step": 3500
    },
    {
      "epoch": 0.17598551890587288,
      "eval_accuracy": 0.5079573624194348,
      "eval_loss": 1.1100460290908813,
      "eval_runtime": 464.6001,
      "eval_samples_per_second": 86.827,
      "eval_steps_per_second": 86.827,
      "step": 3500
    },
    {
      "epoch": 0.17648833467417538,
      "grad_norm": 8.75,
      "learning_rate": 1.964702333065165e-05,
      "loss": 0.9134,
      "step": 3510
    },
    {
      "epoch": 0.17699115044247787,
      "grad_norm": 28.375,
      "learning_rate": 1.9646017699115046e-05,
      "loss": 0.8783,
      "step": 3520
    },
    {
      "epoch": 0.17749396621078037,
      "grad_norm": 50.75,
      "learning_rate": 1.9645012067578442e-05,
      "loss": 1.157,
      "step": 3530
    },
    {
      "epoch": 0.17799678197908286,
      "grad_norm": 7.1875,
      "learning_rate": 1.9644006436041834e-05,
      "loss": 0.9487,
      "step": 3540
    },
    {
      "epoch": 0.17849959774738536,
      "grad_norm": 20.625,
      "learning_rate": 1.964300080450523e-05,
      "loss": 1.0309,
      "step": 3550
    },
    {
      "epoch": 0.17900241351568785,
      "grad_norm": 11.125,
      "learning_rate": 1.9641995172968626e-05,
      "loss": 1.1905,
      "step": 3560
    },
    {
      "epoch": 0.17950522928399035,
      "grad_norm": 48.75,
      "learning_rate": 1.9640989541432022e-05,
      "loss": 1.1218,
      "step": 3570
    },
    {
      "epoch": 0.18000804505229284,
      "grad_norm": 19.0,
      "learning_rate": 1.9639983909895418e-05,
      "loss": 0.6753,
      "step": 3580
    },
    {
      "epoch": 0.18051086082059534,
      "grad_norm": 77.5,
      "learning_rate": 1.963897827835881e-05,
      "loss": 1.2511,
      "step": 3590
    },
    {
      "epoch": 0.18101367658889783,
      "grad_norm": 5.40625,
      "learning_rate": 1.9637972646822206e-05,
      "loss": 0.9225,
      "step": 3600
    },
    {
      "epoch": 0.18151649235720033,
      "grad_norm": 19.625,
      "learning_rate": 1.9636967015285602e-05,
      "loss": 0.8115,
      "step": 3610
    },
    {
      "epoch": 0.18201930812550282,
      "grad_norm": 32.0,
      "learning_rate": 1.9635961383748995e-05,
      "loss": 1.2921,
      "step": 3620
    },
    {
      "epoch": 0.18252212389380532,
      "grad_norm": 8.375,
      "learning_rate": 1.963495575221239e-05,
      "loss": 1.0419,
      "step": 3630
    },
    {
      "epoch": 0.1830249396621078,
      "grad_norm": 20.875,
      "learning_rate": 1.9633950120675786e-05,
      "loss": 0.98,
      "step": 3640
    },
    {
      "epoch": 0.1835277554304103,
      "grad_norm": 9.1875,
      "learning_rate": 1.9632944489139182e-05,
      "loss": 0.9442,
      "step": 3650
    },
    {
      "epoch": 0.1840305711987128,
      "grad_norm": 45.75,
      "learning_rate": 1.9631938857602578e-05,
      "loss": 1.0846,
      "step": 3660
    },
    {
      "epoch": 0.1845333869670153,
      "grad_norm": 22.125,
      "learning_rate": 1.963093322606597e-05,
      "loss": 1.0126,
      "step": 3670
    },
    {
      "epoch": 0.1850362027353178,
      "grad_norm": 43.75,
      "learning_rate": 1.9629927594529367e-05,
      "loss": 1.1099,
      "step": 3680
    },
    {
      "epoch": 0.18553901850362028,
      "grad_norm": 46.75,
      "learning_rate": 1.9628921962992762e-05,
      "loss": 1.1183,
      "step": 3690
    },
    {
      "epoch": 0.18604183427192278,
      "grad_norm": 13.75,
      "learning_rate": 1.9627916331456155e-05,
      "loss": 0.9883,
      "step": 3700
    },
    {
      "epoch": 0.18654465004022527,
      "grad_norm": 25.875,
      "learning_rate": 1.962691069991955e-05,
      "loss": 1.2724,
      "step": 3710
    },
    {
      "epoch": 0.18704746580852777,
      "grad_norm": 24.625,
      "learning_rate": 1.9625905068382947e-05,
      "loss": 1.1897,
      "step": 3720
    },
    {
      "epoch": 0.18755028157683024,
      "grad_norm": 6.1875,
      "learning_rate": 1.962489943684634e-05,
      "loss": 1.0065,
      "step": 3730
    },
    {
      "epoch": 0.18805309734513273,
      "grad_norm": 31.75,
      "learning_rate": 1.962389380530974e-05,
      "loss": 0.7861,
      "step": 3740
    },
    {
      "epoch": 0.18855591311343523,
      "grad_norm": 10.375,
      "learning_rate": 1.962288817377313e-05,
      "loss": 1.023,
      "step": 3750
    },
    {
      "epoch": 0.18905872888173772,
      "grad_norm": 44.75,
      "learning_rate": 1.9621882542236527e-05,
      "loss": 1.2087,
      "step": 3760
    },
    {
      "epoch": 0.18956154465004021,
      "grad_norm": 35.75,
      "learning_rate": 1.9620876910699923e-05,
      "loss": 1.0604,
      "step": 3770
    },
    {
      "epoch": 0.1900643604183427,
      "grad_norm": 12.75,
      "learning_rate": 1.9619871279163315e-05,
      "loss": 1.0656,
      "step": 3780
    },
    {
      "epoch": 0.1905671761866452,
      "grad_norm": 12.375,
      "learning_rate": 1.961886564762671e-05,
      "loss": 0.9752,
      "step": 3790
    },
    {
      "epoch": 0.1910699919549477,
      "grad_norm": 17.0,
      "learning_rate": 1.9617860016090107e-05,
      "loss": 1.2524,
      "step": 3800
    },
    {
      "epoch": 0.1915728077232502,
      "grad_norm": 19.375,
      "learning_rate": 1.96168543845535e-05,
      "loss": 0.7638,
      "step": 3810
    },
    {
      "epoch": 0.1920756234915527,
      "grad_norm": 26.75,
      "learning_rate": 1.96158487530169e-05,
      "loss": 1.0806,
      "step": 3820
    },
    {
      "epoch": 0.19257843925985518,
      "grad_norm": 17.125,
      "learning_rate": 1.961484312148029e-05,
      "loss": 0.9267,
      "step": 3830
    },
    {
      "epoch": 0.19308125502815768,
      "grad_norm": 6.09375,
      "learning_rate": 1.9613837489943687e-05,
      "loss": 1.0405,
      "step": 3840
    },
    {
      "epoch": 0.19358407079646017,
      "grad_norm": 10.5,
      "learning_rate": 1.9612831858407083e-05,
      "loss": 0.968,
      "step": 3850
    },
    {
      "epoch": 0.19408688656476267,
      "grad_norm": 52.25,
      "learning_rate": 1.9611826226870475e-05,
      "loss": 1.0611,
      "step": 3860
    },
    {
      "epoch": 0.19458970233306516,
      "grad_norm": 16.875,
      "learning_rate": 1.961082059533387e-05,
      "loss": 1.0277,
      "step": 3870
    },
    {
      "epoch": 0.19509251810136766,
      "grad_norm": 29.125,
      "learning_rate": 1.9609814963797267e-05,
      "loss": 1.0115,
      "step": 3880
    },
    {
      "epoch": 0.19559533386967015,
      "grad_norm": 17.125,
      "learning_rate": 1.960880933226066e-05,
      "loss": 0.7579,
      "step": 3890
    },
    {
      "epoch": 0.19609814963797265,
      "grad_norm": 12.3125,
      "learning_rate": 1.960780370072406e-05,
      "loss": 0.9264,
      "step": 3900
    },
    {
      "epoch": 0.19660096540627514,
      "grad_norm": 10.25,
      "learning_rate": 1.960679806918745e-05,
      "loss": 0.982,
      "step": 3910
    },
    {
      "epoch": 0.19710378117457764,
      "grad_norm": 17.375,
      "learning_rate": 1.9605792437650847e-05,
      "loss": 1.0946,
      "step": 3920
    },
    {
      "epoch": 0.19760659694288013,
      "grad_norm": 8.3125,
      "learning_rate": 1.9604786806114243e-05,
      "loss": 0.8287,
      "step": 3930
    },
    {
      "epoch": 0.19810941271118263,
      "grad_norm": 60.25,
      "learning_rate": 1.9603781174577636e-05,
      "loss": 0.6994,
      "step": 3940
    },
    {
      "epoch": 0.19861222847948512,
      "grad_norm": 25.375,
      "learning_rate": 1.960277554304103e-05,
      "loss": 1.2245,
      "step": 3950
    },
    {
      "epoch": 0.19911504424778761,
      "grad_norm": 8.5625,
      "learning_rate": 1.9601769911504427e-05,
      "loss": 0.9013,
      "step": 3960
    },
    {
      "epoch": 0.1996178600160901,
      "grad_norm": 27.5,
      "learning_rate": 1.960076427996782e-05,
      "loss": 1.1515,
      "step": 3970
    },
    {
      "epoch": 0.2001206757843926,
      "grad_norm": 10.25,
      "learning_rate": 1.9599758648431216e-05,
      "loss": 1.1107,
      "step": 3980
    },
    {
      "epoch": 0.2006234915526951,
      "grad_norm": 28.75,
      "learning_rate": 1.9598753016894612e-05,
      "loss": 1.2571,
      "step": 3990
    },
    {
      "epoch": 0.2011263073209976,
      "grad_norm": 33.5,
      "learning_rate": 1.9597747385358004e-05,
      "loss": 1.0971,
      "step": 4000
    },
    {
      "epoch": 0.2011263073209976,
      "eval_accuracy": 0.5077590480912246,
      "eval_loss": 1.097894310951233,
      "eval_runtime": 464.6177,
      "eval_samples_per_second": 86.824,
      "eval_steps_per_second": 86.824,
      "step": 4000
    },
    {
      "epoch": 0.2016291230893001,
      "grad_norm": 3.84375,
      "learning_rate": 1.9596741753821404e-05,
      "loss": 1.1303,
      "step": 4010
    },
    {
      "epoch": 0.20213193885760258,
      "grad_norm": 22.375,
      "learning_rate": 1.9595736122284796e-05,
      "loss": 1.0239,
      "step": 4020
    },
    {
      "epoch": 0.20263475462590508,
      "grad_norm": 16.625,
      "learning_rate": 1.9594730490748192e-05,
      "loss": 1.1429,
      "step": 4030
    },
    {
      "epoch": 0.20313757039420757,
      "grad_norm": 36.0,
      "learning_rate": 1.9593724859211588e-05,
      "loss": 0.7378,
      "step": 4040
    },
    {
      "epoch": 0.20364038616251007,
      "grad_norm": 28.375,
      "learning_rate": 1.959271922767498e-05,
      "loss": 0.969,
      "step": 4050
    },
    {
      "epoch": 0.20414320193081256,
      "grad_norm": 14.75,
      "learning_rate": 1.9591713596138376e-05,
      "loss": 0.9995,
      "step": 4060
    },
    {
      "epoch": 0.20464601769911506,
      "grad_norm": 20.125,
      "learning_rate": 1.9590707964601772e-05,
      "loss": 1.1259,
      "step": 4070
    },
    {
      "epoch": 0.20514883346741755,
      "grad_norm": 51.0,
      "learning_rate": 1.9589702333065164e-05,
      "loss": 1.1447,
      "step": 4080
    },
    {
      "epoch": 0.20565164923572002,
      "grad_norm": 23.875,
      "learning_rate": 1.9588696701528564e-05,
      "loss": 0.8061,
      "step": 4090
    },
    {
      "epoch": 0.2061544650040225,
      "grad_norm": 8.6875,
      "learning_rate": 1.9587691069991956e-05,
      "loss": 1.1893,
      "step": 4100
    },
    {
      "epoch": 0.206657280772325,
      "grad_norm": 22.375,
      "learning_rate": 1.9586685438455352e-05,
      "loss": 0.7757,
      "step": 4110
    },
    {
      "epoch": 0.2071600965406275,
      "grad_norm": 22.75,
      "learning_rate": 1.9585679806918748e-05,
      "loss": 1.3131,
      "step": 4120
    },
    {
      "epoch": 0.20766291230893,
      "grad_norm": 9.4375,
      "learning_rate": 1.958467417538214e-05,
      "loss": 0.9484,
      "step": 4130
    },
    {
      "epoch": 0.2081657280772325,
      "grad_norm": 19.875,
      "learning_rate": 1.9583668543845536e-05,
      "loss": 1.1217,
      "step": 4140
    },
    {
      "epoch": 0.208668543845535,
      "grad_norm": 21.25,
      "learning_rate": 1.9582662912308932e-05,
      "loss": 0.8891,
      "step": 4150
    },
    {
      "epoch": 0.20917135961383748,
      "grad_norm": 29.875,
      "learning_rate": 1.9581657280772325e-05,
      "loss": 1.356,
      "step": 4160
    },
    {
      "epoch": 0.20967417538213998,
      "grad_norm": 27.5,
      "learning_rate": 1.9580651649235724e-05,
      "loss": 0.953,
      "step": 4170
    },
    {
      "epoch": 0.21017699115044247,
      "grad_norm": 16.375,
      "learning_rate": 1.9579646017699117e-05,
      "loss": 1.2229,
      "step": 4180
    },
    {
      "epoch": 0.21067980691874497,
      "grad_norm": 10.8125,
      "learning_rate": 1.9578640386162512e-05,
      "loss": 0.8228,
      "step": 4190
    },
    {
      "epoch": 0.21118262268704746,
      "grad_norm": 38.0,
      "learning_rate": 1.9577634754625908e-05,
      "loss": 1.3557,
      "step": 4200
    },
    {
      "epoch": 0.21168543845534996,
      "grad_norm": 39.5,
      "learning_rate": 1.95766291230893e-05,
      "loss": 1.24,
      "step": 4210
    },
    {
      "epoch": 0.21218825422365245,
      "grad_norm": 35.0,
      "learning_rate": 1.9575623491552697e-05,
      "loss": 1.0844,
      "step": 4220
    },
    {
      "epoch": 0.21269106999195495,
      "grad_norm": 28.375,
      "learning_rate": 1.9574617860016093e-05,
      "loss": 1.1495,
      "step": 4230
    },
    {
      "epoch": 0.21319388576025744,
      "grad_norm": 18.75,
      "learning_rate": 1.9573612228479485e-05,
      "loss": 0.9143,
      "step": 4240
    },
    {
      "epoch": 0.21369670152855993,
      "grad_norm": 25.375,
      "learning_rate": 1.957260659694288e-05,
      "loss": 1.1959,
      "step": 4250
    },
    {
      "epoch": 0.21419951729686243,
      "grad_norm": 17.875,
      "learning_rate": 1.9571600965406277e-05,
      "loss": 1.0442,
      "step": 4260
    },
    {
      "epoch": 0.21470233306516492,
      "grad_norm": 13.25,
      "learning_rate": 1.957059533386967e-05,
      "loss": 1.0244,
      "step": 4270
    },
    {
      "epoch": 0.21520514883346742,
      "grad_norm": 21.0,
      "learning_rate": 1.956958970233307e-05,
      "loss": 1.0442,
      "step": 4280
    },
    {
      "epoch": 0.2157079646017699,
      "grad_norm": 15.375,
      "learning_rate": 1.956858407079646e-05,
      "loss": 0.861,
      "step": 4290
    },
    {
      "epoch": 0.2162107803700724,
      "grad_norm": 65.5,
      "learning_rate": 1.9567578439259857e-05,
      "loss": 1.4535,
      "step": 4300
    },
    {
      "epoch": 0.2167135961383749,
      "grad_norm": 23.25,
      "learning_rate": 1.9566572807723253e-05,
      "loss": 1.0114,
      "step": 4310
    },
    {
      "epoch": 0.2172164119066774,
      "grad_norm": 14.1875,
      "learning_rate": 1.9565567176186645e-05,
      "loss": 1.2991,
      "step": 4320
    },
    {
      "epoch": 0.2177192276749799,
      "grad_norm": 48.75,
      "learning_rate": 1.956456154465004e-05,
      "loss": 1.323,
      "step": 4330
    },
    {
      "epoch": 0.2182220434432824,
      "grad_norm": 25.75,
      "learning_rate": 1.9563555913113437e-05,
      "loss": 1.0964,
      "step": 4340
    },
    {
      "epoch": 0.21872485921158488,
      "grad_norm": 12.3125,
      "learning_rate": 1.956255028157683e-05,
      "loss": 1.1695,
      "step": 4350
    },
    {
      "epoch": 0.21922767497988738,
      "grad_norm": 28.625,
      "learning_rate": 1.956154465004023e-05,
      "loss": 1.1963,
      "step": 4360
    },
    {
      "epoch": 0.21973049074818987,
      "grad_norm": 11.9375,
      "learning_rate": 1.956053901850362e-05,
      "loss": 0.8997,
      "step": 4370
    },
    {
      "epoch": 0.22023330651649237,
      "grad_norm": 18.5,
      "learning_rate": 1.9559533386967017e-05,
      "loss": 1.2917,
      "step": 4380
    },
    {
      "epoch": 0.22073612228479486,
      "grad_norm": 6.96875,
      "learning_rate": 1.9558527755430413e-05,
      "loss": 0.684,
      "step": 4390
    },
    {
      "epoch": 0.22123893805309736,
      "grad_norm": 22.25,
      "learning_rate": 1.9557522123893806e-05,
      "loss": 0.9889,
      "step": 4400
    },
    {
      "epoch": 0.22174175382139985,
      "grad_norm": 39.25,
      "learning_rate": 1.95565164923572e-05,
      "loss": 0.9339,
      "step": 4410
    },
    {
      "epoch": 0.22224456958970235,
      "grad_norm": 46.25,
      "learning_rate": 1.9555510860820597e-05,
      "loss": 0.9435,
      "step": 4420
    },
    {
      "epoch": 0.22274738535800484,
      "grad_norm": 36.75,
      "learning_rate": 1.955450522928399e-05,
      "loss": 1.0856,
      "step": 4430
    },
    {
      "epoch": 0.2232502011263073,
      "grad_norm": 15.625,
      "learning_rate": 1.955349959774739e-05,
      "loss": 0.9348,
      "step": 4440
    },
    {
      "epoch": 0.2237530168946098,
      "grad_norm": 27.25,
      "learning_rate": 1.955249396621078e-05,
      "loss": 0.9071,
      "step": 4450
    },
    {
      "epoch": 0.2242558326629123,
      "grad_norm": 25.75,
      "learning_rate": 1.9551488334674177e-05,
      "loss": 0.9518,
      "step": 4460
    },
    {
      "epoch": 0.2247586484312148,
      "grad_norm": 38.25,
      "learning_rate": 1.9550482703137573e-05,
      "loss": 1.0416,
      "step": 4470
    },
    {
      "epoch": 0.2252614641995173,
      "grad_norm": 7.625,
      "learning_rate": 1.9549477071600966e-05,
      "loss": 1.0962,
      "step": 4480
    },
    {
      "epoch": 0.22576427996781978,
      "grad_norm": 47.5,
      "learning_rate": 1.954847144006436e-05,
      "loss": 1.1588,
      "step": 4490
    },
    {
      "epoch": 0.22626709573612228,
      "grad_norm": 31.25,
      "learning_rate": 1.9547465808527758e-05,
      "loss": 1.0928,
      "step": 4500
    },
    {
      "epoch": 0.22626709573612228,
      "eval_accuracy": 0.5092464055528012,
      "eval_loss": 1.0858546495437622,
      "eval_runtime": 465.0066,
      "eval_samples_per_second": 86.751,
      "eval_steps_per_second": 86.751,
      "step": 4500
    },
    {
      "epoch": 0.22676991150442477,
      "grad_norm": 42.75,
      "learning_rate": 1.954646017699115e-05,
      "loss": 0.8462,
      "step": 4510
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 10.625,
      "learning_rate": 1.9545454545454546e-05,
      "loss": 1.0109,
      "step": 4520
    },
    {
      "epoch": 0.22777554304102976,
      "grad_norm": 16.375,
      "learning_rate": 1.9544448913917942e-05,
      "loss": 0.9981,
      "step": 4530
    },
    {
      "epoch": 0.22827835880933225,
      "grad_norm": 23.5,
      "learning_rate": 1.9543443282381338e-05,
      "loss": 0.9579,
      "step": 4540
    },
    {
      "epoch": 0.22878117457763475,
      "grad_norm": 6.0625,
      "learning_rate": 1.9542437650844734e-05,
      "loss": 1.1276,
      "step": 4550
    },
    {
      "epoch": 0.22928399034593724,
      "grad_norm": 29.375,
      "learning_rate": 1.9541432019308126e-05,
      "loss": 1.0457,
      "step": 4560
    },
    {
      "epoch": 0.22978680611423974,
      "grad_norm": 39.5,
      "learning_rate": 1.9540426387771522e-05,
      "loss": 1.1814,
      "step": 4570
    },
    {
      "epoch": 0.23028962188254223,
      "grad_norm": 11.0625,
      "learning_rate": 1.9539420756234918e-05,
      "loss": 1.5126,
      "step": 4580
    },
    {
      "epoch": 0.23079243765084473,
      "grad_norm": 26.0,
      "learning_rate": 1.953841512469831e-05,
      "loss": 0.8664,
      "step": 4590
    },
    {
      "epoch": 0.23129525341914722,
      "grad_norm": 14.4375,
      "learning_rate": 1.9537409493161706e-05,
      "loss": 1.2398,
      "step": 4600
    },
    {
      "epoch": 0.23179806918744972,
      "grad_norm": 25.25,
      "learning_rate": 1.9536403861625102e-05,
      "loss": 1.1701,
      "step": 4610
    },
    {
      "epoch": 0.2323008849557522,
      "grad_norm": 14.125,
      "learning_rate": 1.9535398230088498e-05,
      "loss": 1.1474,
      "step": 4620
    },
    {
      "epoch": 0.2328037007240547,
      "grad_norm": 38.0,
      "learning_rate": 1.9534392598551894e-05,
      "loss": 0.9085,
      "step": 4630
    },
    {
      "epoch": 0.2333065164923572,
      "grad_norm": 63.0,
      "learning_rate": 1.9533386967015286e-05,
      "loss": 0.9802,
      "step": 4640
    },
    {
      "epoch": 0.2338093322606597,
      "grad_norm": 10.6875,
      "learning_rate": 1.9532381335478682e-05,
      "loss": 0.9867,
      "step": 4650
    },
    {
      "epoch": 0.2343121480289622,
      "grad_norm": 7.9375,
      "learning_rate": 1.9531375703942078e-05,
      "loss": 0.8567,
      "step": 4660
    },
    {
      "epoch": 0.2348149637972647,
      "grad_norm": 14.3125,
      "learning_rate": 1.953037007240547e-05,
      "loss": 0.9659,
      "step": 4670
    },
    {
      "epoch": 0.23531777956556718,
      "grad_norm": 22.125,
      "learning_rate": 1.9529364440868866e-05,
      "loss": 0.9663,
      "step": 4680
    },
    {
      "epoch": 0.23582059533386968,
      "grad_norm": 12.3125,
      "learning_rate": 1.9528358809332262e-05,
      "loss": 1.0533,
      "step": 4690
    },
    {
      "epoch": 0.23632341110217217,
      "grad_norm": 41.75,
      "learning_rate": 1.9527353177795658e-05,
      "loss": 1.3924,
      "step": 4700
    },
    {
      "epoch": 0.23682622687047467,
      "grad_norm": 20.25,
      "learning_rate": 1.9526347546259054e-05,
      "loss": 0.8891,
      "step": 4710
    },
    {
      "epoch": 0.23732904263877716,
      "grad_norm": 7.84375,
      "learning_rate": 1.9525341914722447e-05,
      "loss": 1.4057,
      "step": 4720
    },
    {
      "epoch": 0.23783185840707965,
      "grad_norm": 24.25,
      "learning_rate": 1.9524336283185842e-05,
      "loss": 0.9,
      "step": 4730
    },
    {
      "epoch": 0.23833467417538215,
      "grad_norm": 16.0,
      "learning_rate": 1.9523330651649238e-05,
      "loss": 0.7198,
      "step": 4740
    },
    {
      "epoch": 0.23883748994368464,
      "grad_norm": 52.25,
      "learning_rate": 1.952232502011263e-05,
      "loss": 1.1407,
      "step": 4750
    },
    {
      "epoch": 0.23934030571198714,
      "grad_norm": 24.25,
      "learning_rate": 1.9521319388576027e-05,
      "loss": 0.9783,
      "step": 4760
    },
    {
      "epoch": 0.23984312148028963,
      "grad_norm": 16.625,
      "learning_rate": 1.9520313757039423e-05,
      "loss": 0.7954,
      "step": 4770
    },
    {
      "epoch": 0.24034593724859213,
      "grad_norm": 36.0,
      "learning_rate": 1.951930812550282e-05,
      "loss": 0.7206,
      "step": 4780
    },
    {
      "epoch": 0.24084875301689462,
      "grad_norm": 47.0,
      "learning_rate": 1.951830249396621e-05,
      "loss": 1.1326,
      "step": 4790
    },
    {
      "epoch": 0.2413515687851971,
      "grad_norm": 11.25,
      "learning_rate": 1.9517296862429607e-05,
      "loss": 1.0167,
      "step": 4800
    },
    {
      "epoch": 0.24185438455349959,
      "grad_norm": 24.875,
      "learning_rate": 1.9516291230893003e-05,
      "loss": 0.9624,
      "step": 4810
    },
    {
      "epoch": 0.24235720032180208,
      "grad_norm": 20.125,
      "learning_rate": 1.95152855993564e-05,
      "loss": 1.3877,
      "step": 4820
    },
    {
      "epoch": 0.24286001609010457,
      "grad_norm": 8.8125,
      "learning_rate": 1.951427996781979e-05,
      "loss": 1.1241,
      "step": 4830
    },
    {
      "epoch": 0.24336283185840707,
      "grad_norm": 11.4375,
      "learning_rate": 1.9513274336283187e-05,
      "loss": 1.27,
      "step": 4840
    },
    {
      "epoch": 0.24386564762670956,
      "grad_norm": 31.625,
      "learning_rate": 1.9512268704746583e-05,
      "loss": 1.2343,
      "step": 4850
    },
    {
      "epoch": 0.24436846339501206,
      "grad_norm": 34.5,
      "learning_rate": 1.951126307320998e-05,
      "loss": 1.0984,
      "step": 4860
    },
    {
      "epoch": 0.24487127916331455,
      "grad_norm": 15.875,
      "learning_rate": 1.951025744167337e-05,
      "loss": 1.3032,
      "step": 4870
    },
    {
      "epoch": 0.24537409493161705,
      "grad_norm": 59.75,
      "learning_rate": 1.9509251810136767e-05,
      "loss": 1.1212,
      "step": 4880
    },
    {
      "epoch": 0.24587691069991954,
      "grad_norm": 60.0,
      "learning_rate": 1.9508246178600163e-05,
      "loss": 1.2908,
      "step": 4890
    },
    {
      "epoch": 0.24637972646822204,
      "grad_norm": 23.125,
      "learning_rate": 1.950724054706356e-05,
      "loss": 0.9796,
      "step": 4900
    },
    {
      "epoch": 0.24688254223652453,
      "grad_norm": 6.875,
      "learning_rate": 1.950623491552695e-05,
      "loss": 0.8464,
      "step": 4910
    },
    {
      "epoch": 0.24738535800482703,
      "grad_norm": 31.25,
      "learning_rate": 1.9505229283990347e-05,
      "loss": 1.0916,
      "step": 4920
    },
    {
      "epoch": 0.24788817377312952,
      "grad_norm": 8.8125,
      "learning_rate": 1.9504223652453743e-05,
      "loss": 0.9614,
      "step": 4930
    },
    {
      "epoch": 0.24839098954143202,
      "grad_norm": 17.0,
      "learning_rate": 1.950321802091714e-05,
      "loss": 0.9078,
      "step": 4940
    },
    {
      "epoch": 0.2488938053097345,
      "grad_norm": 26.0,
      "learning_rate": 1.950221238938053e-05,
      "loss": 1.055,
      "step": 4950
    },
    {
      "epoch": 0.249396621078037,
      "grad_norm": 35.75,
      "learning_rate": 1.9501206757843927e-05,
      "loss": 1.2193,
      "step": 4960
    },
    {
      "epoch": 0.2498994368463395,
      "grad_norm": 55.75,
      "learning_rate": 1.9500201126307323e-05,
      "loss": 1.2054,
      "step": 4970
    },
    {
      "epoch": 0.250402252614642,
      "grad_norm": 28.0,
      "learning_rate": 1.949919549477072e-05,
      "loss": 1.0101,
      "step": 4980
    },
    {
      "epoch": 0.25090506838294446,
      "grad_norm": 32.75,
      "learning_rate": 1.949818986323411e-05,
      "loss": 1.0498,
      "step": 4990
    },
    {
      "epoch": 0.251407884151247,
      "grad_norm": 19.0,
      "learning_rate": 1.9497184231697507e-05,
      "loss": 1.1074,
      "step": 5000
    },
    {
      "epoch": 0.251407884151247,
      "eval_accuracy": 0.5091224590976698,
      "eval_loss": 1.0767416954040527,
      "eval_runtime": 464.9769,
      "eval_samples_per_second": 86.757,
      "eval_steps_per_second": 86.757,
      "step": 5000
    },
    {
      "epoch": 0.25191069991954945,
      "grad_norm": 30.0,
      "learning_rate": 1.9496178600160903e-05,
      "loss": 0.7497,
      "step": 5010
    },
    {
      "epoch": 0.252413515687852,
      "grad_norm": 33.25,
      "learning_rate": 1.94951729686243e-05,
      "loss": 0.9702,
      "step": 5020
    },
    {
      "epoch": 0.25291633145615444,
      "grad_norm": 6.59375,
      "learning_rate": 1.949416733708769e-05,
      "loss": 1.0224,
      "step": 5030
    },
    {
      "epoch": 0.25341914722445696,
      "grad_norm": 121.0,
      "learning_rate": 1.9493161705551088e-05,
      "loss": 1.2505,
      "step": 5040
    },
    {
      "epoch": 0.25392196299275943,
      "grad_norm": 20.75,
      "learning_rate": 1.9492156074014483e-05,
      "loss": 0.9511,
      "step": 5050
    },
    {
      "epoch": 0.25442477876106195,
      "grad_norm": 9.5625,
      "learning_rate": 1.9491150442477876e-05,
      "loss": 1.1465,
      "step": 5060
    },
    {
      "epoch": 0.2549275945293644,
      "grad_norm": 41.25,
      "learning_rate": 1.9490144810941272e-05,
      "loss": 1.0936,
      "step": 5070
    },
    {
      "epoch": 0.25543041029766694,
      "grad_norm": 24.75,
      "learning_rate": 1.9489139179404668e-05,
      "loss": 1.0785,
      "step": 5080
    },
    {
      "epoch": 0.2559332260659694,
      "grad_norm": 13.6875,
      "learning_rate": 1.9488133547868064e-05,
      "loss": 1.1495,
      "step": 5090
    },
    {
      "epoch": 0.25643604183427193,
      "grad_norm": 34.25,
      "learning_rate": 1.948712791633146e-05,
      "loss": 0.8534,
      "step": 5100
    },
    {
      "epoch": 0.2569388576025744,
      "grad_norm": 27.625,
      "learning_rate": 1.9486122284794852e-05,
      "loss": 1.2319,
      "step": 5110
    },
    {
      "epoch": 0.2574416733708769,
      "grad_norm": 24.5,
      "learning_rate": 1.9485116653258248e-05,
      "loss": 1.2113,
      "step": 5120
    },
    {
      "epoch": 0.2579444891391794,
      "grad_norm": 53.0,
      "learning_rate": 1.9484111021721644e-05,
      "loss": 0.9822,
      "step": 5130
    },
    {
      "epoch": 0.2584473049074819,
      "grad_norm": 43.0,
      "learning_rate": 1.9483105390185036e-05,
      "loss": 1.168,
      "step": 5140
    },
    {
      "epoch": 0.2589501206757844,
      "grad_norm": 12.1875,
      "learning_rate": 1.9482099758648432e-05,
      "loss": 1.1717,
      "step": 5150
    },
    {
      "epoch": 0.2594529364440869,
      "grad_norm": 52.0,
      "learning_rate": 1.9481094127111828e-05,
      "loss": 1.0938,
      "step": 5160
    },
    {
      "epoch": 0.25995575221238937,
      "grad_norm": 26.875,
      "learning_rate": 1.9480088495575224e-05,
      "loss": 0.9132,
      "step": 5170
    },
    {
      "epoch": 0.2604585679806919,
      "grad_norm": 78.5,
      "learning_rate": 1.947908286403862e-05,
      "loss": 1.2098,
      "step": 5180
    },
    {
      "epoch": 0.26096138374899436,
      "grad_norm": 36.75,
      "learning_rate": 1.9478077232502012e-05,
      "loss": 1.2515,
      "step": 5190
    },
    {
      "epoch": 0.2614641995172969,
      "grad_norm": 24.5,
      "learning_rate": 1.9477071600965408e-05,
      "loss": 0.9122,
      "step": 5200
    },
    {
      "epoch": 0.26196701528559935,
      "grad_norm": 7.53125,
      "learning_rate": 1.9476065969428804e-05,
      "loss": 1.2771,
      "step": 5210
    },
    {
      "epoch": 0.26246983105390187,
      "grad_norm": 13.25,
      "learning_rate": 1.9475060337892196e-05,
      "loss": 1.1985,
      "step": 5220
    },
    {
      "epoch": 0.26297264682220434,
      "grad_norm": 43.0,
      "learning_rate": 1.9474054706355592e-05,
      "loss": 1.1263,
      "step": 5230
    },
    {
      "epoch": 0.26347546259050686,
      "grad_norm": 11.4375,
      "learning_rate": 1.9473049074818988e-05,
      "loss": 1.2956,
      "step": 5240
    },
    {
      "epoch": 0.2639782783588093,
      "grad_norm": 53.5,
      "learning_rate": 1.9472043443282384e-05,
      "loss": 1.0463,
      "step": 5250
    },
    {
      "epoch": 0.26448109412711185,
      "grad_norm": 17.375,
      "learning_rate": 1.947103781174578e-05,
      "loss": 0.8553,
      "step": 5260
    },
    {
      "epoch": 0.2649839098954143,
      "grad_norm": 7.1875,
      "learning_rate": 1.9470032180209172e-05,
      "loss": 1.4328,
      "step": 5270
    },
    {
      "epoch": 0.26548672566371684,
      "grad_norm": 13.875,
      "learning_rate": 1.946902654867257e-05,
      "loss": 0.9863,
      "step": 5280
    },
    {
      "epoch": 0.2659895414320193,
      "grad_norm": 26.625,
      "learning_rate": 1.9468020917135964e-05,
      "loss": 1.0275,
      "step": 5290
    },
    {
      "epoch": 0.2664923572003218,
      "grad_norm": 18.25,
      "learning_rate": 1.9467015285599357e-05,
      "loss": 1.0513,
      "step": 5300
    },
    {
      "epoch": 0.2669951729686243,
      "grad_norm": 23.25,
      "learning_rate": 1.9466009654062753e-05,
      "loss": 0.8166,
      "step": 5310
    },
    {
      "epoch": 0.2674979887369268,
      "grad_norm": 50.0,
      "learning_rate": 1.946500402252615e-05,
      "loss": 0.9392,
      "step": 5320
    },
    {
      "epoch": 0.2680008045052293,
      "grad_norm": 64.5,
      "learning_rate": 1.946399839098954e-05,
      "loss": 0.8568,
      "step": 5330
    },
    {
      "epoch": 0.26850362027353175,
      "grad_norm": 19.375,
      "learning_rate": 1.946299275945294e-05,
      "loss": 0.9158,
      "step": 5340
    },
    {
      "epoch": 0.2690064360418343,
      "grad_norm": 12.3125,
      "learning_rate": 1.9461987127916333e-05,
      "loss": 1.0967,
      "step": 5350
    },
    {
      "epoch": 0.26950925181013674,
      "grad_norm": 51.75,
      "learning_rate": 1.946098149637973e-05,
      "loss": 1.1054,
      "step": 5360
    },
    {
      "epoch": 0.27001206757843926,
      "grad_norm": 6.40625,
      "learning_rate": 1.9459975864843124e-05,
      "loss": 1.2348,
      "step": 5370
    },
    {
      "epoch": 0.27051488334674173,
      "grad_norm": 14.9375,
      "learning_rate": 1.9458970233306517e-05,
      "loss": 0.613,
      "step": 5380
    },
    {
      "epoch": 0.27101769911504425,
      "grad_norm": 40.5,
      "learning_rate": 1.9457964601769913e-05,
      "loss": 0.9572,
      "step": 5390
    },
    {
      "epoch": 0.2715205148833467,
      "grad_norm": 56.75,
      "learning_rate": 1.945695897023331e-05,
      "loss": 0.7692,
      "step": 5400
    },
    {
      "epoch": 0.27202333065164924,
      "grad_norm": 25.75,
      "learning_rate": 1.94559533386967e-05,
      "loss": 0.8777,
      "step": 5410
    },
    {
      "epoch": 0.2725261464199517,
      "grad_norm": 16.625,
      "learning_rate": 1.94549477071601e-05,
      "loss": 0.9062,
      "step": 5420
    },
    {
      "epoch": 0.27302896218825423,
      "grad_norm": 50.5,
      "learning_rate": 1.9453942075623493e-05,
      "loss": 1.1149,
      "step": 5430
    },
    {
      "epoch": 0.2735317779565567,
      "grad_norm": 24.625,
      "learning_rate": 1.945293644408689e-05,
      "loss": 0.9286,
      "step": 5440
    },
    {
      "epoch": 0.2740345937248592,
      "grad_norm": 14.5,
      "learning_rate": 1.9451930812550285e-05,
      "loss": 1.0962,
      "step": 5450
    },
    {
      "epoch": 0.2745374094931617,
      "grad_norm": 16.75,
      "learning_rate": 1.9450925181013677e-05,
      "loss": 1.1608,
      "step": 5460
    },
    {
      "epoch": 0.2750402252614642,
      "grad_norm": 51.75,
      "learning_rate": 1.9449919549477073e-05,
      "loss": 1.1965,
      "step": 5470
    },
    {
      "epoch": 0.2755430410297667,
      "grad_norm": 6.3125,
      "learning_rate": 1.944891391794047e-05,
      "loss": 1.126,
      "step": 5480
    },
    {
      "epoch": 0.2760458567980692,
      "grad_norm": 43.25,
      "learning_rate": 1.944790828640386e-05,
      "loss": 0.8564,
      "step": 5490
    },
    {
      "epoch": 0.27654867256637167,
      "grad_norm": 28.5,
      "learning_rate": 1.944690265486726e-05,
      "loss": 1.4304,
      "step": 5500
    },
    {
      "epoch": 0.27654867256637167,
      "eval_accuracy": 0.5097421913733268,
      "eval_loss": 1.0700719356536865,
      "eval_runtime": 464.6496,
      "eval_samples_per_second": 86.818,
      "eval_steps_per_second": 86.818,
      "step": 5500
    },
    {
      "epoch": 0.2770514883346742,
      "grad_norm": 47.25,
      "learning_rate": 1.9445897023330653e-05,
      "loss": 0.8632,
      "step": 5510
    },
    {
      "epoch": 0.27755430410297666,
      "grad_norm": 6.96875,
      "learning_rate": 1.9444891391794046e-05,
      "loss": 0.9396,
      "step": 5520
    },
    {
      "epoch": 0.2780571198712792,
      "grad_norm": 35.0,
      "learning_rate": 1.9443885760257445e-05,
      "loss": 0.9021,
      "step": 5530
    },
    {
      "epoch": 0.27855993563958165,
      "grad_norm": 13.5,
      "learning_rate": 1.9442880128720837e-05,
      "loss": 0.9664,
      "step": 5540
    },
    {
      "epoch": 0.27906275140788417,
      "grad_norm": 31.25,
      "learning_rate": 1.9441874497184233e-05,
      "loss": 1.5082,
      "step": 5550
    },
    {
      "epoch": 0.27956556717618664,
      "grad_norm": 39.75,
      "learning_rate": 1.944086886564763e-05,
      "loss": 1.3065,
      "step": 5560
    },
    {
      "epoch": 0.28006838294448916,
      "grad_norm": 22.125,
      "learning_rate": 1.9439863234111022e-05,
      "loss": 1.3688,
      "step": 5570
    },
    {
      "epoch": 0.2805711987127916,
      "grad_norm": 37.75,
      "learning_rate": 1.9438857602574418e-05,
      "loss": 0.8464,
      "step": 5580
    },
    {
      "epoch": 0.28107401448109415,
      "grad_norm": 19.625,
      "learning_rate": 1.9437851971037813e-05,
      "loss": 1.3876,
      "step": 5590
    },
    {
      "epoch": 0.2815768302493966,
      "grad_norm": 8.5,
      "learning_rate": 1.9436846339501206e-05,
      "loss": 0.9045,
      "step": 5600
    },
    {
      "epoch": 0.28207964601769914,
      "grad_norm": 5.28125,
      "learning_rate": 1.9435840707964605e-05,
      "loss": 0.9288,
      "step": 5610
    },
    {
      "epoch": 0.2825824617860016,
      "grad_norm": 36.0,
      "learning_rate": 1.9434835076427998e-05,
      "loss": 0.8522,
      "step": 5620
    },
    {
      "epoch": 0.2830852775543041,
      "grad_norm": 12.3125,
      "learning_rate": 1.9433829444891394e-05,
      "loss": 0.9965,
      "step": 5630
    },
    {
      "epoch": 0.2835880933226066,
      "grad_norm": 7.71875,
      "learning_rate": 1.943282381335479e-05,
      "loss": 0.8793,
      "step": 5640
    },
    {
      "epoch": 0.2840909090909091,
      "grad_norm": 3.859375,
      "learning_rate": 1.9431818181818182e-05,
      "loss": 0.8295,
      "step": 5650
    },
    {
      "epoch": 0.2845937248592116,
      "grad_norm": 17.875,
      "learning_rate": 1.9430812550281578e-05,
      "loss": 1.0257,
      "step": 5660
    },
    {
      "epoch": 0.2850965406275141,
      "grad_norm": 23.625,
      "learning_rate": 1.9429806918744974e-05,
      "loss": 1.0378,
      "step": 5670
    },
    {
      "epoch": 0.2855993563958166,
      "grad_norm": 23.125,
      "learning_rate": 1.9428801287208366e-05,
      "loss": 0.9957,
      "step": 5680
    },
    {
      "epoch": 0.28610217216411904,
      "grad_norm": 27.625,
      "learning_rate": 1.9427795655671765e-05,
      "loss": 0.9428,
      "step": 5690
    },
    {
      "epoch": 0.28660498793242156,
      "grad_norm": 14.8125,
      "learning_rate": 1.9426790024135158e-05,
      "loss": 0.8269,
      "step": 5700
    },
    {
      "epoch": 0.28710780370072403,
      "grad_norm": 10.3125,
      "learning_rate": 1.9425784392598554e-05,
      "loss": 1.0813,
      "step": 5710
    },
    {
      "epoch": 0.28761061946902655,
      "grad_norm": 62.0,
      "learning_rate": 1.942477876106195e-05,
      "loss": 0.9799,
      "step": 5720
    },
    {
      "epoch": 0.288113435237329,
      "grad_norm": 51.25,
      "learning_rate": 1.9423773129525342e-05,
      "loss": 0.9181,
      "step": 5730
    },
    {
      "epoch": 0.28861625100563154,
      "grad_norm": 9.5,
      "learning_rate": 1.9422767497988738e-05,
      "loss": 0.9958,
      "step": 5740
    },
    {
      "epoch": 0.289119066773934,
      "grad_norm": 59.0,
      "learning_rate": 1.9421761866452134e-05,
      "loss": 1.1421,
      "step": 5750
    },
    {
      "epoch": 0.28962188254223653,
      "grad_norm": 52.5,
      "learning_rate": 1.9420756234915526e-05,
      "loss": 1.0733,
      "step": 5760
    },
    {
      "epoch": 0.290124698310539,
      "grad_norm": 25.625,
      "learning_rate": 1.9419750603378922e-05,
      "loss": 1.0999,
      "step": 5770
    },
    {
      "epoch": 0.2906275140788415,
      "grad_norm": 20.625,
      "learning_rate": 1.9418744971842318e-05,
      "loss": 1.2945,
      "step": 5780
    },
    {
      "epoch": 0.291130329847144,
      "grad_norm": 29.125,
      "learning_rate": 1.9417739340305714e-05,
      "loss": 1.2831,
      "step": 5790
    },
    {
      "epoch": 0.2916331456154465,
      "grad_norm": 35.0,
      "learning_rate": 1.941673370876911e-05,
      "loss": 0.8187,
      "step": 5800
    },
    {
      "epoch": 0.292135961383749,
      "grad_norm": 7.09375,
      "learning_rate": 1.9415728077232502e-05,
      "loss": 0.9297,
      "step": 5810
    },
    {
      "epoch": 0.2926387771520515,
      "grad_norm": 18.5,
      "learning_rate": 1.94147224456959e-05,
      "loss": 1.0773,
      "step": 5820
    },
    {
      "epoch": 0.29314159292035397,
      "grad_norm": 30.75,
      "learning_rate": 1.9413716814159294e-05,
      "loss": 1.1129,
      "step": 5830
    },
    {
      "epoch": 0.2936444086886565,
      "grad_norm": 14.5,
      "learning_rate": 1.9412711182622687e-05,
      "loss": 1.1675,
      "step": 5840
    },
    {
      "epoch": 0.29414722445695896,
      "grad_norm": 40.25,
      "learning_rate": 1.9411705551086083e-05,
      "loss": 0.9529,
      "step": 5850
    },
    {
      "epoch": 0.2946500402252615,
      "grad_norm": 23.375,
      "learning_rate": 1.941069991954948e-05,
      "loss": 1.122,
      "step": 5860
    },
    {
      "epoch": 0.29515285599356395,
      "grad_norm": 25.5,
      "learning_rate": 1.9409694288012874e-05,
      "loss": 0.8783,
      "step": 5870
    },
    {
      "epoch": 0.29565567176186647,
      "grad_norm": 18.125,
      "learning_rate": 1.940868865647627e-05,
      "loss": 1.1482,
      "step": 5880
    },
    {
      "epoch": 0.29615848753016893,
      "grad_norm": 9.6875,
      "learning_rate": 1.9407683024939663e-05,
      "loss": 1.1647,
      "step": 5890
    },
    {
      "epoch": 0.29666130329847146,
      "grad_norm": 18.875,
      "learning_rate": 1.940667739340306e-05,
      "loss": 1.0886,
      "step": 5900
    },
    {
      "epoch": 0.2971641190667739,
      "grad_norm": 27.375,
      "learning_rate": 1.9405671761866454e-05,
      "loss": 0.9522,
      "step": 5910
    },
    {
      "epoch": 0.29766693483507645,
      "grad_norm": 27.75,
      "learning_rate": 1.9404666130329847e-05,
      "loss": 1.321,
      "step": 5920
    },
    {
      "epoch": 0.2981697506033789,
      "grad_norm": 38.75,
      "learning_rate": 1.9403660498793243e-05,
      "loss": 0.8669,
      "step": 5930
    },
    {
      "epoch": 0.29867256637168144,
      "grad_norm": 56.0,
      "learning_rate": 1.940265486725664e-05,
      "loss": 1.1234,
      "step": 5940
    },
    {
      "epoch": 0.2991753821399839,
      "grad_norm": 15.5,
      "learning_rate": 1.9401649235720035e-05,
      "loss": 1.1081,
      "step": 5950
    },
    {
      "epoch": 0.2996781979082864,
      "grad_norm": 30.25,
      "learning_rate": 1.940064360418343e-05,
      "loss": 0.8737,
      "step": 5960
    },
    {
      "epoch": 0.3001810136765889,
      "grad_norm": 17.5,
      "learning_rate": 1.9399637972646823e-05,
      "loss": 0.9869,
      "step": 5970
    },
    {
      "epoch": 0.3006838294448914,
      "grad_norm": 33.0,
      "learning_rate": 1.939863234111022e-05,
      "loss": 1.0905,
      "step": 5980
    },
    {
      "epoch": 0.3011866452131939,
      "grad_norm": 11.5625,
      "learning_rate": 1.9397626709573615e-05,
      "loss": 0.9725,
      "step": 5990
    },
    {
      "epoch": 0.3016894609814964,
      "grad_norm": 57.0,
      "learning_rate": 1.9396621078037007e-05,
      "loss": 1.0491,
      "step": 6000
    },
    {
      "epoch": 0.3016894609814964,
      "eval_accuracy": 0.5098909271194844,
      "eval_loss": 1.0633625984191895,
      "eval_runtime": 465.0206,
      "eval_samples_per_second": 86.749,
      "eval_steps_per_second": 86.749,
      "step": 6000
    },
    {
      "epoch": 0.30219227674979887,
      "grad_norm": 28.25,
      "learning_rate": 1.9395615446500403e-05,
      "loss": 0.9322,
      "step": 6010
    },
    {
      "epoch": 0.3026950925181014,
      "grad_norm": 12.3125,
      "learning_rate": 1.93946098149638e-05,
      "loss": 0.8184,
      "step": 6020
    },
    {
      "epoch": 0.30319790828640386,
      "grad_norm": 19.625,
      "learning_rate": 1.9393604183427195e-05,
      "loss": 1.1346,
      "step": 6030
    },
    {
      "epoch": 0.30370072405470633,
      "grad_norm": 6.53125,
      "learning_rate": 1.9392598551890587e-05,
      "loss": 0.8971,
      "step": 6040
    },
    {
      "epoch": 0.30420353982300885,
      "grad_norm": 16.875,
      "learning_rate": 1.9391592920353983e-05,
      "loss": 0.8224,
      "step": 6050
    },
    {
      "epoch": 0.3047063555913113,
      "grad_norm": 9.6875,
      "learning_rate": 1.939058728881738e-05,
      "loss": 0.9422,
      "step": 6060
    },
    {
      "epoch": 0.30520917135961384,
      "grad_norm": 8.5625,
      "learning_rate": 1.9389581657280775e-05,
      "loss": 1.1874,
      "step": 6070
    },
    {
      "epoch": 0.3057119871279163,
      "grad_norm": 6.71875,
      "learning_rate": 1.9388576025744167e-05,
      "loss": 0.9917,
      "step": 6080
    },
    {
      "epoch": 0.30621480289621883,
      "grad_norm": 23.125,
      "learning_rate": 1.9387570394207563e-05,
      "loss": 0.8756,
      "step": 6090
    },
    {
      "epoch": 0.3067176186645213,
      "grad_norm": 7.21875,
      "learning_rate": 1.938656476267096e-05,
      "loss": 0.9201,
      "step": 6100
    },
    {
      "epoch": 0.3072204344328238,
      "grad_norm": 47.0,
      "learning_rate": 1.9385559131134355e-05,
      "loss": 0.9502,
      "step": 6110
    },
    {
      "epoch": 0.3077232502011263,
      "grad_norm": 4.625,
      "learning_rate": 1.9384553499597748e-05,
      "loss": 0.9943,
      "step": 6120
    },
    {
      "epoch": 0.3082260659694288,
      "grad_norm": 18.5,
      "learning_rate": 1.9383547868061143e-05,
      "loss": 1.2862,
      "step": 6130
    },
    {
      "epoch": 0.3087288817377313,
      "grad_norm": 40.5,
      "learning_rate": 1.938254223652454e-05,
      "loss": 1.1277,
      "step": 6140
    },
    {
      "epoch": 0.3092316975060338,
      "grad_norm": 57.5,
      "learning_rate": 1.9381536604987935e-05,
      "loss": 1.1259,
      "step": 6150
    },
    {
      "epoch": 0.30973451327433627,
      "grad_norm": 21.5,
      "learning_rate": 1.9380530973451328e-05,
      "loss": 0.8125,
      "step": 6160
    },
    {
      "epoch": 0.3102373290426388,
      "grad_norm": 5.03125,
      "learning_rate": 1.9379525341914724e-05,
      "loss": 0.6966,
      "step": 6170
    },
    {
      "epoch": 0.31074014481094125,
      "grad_norm": 4.8125,
      "learning_rate": 1.937851971037812e-05,
      "loss": 0.847,
      "step": 6180
    },
    {
      "epoch": 0.3112429605792438,
      "grad_norm": 9.5625,
      "learning_rate": 1.9377514078841515e-05,
      "loss": 1.0952,
      "step": 6190
    },
    {
      "epoch": 0.31174577634754624,
      "grad_norm": 18.875,
      "learning_rate": 1.9376508447304908e-05,
      "loss": 0.9307,
      "step": 6200
    },
    {
      "epoch": 0.31224859211584877,
      "grad_norm": 6.625,
      "learning_rate": 1.9375502815768304e-05,
      "loss": 0.9254,
      "step": 6210
    },
    {
      "epoch": 0.31275140788415123,
      "grad_norm": 23.875,
      "learning_rate": 1.93744971842317e-05,
      "loss": 1.2069,
      "step": 6220
    },
    {
      "epoch": 0.31325422365245376,
      "grad_norm": 65.0,
      "learning_rate": 1.9373491552695096e-05,
      "loss": 1.0764,
      "step": 6230
    },
    {
      "epoch": 0.3137570394207562,
      "grad_norm": 29.875,
      "learning_rate": 1.9372485921158488e-05,
      "loss": 1.1759,
      "step": 6240
    },
    {
      "epoch": 0.31425985518905875,
      "grad_norm": 32.75,
      "learning_rate": 1.9371480289621884e-05,
      "loss": 1.2464,
      "step": 6250
    },
    {
      "epoch": 0.3147626709573612,
      "grad_norm": 25.75,
      "learning_rate": 1.937047465808528e-05,
      "loss": 1.1105,
      "step": 6260
    },
    {
      "epoch": 0.31526548672566373,
      "grad_norm": 10.5,
      "learning_rate": 1.9369469026548676e-05,
      "loss": 1.0945,
      "step": 6270
    },
    {
      "epoch": 0.3157683024939662,
      "grad_norm": 15.0,
      "learning_rate": 1.9368463395012068e-05,
      "loss": 0.8642,
      "step": 6280
    },
    {
      "epoch": 0.3162711182622687,
      "grad_norm": 16.125,
      "learning_rate": 1.9367457763475464e-05,
      "loss": 1.229,
      "step": 6290
    },
    {
      "epoch": 0.3167739340305712,
      "grad_norm": 65.5,
      "learning_rate": 1.936645213193886e-05,
      "loss": 1.4241,
      "step": 6300
    },
    {
      "epoch": 0.3172767497988737,
      "grad_norm": 9.0625,
      "learning_rate": 1.9365446500402252e-05,
      "loss": 0.9,
      "step": 6310
    },
    {
      "epoch": 0.3177795655671762,
      "grad_norm": 16.875,
      "learning_rate": 1.9364440868865648e-05,
      "loss": 1.0545,
      "step": 6320
    },
    {
      "epoch": 0.3182823813354787,
      "grad_norm": 12.8125,
      "learning_rate": 1.9363435237329044e-05,
      "loss": 0.6815,
      "step": 6330
    },
    {
      "epoch": 0.31878519710378117,
      "grad_norm": 32.5,
      "learning_rate": 1.936242960579244e-05,
      "loss": 1.2467,
      "step": 6340
    },
    {
      "epoch": 0.3192880128720837,
      "grad_norm": 34.5,
      "learning_rate": 1.9361423974255836e-05,
      "loss": 1.2204,
      "step": 6350
    },
    {
      "epoch": 0.31979082864038616,
      "grad_norm": 54.0,
      "learning_rate": 1.936041834271923e-05,
      "loss": 1.23,
      "step": 6360
    },
    {
      "epoch": 0.3202936444086887,
      "grad_norm": 25.625,
      "learning_rate": 1.9359412711182624e-05,
      "loss": 1.0943,
      "step": 6370
    },
    {
      "epoch": 0.32079646017699115,
      "grad_norm": 19.5,
      "learning_rate": 1.935840707964602e-05,
      "loss": 1.0874,
      "step": 6380
    },
    {
      "epoch": 0.32129927594529367,
      "grad_norm": 35.75,
      "learning_rate": 1.9357401448109413e-05,
      "loss": 1.1238,
      "step": 6390
    },
    {
      "epoch": 0.32180209171359614,
      "grad_norm": 7.3125,
      "learning_rate": 1.935639581657281e-05,
      "loss": 0.7692,
      "step": 6400
    },
    {
      "epoch": 0.3223049074818986,
      "grad_norm": 16.125,
      "learning_rate": 1.9355390185036204e-05,
      "loss": 0.8467,
      "step": 6410
    },
    {
      "epoch": 0.32280772325020113,
      "grad_norm": 47.75,
      "learning_rate": 1.93543845534996e-05,
      "loss": 1.0626,
      "step": 6420
    },
    {
      "epoch": 0.3233105390185036,
      "grad_norm": 21.375,
      "learning_rate": 1.9353378921962996e-05,
      "loss": 0.8573,
      "step": 6430
    },
    {
      "epoch": 0.3238133547868061,
      "grad_norm": 16.625,
      "learning_rate": 1.935237329042639e-05,
      "loss": 0.9439,
      "step": 6440
    },
    {
      "epoch": 0.3243161705551086,
      "grad_norm": 21.25,
      "learning_rate": 1.9351367658889785e-05,
      "loss": 0.9823,
      "step": 6450
    },
    {
      "epoch": 0.3248189863234111,
      "grad_norm": 10.75,
      "learning_rate": 1.935036202735318e-05,
      "loss": 1.2026,
      "step": 6460
    },
    {
      "epoch": 0.3253218020917136,
      "grad_norm": 21.625,
      "learning_rate": 1.9349356395816573e-05,
      "loss": 0.8784,
      "step": 6470
    },
    {
      "epoch": 0.3258246178600161,
      "grad_norm": 18.375,
      "learning_rate": 1.934835076427997e-05,
      "loss": 0.8289,
      "step": 6480
    },
    {
      "epoch": 0.32632743362831856,
      "grad_norm": 9.4375,
      "learning_rate": 1.9347345132743365e-05,
      "loss": 0.7673,
      "step": 6490
    },
    {
      "epoch": 0.3268302493966211,
      "grad_norm": 8.25,
      "learning_rate": 1.934633950120676e-05,
      "loss": 1.2352,
      "step": 6500
    },
    {
      "epoch": 0.3268302493966211,
      "eval_accuracy": 0.5103123450669311,
      "eval_loss": 1.0592018365859985,
      "eval_runtime": 465.1263,
      "eval_samples_per_second": 86.729,
      "eval_steps_per_second": 86.729,
      "step": 6500
    },
    {
      "epoch": 0.32733306516492355,
      "grad_norm": 29.25,
      "learning_rate": 1.9345333869670156e-05,
      "loss": 1.6512,
      "step": 6510
    },
    {
      "epoch": 0.3278358809332261,
      "grad_norm": 20.5,
      "learning_rate": 1.934432823813355e-05,
      "loss": 1.0621,
      "step": 6520
    },
    {
      "epoch": 0.32833869670152854,
      "grad_norm": 52.0,
      "learning_rate": 1.9343322606596945e-05,
      "loss": 1.0706,
      "step": 6530
    },
    {
      "epoch": 0.32884151246983107,
      "grad_norm": 61.5,
      "learning_rate": 1.934231697506034e-05,
      "loss": 1.037,
      "step": 6540
    },
    {
      "epoch": 0.32934432823813353,
      "grad_norm": 6.65625,
      "learning_rate": 1.9341311343523733e-05,
      "loss": 1.4612,
      "step": 6550
    },
    {
      "epoch": 0.32984714400643605,
      "grad_norm": 4.28125,
      "learning_rate": 1.934030571198713e-05,
      "loss": 1.2114,
      "step": 6560
    },
    {
      "epoch": 0.3303499597747385,
      "grad_norm": 51.25,
      "learning_rate": 1.9339300080450525e-05,
      "loss": 1.1068,
      "step": 6570
    },
    {
      "epoch": 0.33085277554304104,
      "grad_norm": 44.0,
      "learning_rate": 1.9338294448913917e-05,
      "loss": 1.0055,
      "step": 6580
    },
    {
      "epoch": 0.3313555913113435,
      "grad_norm": 22.125,
      "learning_rate": 1.9337288817377317e-05,
      "loss": 1.1777,
      "step": 6590
    },
    {
      "epoch": 0.33185840707964603,
      "grad_norm": 18.125,
      "learning_rate": 1.933628318584071e-05,
      "loss": 1.0778,
      "step": 6600
    },
    {
      "epoch": 0.3323612228479485,
      "grad_norm": 30.375,
      "learning_rate": 1.9335277554304105e-05,
      "loss": 1.153,
      "step": 6610
    },
    {
      "epoch": 0.332864038616251,
      "grad_norm": 11.0,
      "learning_rate": 1.93342719227675e-05,
      "loss": 0.6577,
      "step": 6620
    },
    {
      "epoch": 0.3333668543845535,
      "grad_norm": 34.0,
      "learning_rate": 1.9333266291230893e-05,
      "loss": 1.2735,
      "step": 6630
    },
    {
      "epoch": 0.333869670152856,
      "grad_norm": 32.5,
      "learning_rate": 1.933226065969429e-05,
      "loss": 1.0711,
      "step": 6640
    },
    {
      "epoch": 0.3343724859211585,
      "grad_norm": 37.25,
      "learning_rate": 1.9331255028157685e-05,
      "loss": 0.8636,
      "step": 6650
    },
    {
      "epoch": 0.334875301689461,
      "grad_norm": 26.125,
      "learning_rate": 1.9330249396621078e-05,
      "loss": 0.9447,
      "step": 6660
    },
    {
      "epoch": 0.33537811745776347,
      "grad_norm": 90.5,
      "learning_rate": 1.9329243765084477e-05,
      "loss": 1.0429,
      "step": 6670
    },
    {
      "epoch": 0.335880933226066,
      "grad_norm": 30.875,
      "learning_rate": 1.932823813354787e-05,
      "loss": 0.9965,
      "step": 6680
    },
    {
      "epoch": 0.33638374899436846,
      "grad_norm": 17.375,
      "learning_rate": 1.9327232502011265e-05,
      "loss": 1.1872,
      "step": 6690
    },
    {
      "epoch": 0.336886564762671,
      "grad_norm": 83.0,
      "learning_rate": 1.932622687047466e-05,
      "loss": 1.1232,
      "step": 6700
    },
    {
      "epoch": 0.33738938053097345,
      "grad_norm": 27.5,
      "learning_rate": 1.9325221238938054e-05,
      "loss": 0.7805,
      "step": 6710
    },
    {
      "epoch": 0.33789219629927597,
      "grad_norm": 12.125,
      "learning_rate": 1.932421560740145e-05,
      "loss": 1.1476,
      "step": 6720
    },
    {
      "epoch": 0.33839501206757844,
      "grad_norm": 53.0,
      "learning_rate": 1.9323209975864845e-05,
      "loss": 0.8124,
      "step": 6730
    },
    {
      "epoch": 0.33889782783588096,
      "grad_norm": 8.25,
      "learning_rate": 1.9322204344328238e-05,
      "loss": 0.7595,
      "step": 6740
    },
    {
      "epoch": 0.3394006436041834,
      "grad_norm": 12.5,
      "learning_rate": 1.9321198712791637e-05,
      "loss": 1.0598,
      "step": 6750
    },
    {
      "epoch": 0.3399034593724859,
      "grad_norm": 49.0,
      "learning_rate": 1.932019308125503e-05,
      "loss": 1.1691,
      "step": 6760
    },
    {
      "epoch": 0.3404062751407884,
      "grad_norm": 38.75,
      "learning_rate": 1.9319187449718426e-05,
      "loss": 0.9699,
      "step": 6770
    },
    {
      "epoch": 0.3409090909090909,
      "grad_norm": 31.875,
      "learning_rate": 1.931818181818182e-05,
      "loss": 1.1171,
      "step": 6780
    },
    {
      "epoch": 0.3414119066773934,
      "grad_norm": 25.125,
      "learning_rate": 1.9317176186645214e-05,
      "loss": 0.5776,
      "step": 6790
    },
    {
      "epoch": 0.3419147224456959,
      "grad_norm": 17.375,
      "learning_rate": 1.931617055510861e-05,
      "loss": 0.9779,
      "step": 6800
    },
    {
      "epoch": 0.3424175382139984,
      "grad_norm": 14.3125,
      "learning_rate": 1.9315164923572006e-05,
      "loss": 1.0571,
      "step": 6810
    },
    {
      "epoch": 0.34292035398230086,
      "grad_norm": 24.75,
      "learning_rate": 1.9314159292035398e-05,
      "loss": 0.9465,
      "step": 6820
    },
    {
      "epoch": 0.3434231697506034,
      "grad_norm": 7.15625,
      "learning_rate": 1.9313153660498794e-05,
      "loss": 0.9995,
      "step": 6830
    },
    {
      "epoch": 0.34392598551890585,
      "grad_norm": 26.875,
      "learning_rate": 1.931214802896219e-05,
      "loss": 1.1786,
      "step": 6840
    },
    {
      "epoch": 0.3444288012872084,
      "grad_norm": 6.09375,
      "learning_rate": 1.9311142397425582e-05,
      "loss": 0.754,
      "step": 6850
    },
    {
      "epoch": 0.34493161705551084,
      "grad_norm": 9.0,
      "learning_rate": 1.931013676588898e-05,
      "loss": 0.8876,
      "step": 6860
    },
    {
      "epoch": 0.34543443282381336,
      "grad_norm": 30.5,
      "learning_rate": 1.9309131134352374e-05,
      "loss": 1.1487,
      "step": 6870
    },
    {
      "epoch": 0.34593724859211583,
      "grad_norm": 43.5,
      "learning_rate": 1.930812550281577e-05,
      "loss": 1.1542,
      "step": 6880
    },
    {
      "epoch": 0.34644006436041835,
      "grad_norm": 26.5,
      "learning_rate": 1.9307119871279166e-05,
      "loss": 0.9956,
      "step": 6890
    },
    {
      "epoch": 0.3469428801287208,
      "grad_norm": 8.25,
      "learning_rate": 1.930611423974256e-05,
      "loss": 0.6942,
      "step": 6900
    },
    {
      "epoch": 0.34744569589702334,
      "grad_norm": 53.0,
      "learning_rate": 1.9305108608205954e-05,
      "loss": 1.3938,
      "step": 6910
    },
    {
      "epoch": 0.3479485116653258,
      "grad_norm": 22.25,
      "learning_rate": 1.930410297666935e-05,
      "loss": 0.9943,
      "step": 6920
    },
    {
      "epoch": 0.34845132743362833,
      "grad_norm": 21.0,
      "learning_rate": 1.9303097345132743e-05,
      "loss": 0.8032,
      "step": 6930
    },
    {
      "epoch": 0.3489541432019308,
      "grad_norm": 8.8125,
      "learning_rate": 1.9302091713596142e-05,
      "loss": 1.0394,
      "step": 6940
    },
    {
      "epoch": 0.3494569589702333,
      "grad_norm": 62.0,
      "learning_rate": 1.9301086082059534e-05,
      "loss": 0.9424,
      "step": 6950
    },
    {
      "epoch": 0.3499597747385358,
      "grad_norm": 31.375,
      "learning_rate": 1.930008045052293e-05,
      "loss": 1.0775,
      "step": 6960
    },
    {
      "epoch": 0.3504625905068383,
      "grad_norm": 10.4375,
      "learning_rate": 1.9299074818986326e-05,
      "loss": 0.6933,
      "step": 6970
    },
    {
      "epoch": 0.3509654062751408,
      "grad_norm": 9.625,
      "learning_rate": 1.929806918744972e-05,
      "loss": 0.8061,
      "step": 6980
    },
    {
      "epoch": 0.3514682220434433,
      "grad_norm": 20.125,
      "learning_rate": 1.9297063555913115e-05,
      "loss": 1.1225,
      "step": 6990
    },
    {
      "epoch": 0.35197103781174577,
      "grad_norm": 23.5,
      "learning_rate": 1.929605792437651e-05,
      "loss": 0.9743,
      "step": 7000
    },
    {
      "epoch": 0.35197103781174577,
      "eval_accuracy": 0.5102379771938522,
      "eval_loss": 1.0577977895736694,
      "eval_runtime": 464.4627,
      "eval_samples_per_second": 86.853,
      "eval_steps_per_second": 86.853,
      "step": 7000
    },
    {
      "epoch": 0.3524738535800483,
      "grad_norm": 47.75,
      "learning_rate": 1.9295052292839903e-05,
      "loss": 1.0009,
      "step": 7010
    },
    {
      "epoch": 0.35297666934835076,
      "grad_norm": 49.25,
      "learning_rate": 1.9294046661303302e-05,
      "loss": 0.9239,
      "step": 7020
    },
    {
      "epoch": 0.3534794851166533,
      "grad_norm": 22.875,
      "learning_rate": 1.9293041029766695e-05,
      "loss": 1.0584,
      "step": 7030
    },
    {
      "epoch": 0.35398230088495575,
      "grad_norm": 9.75,
      "learning_rate": 1.929203539823009e-05,
      "loss": 1.0942,
      "step": 7040
    },
    {
      "epoch": 0.35448511665325827,
      "grad_norm": 16.625,
      "learning_rate": 1.9291029766693486e-05,
      "loss": 0.8516,
      "step": 7050
    },
    {
      "epoch": 0.35498793242156074,
      "grad_norm": 33.5,
      "learning_rate": 1.929002413515688e-05,
      "loss": 1.2032,
      "step": 7060
    },
    {
      "epoch": 0.35549074818986326,
      "grad_norm": 54.5,
      "learning_rate": 1.9289018503620275e-05,
      "loss": 0.899,
      "step": 7070
    },
    {
      "epoch": 0.3559935639581657,
      "grad_norm": 8.375,
      "learning_rate": 1.928801287208367e-05,
      "loss": 1.1606,
      "step": 7080
    },
    {
      "epoch": 0.35649637972646825,
      "grad_norm": 12.6875,
      "learning_rate": 1.9287007240547063e-05,
      "loss": 1.0961,
      "step": 7090
    },
    {
      "epoch": 0.3569991954947707,
      "grad_norm": 24.5,
      "learning_rate": 1.928600160901046e-05,
      "loss": 0.9918,
      "step": 7100
    },
    {
      "epoch": 0.3575020112630732,
      "grad_norm": 18.75,
      "learning_rate": 1.9284995977473855e-05,
      "loss": 1.0949,
      "step": 7110
    },
    {
      "epoch": 0.3580048270313757,
      "grad_norm": 8.4375,
      "learning_rate": 1.928399034593725e-05,
      "loss": 1.0329,
      "step": 7120
    },
    {
      "epoch": 0.3585076427996782,
      "grad_norm": 16.25,
      "learning_rate": 1.9282984714400647e-05,
      "loss": 1.004,
      "step": 7130
    },
    {
      "epoch": 0.3590104585679807,
      "grad_norm": 21.0,
      "learning_rate": 1.928197908286404e-05,
      "loss": 0.9468,
      "step": 7140
    },
    {
      "epoch": 0.35951327433628316,
      "grad_norm": 30.75,
      "learning_rate": 1.9280973451327435e-05,
      "loss": 1.0689,
      "step": 7150
    },
    {
      "epoch": 0.3600160901045857,
      "grad_norm": 10.375,
      "learning_rate": 1.927996781979083e-05,
      "loss": 0.9706,
      "step": 7160
    },
    {
      "epoch": 0.36051890587288815,
      "grad_norm": 14.8125,
      "learning_rate": 1.9278962188254223e-05,
      "loss": 1.1856,
      "step": 7170
    },
    {
      "epoch": 0.3610217216411907,
      "grad_norm": 10.75,
      "learning_rate": 1.927795655671762e-05,
      "loss": 1.1321,
      "step": 7180
    },
    {
      "epoch": 0.36152453740949314,
      "grad_norm": 9.5,
      "learning_rate": 1.9276950925181015e-05,
      "loss": 0.8051,
      "step": 7190
    },
    {
      "epoch": 0.36202735317779566,
      "grad_norm": 10.9375,
      "learning_rate": 1.927594529364441e-05,
      "loss": 1.1006,
      "step": 7200
    },
    {
      "epoch": 0.36253016894609813,
      "grad_norm": 10.125,
      "learning_rate": 1.9274939662107807e-05,
      "loss": 1.0675,
      "step": 7210
    },
    {
      "epoch": 0.36303298471440065,
      "grad_norm": 51.5,
      "learning_rate": 1.92739340305712e-05,
      "loss": 0.897,
      "step": 7220
    },
    {
      "epoch": 0.3635358004827031,
      "grad_norm": 13.6875,
      "learning_rate": 1.9272928399034595e-05,
      "loss": 0.9654,
      "step": 7230
    },
    {
      "epoch": 0.36403861625100564,
      "grad_norm": 20.0,
      "learning_rate": 1.927192276749799e-05,
      "loss": 0.7575,
      "step": 7240
    },
    {
      "epoch": 0.3645414320193081,
      "grad_norm": 6.4375,
      "learning_rate": 1.9270917135961384e-05,
      "loss": 1.1458,
      "step": 7250
    },
    {
      "epoch": 0.36504424778761063,
      "grad_norm": 21.625,
      "learning_rate": 1.926991150442478e-05,
      "loss": 1.418,
      "step": 7260
    },
    {
      "epoch": 0.3655470635559131,
      "grad_norm": 27.75,
      "learning_rate": 1.9268905872888175e-05,
      "loss": 1.0138,
      "step": 7270
    },
    {
      "epoch": 0.3660498793242156,
      "grad_norm": 24.375,
      "learning_rate": 1.926790024135157e-05,
      "loss": 1.2269,
      "step": 7280
    },
    {
      "epoch": 0.3665526950925181,
      "grad_norm": 10.3125,
      "learning_rate": 1.9266894609814967e-05,
      "loss": 0.933,
      "step": 7290
    },
    {
      "epoch": 0.3670555108608206,
      "grad_norm": 22.0,
      "learning_rate": 1.926588897827836e-05,
      "loss": 1.0832,
      "step": 7300
    },
    {
      "epoch": 0.3675583266291231,
      "grad_norm": 7.8125,
      "learning_rate": 1.9264883346741756e-05,
      "loss": 0.9513,
      "step": 7310
    },
    {
      "epoch": 0.3680611423974256,
      "grad_norm": 24.5,
      "learning_rate": 1.926387771520515e-05,
      "loss": 1.1514,
      "step": 7320
    },
    {
      "epoch": 0.36856395816572807,
      "grad_norm": 89.0,
      "learning_rate": 1.9262872083668544e-05,
      "loss": 1.2868,
      "step": 7330
    },
    {
      "epoch": 0.3690667739340306,
      "grad_norm": 16.25,
      "learning_rate": 1.926186645213194e-05,
      "loss": 1.0162,
      "step": 7340
    },
    {
      "epoch": 0.36956958970233306,
      "grad_norm": 22.5,
      "learning_rate": 1.9260860820595336e-05,
      "loss": 0.9011,
      "step": 7350
    },
    {
      "epoch": 0.3700724054706356,
      "grad_norm": 40.0,
      "learning_rate": 1.925985518905873e-05,
      "loss": 0.9481,
      "step": 7360
    },
    {
      "epoch": 0.37057522123893805,
      "grad_norm": 86.0,
      "learning_rate": 1.9258849557522124e-05,
      "loss": 1.1046,
      "step": 7370
    },
    {
      "epoch": 0.37107803700724057,
      "grad_norm": 14.6875,
      "learning_rate": 1.925784392598552e-05,
      "loss": 0.8537,
      "step": 7380
    },
    {
      "epoch": 0.37158085277554304,
      "grad_norm": 11.6875,
      "learning_rate": 1.9256838294448916e-05,
      "loss": 1.0178,
      "step": 7390
    },
    {
      "epoch": 0.37208366854384556,
      "grad_norm": 11.9375,
      "learning_rate": 1.9255832662912312e-05,
      "loss": 1.2512,
      "step": 7400
    },
    {
      "epoch": 0.372586484312148,
      "grad_norm": 9.1875,
      "learning_rate": 1.9254827031375704e-05,
      "loss": 0.8969,
      "step": 7410
    },
    {
      "epoch": 0.37308930008045055,
      "grad_norm": 7.625,
      "learning_rate": 1.92538213998391e-05,
      "loss": 0.7851,
      "step": 7420
    },
    {
      "epoch": 0.373592115848753,
      "grad_norm": 22.0,
      "learning_rate": 1.9252815768302496e-05,
      "loss": 1.025,
      "step": 7430
    },
    {
      "epoch": 0.37409493161705554,
      "grad_norm": 22.0,
      "learning_rate": 1.9251810136765892e-05,
      "loss": 1.5303,
      "step": 7440
    },
    {
      "epoch": 0.374597747385358,
      "grad_norm": 21.5,
      "learning_rate": 1.9250804505229284e-05,
      "loss": 0.9467,
      "step": 7450
    },
    {
      "epoch": 0.37510056315366047,
      "grad_norm": 15.75,
      "learning_rate": 1.924979887369268e-05,
      "loss": 0.9006,
      "step": 7460
    },
    {
      "epoch": 0.375603378921963,
      "grad_norm": 37.75,
      "learning_rate": 1.9248793242156076e-05,
      "loss": 1.4195,
      "step": 7470
    },
    {
      "epoch": 0.37610619469026546,
      "grad_norm": 30.875,
      "learning_rate": 1.9247787610619472e-05,
      "loss": 0.9651,
      "step": 7480
    },
    {
      "epoch": 0.376609010458568,
      "grad_norm": 17.5,
      "learning_rate": 1.9246781979082864e-05,
      "loss": 1.0637,
      "step": 7490
    },
    {
      "epoch": 0.37711182622687045,
      "grad_norm": 8.5,
      "learning_rate": 1.924577634754626e-05,
      "loss": 0.8582,
      "step": 7500
    },
    {
      "epoch": 0.37711182622687045,
      "eval_accuracy": 0.5097174020823004,
      "eval_loss": 1.0517877340316772,
      "eval_runtime": 466.1231,
      "eval_samples_per_second": 86.544,
      "eval_steps_per_second": 86.544,
      "step": 7500
    },
    {
      "epoch": 0.377614641995173,
      "grad_norm": 7.40625,
      "learning_rate": 1.9244770716009656e-05,
      "loss": 1.2873,
      "step": 7510
    },
    {
      "epoch": 0.37811745776347544,
      "grad_norm": 20.75,
      "learning_rate": 1.9243765084473052e-05,
      "loss": 1.0061,
      "step": 7520
    },
    {
      "epoch": 0.37862027353177796,
      "grad_norm": 39.5,
      "learning_rate": 1.9242759452936445e-05,
      "loss": 1.1145,
      "step": 7530
    },
    {
      "epoch": 0.37912308930008043,
      "grad_norm": 43.25,
      "learning_rate": 1.924175382139984e-05,
      "loss": 0.8651,
      "step": 7540
    },
    {
      "epoch": 0.37962590506838295,
      "grad_norm": 25.0,
      "learning_rate": 1.9240748189863236e-05,
      "loss": 0.8312,
      "step": 7550
    },
    {
      "epoch": 0.3801287208366854,
      "grad_norm": 11.875,
      "learning_rate": 1.9239742558326632e-05,
      "loss": 1.1551,
      "step": 7560
    },
    {
      "epoch": 0.38063153660498794,
      "grad_norm": 37.25,
      "learning_rate": 1.9238736926790025e-05,
      "loss": 0.7049,
      "step": 7570
    },
    {
      "epoch": 0.3811343523732904,
      "grad_norm": 18.375,
      "learning_rate": 1.923773129525342e-05,
      "loss": 1.0855,
      "step": 7580
    },
    {
      "epoch": 0.38163716814159293,
      "grad_norm": 10.5625,
      "learning_rate": 1.9236725663716816e-05,
      "loss": 1.0569,
      "step": 7590
    },
    {
      "epoch": 0.3821399839098954,
      "grad_norm": 25.25,
      "learning_rate": 1.9235720032180212e-05,
      "loss": 1.1306,
      "step": 7600
    },
    {
      "epoch": 0.3826427996781979,
      "grad_norm": 10.875,
      "learning_rate": 1.9234714400643605e-05,
      "loss": 0.9195,
      "step": 7610
    },
    {
      "epoch": 0.3831456154465004,
      "grad_norm": 12.875,
      "learning_rate": 1.9233708769107e-05,
      "loss": 0.9547,
      "step": 7620
    },
    {
      "epoch": 0.3836484312148029,
      "grad_norm": 28.625,
      "learning_rate": 1.9232703137570397e-05,
      "loss": 1.0324,
      "step": 7630
    },
    {
      "epoch": 0.3841512469831054,
      "grad_norm": 5.78125,
      "learning_rate": 1.923169750603379e-05,
      "loss": 0.6095,
      "step": 7640
    },
    {
      "epoch": 0.3846540627514079,
      "grad_norm": 36.75,
      "learning_rate": 1.9230691874497185e-05,
      "loss": 0.9417,
      "step": 7650
    },
    {
      "epoch": 0.38515687851971037,
      "grad_norm": 21.5,
      "learning_rate": 1.922968624296058e-05,
      "loss": 1.4055,
      "step": 7660
    },
    {
      "epoch": 0.3856596942880129,
      "grad_norm": 55.75,
      "learning_rate": 1.9228680611423977e-05,
      "loss": 1.0479,
      "step": 7670
    },
    {
      "epoch": 0.38616251005631536,
      "grad_norm": 16.625,
      "learning_rate": 1.9227674979887373e-05,
      "loss": 0.9061,
      "step": 7680
    },
    {
      "epoch": 0.3866653258246179,
      "grad_norm": 31.625,
      "learning_rate": 1.9226669348350765e-05,
      "loss": 1.1156,
      "step": 7690
    },
    {
      "epoch": 0.38716814159292035,
      "grad_norm": 6.09375,
      "learning_rate": 1.922566371681416e-05,
      "loss": 1.0987,
      "step": 7700
    },
    {
      "epoch": 0.38767095736122287,
      "grad_norm": 18.5,
      "learning_rate": 1.9224658085277557e-05,
      "loss": 0.5755,
      "step": 7710
    },
    {
      "epoch": 0.38817377312952533,
      "grad_norm": 28.625,
      "learning_rate": 1.922365245374095e-05,
      "loss": 1.0541,
      "step": 7720
    },
    {
      "epoch": 0.38867658889782786,
      "grad_norm": 12.375,
      "learning_rate": 1.9222646822204345e-05,
      "loss": 0.8172,
      "step": 7730
    },
    {
      "epoch": 0.3891794046661303,
      "grad_norm": 50.25,
      "learning_rate": 1.922164119066774e-05,
      "loss": 1.1797,
      "step": 7740
    },
    {
      "epoch": 0.38968222043443285,
      "grad_norm": 16.0,
      "learning_rate": 1.9220635559131137e-05,
      "loss": 1.157,
      "step": 7750
    },
    {
      "epoch": 0.3901850362027353,
      "grad_norm": 12.6875,
      "learning_rate": 1.9219629927594533e-05,
      "loss": 0.9178,
      "step": 7760
    },
    {
      "epoch": 0.39068785197103784,
      "grad_norm": 19.875,
      "learning_rate": 1.9218624296057925e-05,
      "loss": 1.0154,
      "step": 7770
    },
    {
      "epoch": 0.3911906677393403,
      "grad_norm": 49.5,
      "learning_rate": 1.921761866452132e-05,
      "loss": 1.3162,
      "step": 7780
    },
    {
      "epoch": 0.3916934835076428,
      "grad_norm": 12.1875,
      "learning_rate": 1.9216613032984717e-05,
      "loss": 1.0808,
      "step": 7790
    },
    {
      "epoch": 0.3921962992759453,
      "grad_norm": 7.09375,
      "learning_rate": 1.921560740144811e-05,
      "loss": 1.0398,
      "step": 7800
    },
    {
      "epoch": 0.3926991150442478,
      "grad_norm": 41.5,
      "learning_rate": 1.9214601769911505e-05,
      "loss": 0.9079,
      "step": 7810
    },
    {
      "epoch": 0.3932019308125503,
      "grad_norm": 37.0,
      "learning_rate": 1.92135961383749e-05,
      "loss": 1.0544,
      "step": 7820
    },
    {
      "epoch": 0.39370474658085275,
      "grad_norm": 24.875,
      "learning_rate": 1.9212590506838297e-05,
      "loss": 1.2406,
      "step": 7830
    },
    {
      "epoch": 0.39420756234915527,
      "grad_norm": 26.25,
      "learning_rate": 1.9211584875301693e-05,
      "loss": 0.9638,
      "step": 7840
    },
    {
      "epoch": 0.39471037811745774,
      "grad_norm": 15.25,
      "learning_rate": 1.9210579243765086e-05,
      "loss": 1.0341,
      "step": 7850
    },
    {
      "epoch": 0.39521319388576026,
      "grad_norm": 12.5625,
      "learning_rate": 1.920957361222848e-05,
      "loss": 1.0407,
      "step": 7860
    },
    {
      "epoch": 0.39571600965406273,
      "grad_norm": 16.625,
      "learning_rate": 1.9208567980691877e-05,
      "loss": 0.8385,
      "step": 7870
    },
    {
      "epoch": 0.39621882542236525,
      "grad_norm": 34.5,
      "learning_rate": 1.920756234915527e-05,
      "loss": 0.9981,
      "step": 7880
    },
    {
      "epoch": 0.3967216411906677,
      "grad_norm": 7.5,
      "learning_rate": 1.9206556717618666e-05,
      "loss": 0.8401,
      "step": 7890
    },
    {
      "epoch": 0.39722445695897024,
      "grad_norm": 9.4375,
      "learning_rate": 1.920555108608206e-05,
      "loss": 1.1007,
      "step": 7900
    },
    {
      "epoch": 0.3977272727272727,
      "grad_norm": 19.75,
      "learning_rate": 1.9204545454545454e-05,
      "loss": 1.3505,
      "step": 7910
    },
    {
      "epoch": 0.39823008849557523,
      "grad_norm": 9.625,
      "learning_rate": 1.9203539823008853e-05,
      "loss": 1.5682,
      "step": 7920
    },
    {
      "epoch": 0.3987329042638777,
      "grad_norm": 12.5625,
      "learning_rate": 1.9202534191472246e-05,
      "loss": 0.9939,
      "step": 7930
    },
    {
      "epoch": 0.3992357200321802,
      "grad_norm": 16.25,
      "learning_rate": 1.9201528559935642e-05,
      "loss": 1.0358,
      "step": 7940
    },
    {
      "epoch": 0.3997385358004827,
      "grad_norm": 15.3125,
      "learning_rate": 1.9200522928399038e-05,
      "loss": 0.9366,
      "step": 7950
    },
    {
      "epoch": 0.4002413515687852,
      "grad_norm": 47.5,
      "learning_rate": 1.919951729686243e-05,
      "loss": 1.0754,
      "step": 7960
    },
    {
      "epoch": 0.4007441673370877,
      "grad_norm": 12.0,
      "learning_rate": 1.9198511665325826e-05,
      "loss": 0.9934,
      "step": 7970
    },
    {
      "epoch": 0.4012469831053902,
      "grad_norm": 26.125,
      "learning_rate": 1.9197506033789222e-05,
      "loss": 1.0099,
      "step": 7980
    },
    {
      "epoch": 0.40174979887369267,
      "grad_norm": 3.578125,
      "learning_rate": 1.9196500402252614e-05,
      "loss": 0.9386,
      "step": 7990
    },
    {
      "epoch": 0.4022526146419952,
      "grad_norm": 5.25,
      "learning_rate": 1.9195494770716014e-05,
      "loss": 0.9987,
      "step": 8000
    },
    {
      "epoch": 0.4022526146419952,
      "eval_accuracy": 0.5106346058502726,
      "eval_loss": 1.047487497329712,
      "eval_runtime": 466.1439,
      "eval_samples_per_second": 86.54,
      "eval_steps_per_second": 86.54,
      "step": 8000
    },
    {
      "epoch": 0.40275543041029765,
      "grad_norm": 11.0,
      "learning_rate": 1.9194489139179406e-05,
      "loss": 1.0605,
      "step": 8010
    },
    {
      "epoch": 0.4032582461786002,
      "grad_norm": 13.75,
      "learning_rate": 1.9193483507642802e-05,
      "loss": 1.0363,
      "step": 8020
    },
    {
      "epoch": 0.40376106194690264,
      "grad_norm": 22.0,
      "learning_rate": 1.9192477876106198e-05,
      "loss": 1.0794,
      "step": 8030
    },
    {
      "epoch": 0.40426387771520517,
      "grad_norm": 12.625,
      "learning_rate": 1.919147224456959e-05,
      "loss": 0.7906,
      "step": 8040
    },
    {
      "epoch": 0.40476669348350763,
      "grad_norm": 12.0625,
      "learning_rate": 1.9190466613032986e-05,
      "loss": 0.931,
      "step": 8050
    },
    {
      "epoch": 0.40526950925181016,
      "grad_norm": 22.0,
      "learning_rate": 1.9189460981496382e-05,
      "loss": 1.0059,
      "step": 8060
    },
    {
      "epoch": 0.4057723250201126,
      "grad_norm": 34.5,
      "learning_rate": 1.9188455349959775e-05,
      "loss": 1.0417,
      "step": 8070
    },
    {
      "epoch": 0.40627514078841515,
      "grad_norm": 21.25,
      "learning_rate": 1.9187449718423174e-05,
      "loss": 1.1063,
      "step": 8080
    },
    {
      "epoch": 0.4067779565567176,
      "grad_norm": 76.5,
      "learning_rate": 1.9186444086886566e-05,
      "loss": 1.2771,
      "step": 8090
    },
    {
      "epoch": 0.40728077232502014,
      "grad_norm": 15.9375,
      "learning_rate": 1.9185438455349962e-05,
      "loss": 1.1099,
      "step": 8100
    },
    {
      "epoch": 0.4077835880933226,
      "grad_norm": 20.75,
      "learning_rate": 1.9184432823813358e-05,
      "loss": 0.8827,
      "step": 8110
    },
    {
      "epoch": 0.4082864038616251,
      "grad_norm": 67.0,
      "learning_rate": 1.918342719227675e-05,
      "loss": 1.0049,
      "step": 8120
    },
    {
      "epoch": 0.4087892196299276,
      "grad_norm": 22.875,
      "learning_rate": 1.9182421560740146e-05,
      "loss": 1.0271,
      "step": 8130
    },
    {
      "epoch": 0.4092920353982301,
      "grad_norm": 10.5,
      "learning_rate": 1.9181415929203542e-05,
      "loss": 0.8669,
      "step": 8140
    },
    {
      "epoch": 0.4097948511665326,
      "grad_norm": 26.0,
      "learning_rate": 1.9180410297666935e-05,
      "loss": 1.3271,
      "step": 8150
    },
    {
      "epoch": 0.4102976669348351,
      "grad_norm": 16.375,
      "learning_rate": 1.917940466613033e-05,
      "loss": 0.8798,
      "step": 8160
    },
    {
      "epoch": 0.41080048270313757,
      "grad_norm": 27.875,
      "learning_rate": 1.9178399034593727e-05,
      "loss": 1.0289,
      "step": 8170
    },
    {
      "epoch": 0.41130329847144004,
      "grad_norm": 6.21875,
      "learning_rate": 1.917739340305712e-05,
      "loss": 0.9026,
      "step": 8180
    },
    {
      "epoch": 0.41180611423974256,
      "grad_norm": 23.75,
      "learning_rate": 1.917638777152052e-05,
      "loss": 1.0456,
      "step": 8190
    },
    {
      "epoch": 0.412308930008045,
      "grad_norm": 30.875,
      "learning_rate": 1.917538213998391e-05,
      "loss": 1.0572,
      "step": 8200
    },
    {
      "epoch": 0.41281174577634755,
      "grad_norm": 18.875,
      "learning_rate": 1.9174376508447307e-05,
      "loss": 0.9083,
      "step": 8210
    },
    {
      "epoch": 0.41331456154465,
      "grad_norm": 30.625,
      "learning_rate": 1.9173370876910703e-05,
      "loss": 0.9577,
      "step": 8220
    },
    {
      "epoch": 0.41381737731295254,
      "grad_norm": 5.40625,
      "learning_rate": 1.9172365245374095e-05,
      "loss": 0.9914,
      "step": 8230
    },
    {
      "epoch": 0.414320193081255,
      "grad_norm": 8.0625,
      "learning_rate": 1.917135961383749e-05,
      "loss": 1.0267,
      "step": 8240
    },
    {
      "epoch": 0.41482300884955753,
      "grad_norm": 18.25,
      "learning_rate": 1.9170353982300887e-05,
      "loss": 1.1447,
      "step": 8250
    },
    {
      "epoch": 0.41532582461786,
      "grad_norm": 16.875,
      "learning_rate": 1.916934835076428e-05,
      "loss": 0.9569,
      "step": 8260
    },
    {
      "epoch": 0.4158286403861625,
      "grad_norm": 18.25,
      "learning_rate": 1.916834271922768e-05,
      "loss": 0.7938,
      "step": 8270
    },
    {
      "epoch": 0.416331456154465,
      "grad_norm": 34.75,
      "learning_rate": 1.916733708769107e-05,
      "loss": 1.0018,
      "step": 8280
    },
    {
      "epoch": 0.4168342719227675,
      "grad_norm": 9.6875,
      "learning_rate": 1.9166331456154467e-05,
      "loss": 0.8144,
      "step": 8290
    },
    {
      "epoch": 0.41733708769107,
      "grad_norm": 10.375,
      "learning_rate": 1.9165325824617863e-05,
      "loss": 0.8868,
      "step": 8300
    },
    {
      "epoch": 0.4178399034593725,
      "grad_norm": 5.40625,
      "learning_rate": 1.9164320193081255e-05,
      "loss": 0.9223,
      "step": 8310
    },
    {
      "epoch": 0.41834271922767496,
      "grad_norm": 27.0,
      "learning_rate": 1.916331456154465e-05,
      "loss": 1.0824,
      "step": 8320
    },
    {
      "epoch": 0.4188455349959775,
      "grad_norm": 24.5,
      "learning_rate": 1.9162308930008047e-05,
      "loss": 1.0215,
      "step": 8330
    },
    {
      "epoch": 0.41934835076427995,
      "grad_norm": 9.0,
      "learning_rate": 1.916130329847144e-05,
      "loss": 0.9315,
      "step": 8340
    },
    {
      "epoch": 0.4198511665325825,
      "grad_norm": 31.5,
      "learning_rate": 1.916029766693484e-05,
      "loss": 1.4123,
      "step": 8350
    },
    {
      "epoch": 0.42035398230088494,
      "grad_norm": 6.78125,
      "learning_rate": 1.915929203539823e-05,
      "loss": 0.9441,
      "step": 8360
    },
    {
      "epoch": 0.42085679806918747,
      "grad_norm": 9.625,
      "learning_rate": 1.9158286403861627e-05,
      "loss": 0.8382,
      "step": 8370
    },
    {
      "epoch": 0.42135961383748993,
      "grad_norm": 45.75,
      "learning_rate": 1.9157280772325023e-05,
      "loss": 1.0135,
      "step": 8380
    },
    {
      "epoch": 0.42186242960579245,
      "grad_norm": 3.609375,
      "learning_rate": 1.9156275140788416e-05,
      "loss": 1.0677,
      "step": 8390
    },
    {
      "epoch": 0.4223652453740949,
      "grad_norm": 65.5,
      "learning_rate": 1.915526950925181e-05,
      "loss": 1.0144,
      "step": 8400
    },
    {
      "epoch": 0.42286806114239744,
      "grad_norm": 51.25,
      "learning_rate": 1.9154263877715207e-05,
      "loss": 1.6016,
      "step": 8410
    },
    {
      "epoch": 0.4233708769106999,
      "grad_norm": 8.875,
      "learning_rate": 1.91532582461786e-05,
      "loss": 1.062,
      "step": 8420
    },
    {
      "epoch": 0.42387369267900243,
      "grad_norm": 7.0,
      "learning_rate": 1.9152252614641996e-05,
      "loss": 0.8623,
      "step": 8430
    },
    {
      "epoch": 0.4243765084473049,
      "grad_norm": 35.25,
      "learning_rate": 1.915124698310539e-05,
      "loss": 1.0106,
      "step": 8440
    },
    {
      "epoch": 0.4248793242156074,
      "grad_norm": 36.5,
      "learning_rate": 1.9150241351568784e-05,
      "loss": 0.9767,
      "step": 8450
    },
    {
      "epoch": 0.4253821399839099,
      "grad_norm": 36.5,
      "learning_rate": 1.9149235720032183e-05,
      "loss": 1.1468,
      "step": 8460
    },
    {
      "epoch": 0.4258849557522124,
      "grad_norm": 23.375,
      "learning_rate": 1.9148230088495576e-05,
      "loss": 1.1806,
      "step": 8470
    },
    {
      "epoch": 0.4263877715205149,
      "grad_norm": 29.0,
      "learning_rate": 1.9147224456958972e-05,
      "loss": 0.8954,
      "step": 8480
    },
    {
      "epoch": 0.4268905872888174,
      "grad_norm": 31.125,
      "learning_rate": 1.9146218825422368e-05,
      "loss": 0.9266,
      "step": 8490
    },
    {
      "epoch": 0.42739340305711987,
      "grad_norm": 17.5,
      "learning_rate": 1.914521319388576e-05,
      "loss": 1.126,
      "step": 8500
    },
    {
      "epoch": 0.42739340305711987,
      "eval_accuracy": 0.5114278631631135,
      "eval_loss": 1.0403167009353638,
      "eval_runtime": 466.2786,
      "eval_samples_per_second": 86.515,
      "eval_steps_per_second": 86.515,
      "step": 8500
    },
    {
      "epoch": 0.4278962188254224,
      "grad_norm": 32.25,
      "learning_rate": 1.9144207562349156e-05,
      "loss": 0.9443,
      "step": 8510
    },
    {
      "epoch": 0.42839903459372486,
      "grad_norm": 47.5,
      "learning_rate": 1.9143201930812552e-05,
      "loss": 0.9892,
      "step": 8520
    },
    {
      "epoch": 0.4289018503620273,
      "grad_norm": 59.5,
      "learning_rate": 1.9142196299275944e-05,
      "loss": 1.1795,
      "step": 8530
    },
    {
      "epoch": 0.42940466613032985,
      "grad_norm": 41.75,
      "learning_rate": 1.9141190667739344e-05,
      "loss": 1.228,
      "step": 8540
    },
    {
      "epoch": 0.4299074818986323,
      "grad_norm": 9.125,
      "learning_rate": 1.9140185036202736e-05,
      "loss": 0.9201,
      "step": 8550
    },
    {
      "epoch": 0.43041029766693484,
      "grad_norm": 36.5,
      "learning_rate": 1.9139179404666132e-05,
      "loss": 0.8624,
      "step": 8560
    },
    {
      "epoch": 0.4309131134352373,
      "grad_norm": 12.8125,
      "learning_rate": 1.9138173773129528e-05,
      "loss": 1.1639,
      "step": 8570
    },
    {
      "epoch": 0.4314159292035398,
      "grad_norm": 48.5,
      "learning_rate": 1.913716814159292e-05,
      "loss": 0.8888,
      "step": 8580
    },
    {
      "epoch": 0.4319187449718423,
      "grad_norm": 23.125,
      "learning_rate": 1.9136162510056316e-05,
      "loss": 0.9679,
      "step": 8590
    },
    {
      "epoch": 0.4324215607401448,
      "grad_norm": 62.75,
      "learning_rate": 1.9135156878519712e-05,
      "loss": 0.9227,
      "step": 8600
    },
    {
      "epoch": 0.4329243765084473,
      "grad_norm": 15.625,
      "learning_rate": 1.9134151246983105e-05,
      "loss": 1.0606,
      "step": 8610
    },
    {
      "epoch": 0.4334271922767498,
      "grad_norm": 14.9375,
      "learning_rate": 1.9133145615446504e-05,
      "loss": 1.3711,
      "step": 8620
    },
    {
      "epoch": 0.4339300080450523,
      "grad_norm": 23.75,
      "learning_rate": 1.9132139983909896e-05,
      "loss": 1.304,
      "step": 8630
    },
    {
      "epoch": 0.4344328238133548,
      "grad_norm": 10.0625,
      "learning_rate": 1.9131134352373292e-05,
      "loss": 1.2273,
      "step": 8640
    },
    {
      "epoch": 0.43493563958165726,
      "grad_norm": 20.25,
      "learning_rate": 1.9130128720836688e-05,
      "loss": 1.1622,
      "step": 8650
    },
    {
      "epoch": 0.4354384553499598,
      "grad_norm": 13.0,
      "learning_rate": 1.912912308930008e-05,
      "loss": 0.9036,
      "step": 8660
    },
    {
      "epoch": 0.43594127111826225,
      "grad_norm": 17.75,
      "learning_rate": 1.9128117457763477e-05,
      "loss": 0.9222,
      "step": 8670
    },
    {
      "epoch": 0.4364440868865648,
      "grad_norm": 24.25,
      "learning_rate": 1.9127111826226872e-05,
      "loss": 0.6133,
      "step": 8680
    },
    {
      "epoch": 0.43694690265486724,
      "grad_norm": 6.28125,
      "learning_rate": 1.9126106194690265e-05,
      "loss": 0.8539,
      "step": 8690
    },
    {
      "epoch": 0.43744971842316976,
      "grad_norm": 21.375,
      "learning_rate": 1.912510056315366e-05,
      "loss": 1.1073,
      "step": 8700
    },
    {
      "epoch": 0.43795253419147223,
      "grad_norm": 28.75,
      "learning_rate": 1.9124094931617057e-05,
      "loss": 1.1409,
      "step": 8710
    },
    {
      "epoch": 0.43845534995977475,
      "grad_norm": 30.5,
      "learning_rate": 1.9123089300080453e-05,
      "loss": 0.989,
      "step": 8720
    },
    {
      "epoch": 0.4389581657280772,
      "grad_norm": 13.3125,
      "learning_rate": 1.912208366854385e-05,
      "loss": 1.1508,
      "step": 8730
    },
    {
      "epoch": 0.43946098149637974,
      "grad_norm": 29.625,
      "learning_rate": 1.912107803700724e-05,
      "loss": 1.114,
      "step": 8740
    },
    {
      "epoch": 0.4399637972646822,
      "grad_norm": 16.625,
      "learning_rate": 1.9120072405470637e-05,
      "loss": 0.8936,
      "step": 8750
    },
    {
      "epoch": 0.44046661303298473,
      "grad_norm": 15.375,
      "learning_rate": 1.9119066773934033e-05,
      "loss": 0.9774,
      "step": 8760
    },
    {
      "epoch": 0.4409694288012872,
      "grad_norm": 63.0,
      "learning_rate": 1.9118061142397425e-05,
      "loss": 1.1199,
      "step": 8770
    },
    {
      "epoch": 0.4414722445695897,
      "grad_norm": 58.75,
      "learning_rate": 1.911705551086082e-05,
      "loss": 0.8529,
      "step": 8780
    },
    {
      "epoch": 0.4419750603378922,
      "grad_norm": 49.75,
      "learning_rate": 1.9116049879324217e-05,
      "loss": 1.2643,
      "step": 8790
    },
    {
      "epoch": 0.4424778761061947,
      "grad_norm": 13.5,
      "learning_rate": 1.9115044247787613e-05,
      "loss": 1.2814,
      "step": 8800
    },
    {
      "epoch": 0.4429806918744972,
      "grad_norm": 13.4375,
      "learning_rate": 1.911403861625101e-05,
      "loss": 1.1046,
      "step": 8810
    },
    {
      "epoch": 0.4434835076427997,
      "grad_norm": 5.71875,
      "learning_rate": 1.91130329847144e-05,
      "loss": 0.9602,
      "step": 8820
    },
    {
      "epoch": 0.44398632341110217,
      "grad_norm": 9.25,
      "learning_rate": 1.9112027353177797e-05,
      "loss": 0.7677,
      "step": 8830
    },
    {
      "epoch": 0.4444891391794047,
      "grad_norm": 7.5625,
      "learning_rate": 1.9111021721641193e-05,
      "loss": 0.9796,
      "step": 8840
    },
    {
      "epoch": 0.44499195494770716,
      "grad_norm": 48.75,
      "learning_rate": 1.9110016090104585e-05,
      "loss": 1.1555,
      "step": 8850
    },
    {
      "epoch": 0.4454947707160097,
      "grad_norm": 6.8125,
      "learning_rate": 1.910901045856798e-05,
      "loss": 0.8604,
      "step": 8860
    },
    {
      "epoch": 0.44599758648431215,
      "grad_norm": 42.25,
      "learning_rate": 1.9108004827031377e-05,
      "loss": 1.484,
      "step": 8870
    },
    {
      "epoch": 0.4465004022526146,
      "grad_norm": 10.3125,
      "learning_rate": 1.9106999195494773e-05,
      "loss": 0.7972,
      "step": 8880
    },
    {
      "epoch": 0.44700321802091714,
      "grad_norm": 54.5,
      "learning_rate": 1.910599356395817e-05,
      "loss": 0.8643,
      "step": 8890
    },
    {
      "epoch": 0.4475060337892196,
      "grad_norm": 6.59375,
      "learning_rate": 1.910498793242156e-05,
      "loss": 1.2997,
      "step": 8900
    },
    {
      "epoch": 0.4480088495575221,
      "grad_norm": 10.125,
      "learning_rate": 1.9103982300884957e-05,
      "loss": 1.0439,
      "step": 8910
    },
    {
      "epoch": 0.4485116653258246,
      "grad_norm": 13.4375,
      "learning_rate": 1.9102976669348353e-05,
      "loss": 0.929,
      "step": 8920
    },
    {
      "epoch": 0.4490144810941271,
      "grad_norm": 9.1875,
      "learning_rate": 1.910197103781175e-05,
      "loss": 1.2822,
      "step": 8930
    },
    {
      "epoch": 0.4495172968624296,
      "grad_norm": 10.9375,
      "learning_rate": 1.910096540627514e-05,
      "loss": 0.9477,
      "step": 8940
    },
    {
      "epoch": 0.4500201126307321,
      "grad_norm": 22.375,
      "learning_rate": 1.9099959774738537e-05,
      "loss": 1.0526,
      "step": 8950
    },
    {
      "epoch": 0.4505229283990346,
      "grad_norm": 17.5,
      "learning_rate": 1.9098954143201933e-05,
      "loss": 0.9101,
      "step": 8960
    },
    {
      "epoch": 0.4510257441673371,
      "grad_norm": 11.0,
      "learning_rate": 1.9097948511665326e-05,
      "loss": 0.9082,
      "step": 8970
    },
    {
      "epoch": 0.45152855993563956,
      "grad_norm": 11.6875,
      "learning_rate": 1.909694288012872e-05,
      "loss": 1.013,
      "step": 8980
    },
    {
      "epoch": 0.4520313757039421,
      "grad_norm": 35.5,
      "learning_rate": 1.9095937248592118e-05,
      "loss": 0.7793,
      "step": 8990
    },
    {
      "epoch": 0.45253419147224455,
      "grad_norm": 21.5,
      "learning_rate": 1.9094931617055513e-05,
      "loss": 1.3949,
      "step": 9000
    },
    {
      "epoch": 0.45253419147224455,
      "eval_accuracy": 0.5107833415964304,
      "eval_loss": 1.0375735759735107,
      "eval_runtime": 465.7876,
      "eval_samples_per_second": 86.606,
      "eval_steps_per_second": 86.606,
      "step": 9000
    },
    {
      "epoch": 0.4530370072405471,
      "grad_norm": 36.0,
      "learning_rate": 1.909392598551891e-05,
      "loss": 1.1965,
      "step": 9010
    },
    {
      "epoch": 0.45353982300884954,
      "grad_norm": 35.75,
      "learning_rate": 1.9092920353982302e-05,
      "loss": 0.9698,
      "step": 9020
    },
    {
      "epoch": 0.45404263877715206,
      "grad_norm": 9.9375,
      "learning_rate": 1.9091914722445698e-05,
      "loss": 1.0362,
      "step": 9030
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 42.5,
      "learning_rate": 1.9090909090909094e-05,
      "loss": 1.1106,
      "step": 9040
    },
    {
      "epoch": 0.45504827031375705,
      "grad_norm": 8.4375,
      "learning_rate": 1.9089903459372486e-05,
      "loss": 1.1679,
      "step": 9050
    },
    {
      "epoch": 0.4555510860820595,
      "grad_norm": 47.5,
      "learning_rate": 1.9088897827835882e-05,
      "loss": 1.0969,
      "step": 9060
    },
    {
      "epoch": 0.45605390185036204,
      "grad_norm": 14.3125,
      "learning_rate": 1.9087892196299278e-05,
      "loss": 1.2004,
      "step": 9070
    },
    {
      "epoch": 0.4565567176186645,
      "grad_norm": 25.25,
      "learning_rate": 1.9086886564762674e-05,
      "loss": 1.2328,
      "step": 9080
    },
    {
      "epoch": 0.45705953338696703,
      "grad_norm": 13.4375,
      "learning_rate": 1.908588093322607e-05,
      "loss": 0.9409,
      "step": 9090
    },
    {
      "epoch": 0.4575623491552695,
      "grad_norm": 34.25,
      "learning_rate": 1.9084875301689462e-05,
      "loss": 0.9461,
      "step": 9100
    },
    {
      "epoch": 0.458065164923572,
      "grad_norm": 62.0,
      "learning_rate": 1.9083869670152858e-05,
      "loss": 1.2634,
      "step": 9110
    },
    {
      "epoch": 0.4585679806918745,
      "grad_norm": 33.75,
      "learning_rate": 1.9082864038616254e-05,
      "loss": 0.9378,
      "step": 9120
    },
    {
      "epoch": 0.459070796460177,
      "grad_norm": 24.25,
      "learning_rate": 1.9081858407079646e-05,
      "loss": 0.7848,
      "step": 9130
    },
    {
      "epoch": 0.4595736122284795,
      "grad_norm": 6.6875,
      "learning_rate": 1.9080852775543042e-05,
      "loss": 0.9517,
      "step": 9140
    },
    {
      "epoch": 0.460076427996782,
      "grad_norm": 5.84375,
      "learning_rate": 1.9079847144006438e-05,
      "loss": 1.0956,
      "step": 9150
    },
    {
      "epoch": 0.46057924376508447,
      "grad_norm": 21.75,
      "learning_rate": 1.9078841512469834e-05,
      "loss": 0.8775,
      "step": 9160
    },
    {
      "epoch": 0.461082059533387,
      "grad_norm": 41.25,
      "learning_rate": 1.907783588093323e-05,
      "loss": 0.8674,
      "step": 9170
    },
    {
      "epoch": 0.46158487530168946,
      "grad_norm": 19.375,
      "learning_rate": 1.9076830249396622e-05,
      "loss": 1.0514,
      "step": 9180
    },
    {
      "epoch": 0.462087691069992,
      "grad_norm": 19.5,
      "learning_rate": 1.9075824617860018e-05,
      "loss": 0.9587,
      "step": 9190
    },
    {
      "epoch": 0.46259050683829445,
      "grad_norm": 38.75,
      "learning_rate": 1.9074818986323414e-05,
      "loss": 1.097,
      "step": 9200
    },
    {
      "epoch": 0.46309332260659697,
      "grad_norm": 73.0,
      "learning_rate": 1.9073813354786807e-05,
      "loss": 1.2164,
      "step": 9210
    },
    {
      "epoch": 0.46359613837489944,
      "grad_norm": 33.0,
      "learning_rate": 1.9072807723250202e-05,
      "loss": 1.4168,
      "step": 9220
    },
    {
      "epoch": 0.46409895414320196,
      "grad_norm": 11.9375,
      "learning_rate": 1.90718020917136e-05,
      "loss": 0.9627,
      "step": 9230
    },
    {
      "epoch": 0.4646017699115044,
      "grad_norm": 18.625,
      "learning_rate": 1.907079646017699e-05,
      "loss": 0.8799,
      "step": 9240
    },
    {
      "epoch": 0.4651045856798069,
      "grad_norm": 15.8125,
      "learning_rate": 1.906979082864039e-05,
      "loss": 1.0375,
      "step": 9250
    },
    {
      "epoch": 0.4656074014481094,
      "grad_norm": 4.90625,
      "learning_rate": 1.9068785197103783e-05,
      "loss": 0.8948,
      "step": 9260
    },
    {
      "epoch": 0.4661102172164119,
      "grad_norm": 17.625,
      "learning_rate": 1.906777956556718e-05,
      "loss": 1.2458,
      "step": 9270
    },
    {
      "epoch": 0.4666130329847144,
      "grad_norm": 51.0,
      "learning_rate": 1.9066773934030574e-05,
      "loss": 0.9732,
      "step": 9280
    },
    {
      "epoch": 0.46711584875301687,
      "grad_norm": 14.5,
      "learning_rate": 1.9065768302493967e-05,
      "loss": 1.1307,
      "step": 9290
    },
    {
      "epoch": 0.4676186645213194,
      "grad_norm": 13.6875,
      "learning_rate": 1.9064762670957363e-05,
      "loss": 0.967,
      "step": 9300
    },
    {
      "epoch": 0.46812148028962186,
      "grad_norm": 32.25,
      "learning_rate": 1.906375703942076e-05,
      "loss": 1.0604,
      "step": 9310
    },
    {
      "epoch": 0.4686242960579244,
      "grad_norm": 18.25,
      "learning_rate": 1.906275140788415e-05,
      "loss": 0.8459,
      "step": 9320
    },
    {
      "epoch": 0.46912711182622685,
      "grad_norm": 22.0,
      "learning_rate": 1.906174577634755e-05,
      "loss": 1.0903,
      "step": 9330
    },
    {
      "epoch": 0.4696299275945294,
      "grad_norm": 48.75,
      "learning_rate": 1.9060740144810943e-05,
      "loss": 1.1277,
      "step": 9340
    },
    {
      "epoch": 0.47013274336283184,
      "grad_norm": 40.0,
      "learning_rate": 1.905973451327434e-05,
      "loss": 0.9395,
      "step": 9350
    },
    {
      "epoch": 0.47063555913113436,
      "grad_norm": 6.21875,
      "learning_rate": 1.9058728881737735e-05,
      "loss": 0.93,
      "step": 9360
    },
    {
      "epoch": 0.47113837489943683,
      "grad_norm": 29.25,
      "learning_rate": 1.9057723250201127e-05,
      "loss": 1.3166,
      "step": 9370
    },
    {
      "epoch": 0.47164119066773935,
      "grad_norm": 34.0,
      "learning_rate": 1.9056717618664523e-05,
      "loss": 0.9168,
      "step": 9380
    },
    {
      "epoch": 0.4721440064360418,
      "grad_norm": 44.0,
      "learning_rate": 1.905571198712792e-05,
      "loss": 1.0779,
      "step": 9390
    },
    {
      "epoch": 0.47264682220434434,
      "grad_norm": 41.25,
      "learning_rate": 1.905470635559131e-05,
      "loss": 0.8738,
      "step": 9400
    },
    {
      "epoch": 0.4731496379726468,
      "grad_norm": 27.125,
      "learning_rate": 1.905370072405471e-05,
      "loss": 0.9966,
      "step": 9410
    },
    {
      "epoch": 0.47365245374094933,
      "grad_norm": 61.5,
      "learning_rate": 1.9052695092518103e-05,
      "loss": 1.3641,
      "step": 9420
    },
    {
      "epoch": 0.4741552695092518,
      "grad_norm": 13.6875,
      "learning_rate": 1.90516894609815e-05,
      "loss": 0.9884,
      "step": 9430
    },
    {
      "epoch": 0.4746580852775543,
      "grad_norm": 10.4375,
      "learning_rate": 1.9050683829444895e-05,
      "loss": 1.0463,
      "step": 9440
    },
    {
      "epoch": 0.4751609010458568,
      "grad_norm": 4.9375,
      "learning_rate": 1.9049678197908287e-05,
      "loss": 0.8838,
      "step": 9450
    },
    {
      "epoch": 0.4756637168141593,
      "grad_norm": 51.0,
      "learning_rate": 1.9048672566371683e-05,
      "loss": 0.8219,
      "step": 9460
    },
    {
      "epoch": 0.4761665325824618,
      "grad_norm": 42.25,
      "learning_rate": 1.904766693483508e-05,
      "loss": 1.2924,
      "step": 9470
    },
    {
      "epoch": 0.4766693483507643,
      "grad_norm": 17.875,
      "learning_rate": 1.904666130329847e-05,
      "loss": 0.7876,
      "step": 9480
    },
    {
      "epoch": 0.47717216411906677,
      "grad_norm": 21.0,
      "learning_rate": 1.9045655671761867e-05,
      "loss": 1.0719,
      "step": 9490
    },
    {
      "epoch": 0.4776749798873693,
      "grad_norm": 8.25,
      "learning_rate": 1.9044650040225263e-05,
      "loss": 1.0151,
      "step": 9500
    },
    {
      "epoch": 0.4776749798873693,
      "eval_accuracy": 0.5104610808130887,
      "eval_loss": 1.034855604171753,
      "eval_runtime": 466.3895,
      "eval_samples_per_second": 86.494,
      "eval_steps_per_second": 86.494,
      "step": 9500
    },
    {
      "epoch": 0.47817779565567176,
      "grad_norm": 51.0,
      "learning_rate": 1.9043644408688656e-05,
      "loss": 1.219,
      "step": 9510
    },
    {
      "epoch": 0.4786806114239743,
      "grad_norm": 9.25,
      "learning_rate": 1.9042638777152055e-05,
      "loss": 1.0742,
      "step": 9520
    },
    {
      "epoch": 0.47918342719227675,
      "grad_norm": 29.25,
      "learning_rate": 1.9041633145615448e-05,
      "loss": 0.8912,
      "step": 9530
    },
    {
      "epoch": 0.47968624296057927,
      "grad_norm": 17.25,
      "learning_rate": 1.9040627514078843e-05,
      "loss": 1.0665,
      "step": 9540
    },
    {
      "epoch": 0.48018905872888173,
      "grad_norm": 28.375,
      "learning_rate": 1.903962188254224e-05,
      "loss": 1.1278,
      "step": 9550
    },
    {
      "epoch": 0.48069187449718426,
      "grad_norm": 10.25,
      "learning_rate": 1.9038616251005632e-05,
      "loss": 1.1241,
      "step": 9560
    },
    {
      "epoch": 0.4811946902654867,
      "grad_norm": 29.25,
      "learning_rate": 1.9037610619469028e-05,
      "loss": 0.9313,
      "step": 9570
    },
    {
      "epoch": 0.48169750603378925,
      "grad_norm": 18.25,
      "learning_rate": 1.9036604987932424e-05,
      "loss": 1.1297,
      "step": 9580
    },
    {
      "epoch": 0.4822003218020917,
      "grad_norm": 71.5,
      "learning_rate": 1.9035599356395816e-05,
      "loss": 1.1165,
      "step": 9590
    },
    {
      "epoch": 0.4827031375703942,
      "grad_norm": 23.0,
      "learning_rate": 1.9034593724859215e-05,
      "loss": 0.93,
      "step": 9600
    },
    {
      "epoch": 0.4832059533386967,
      "grad_norm": 65.0,
      "learning_rate": 1.9033588093322608e-05,
      "loss": 0.8615,
      "step": 9610
    },
    {
      "epoch": 0.48370876910699917,
      "grad_norm": 37.5,
      "learning_rate": 1.9032582461786004e-05,
      "loss": 0.8139,
      "step": 9620
    },
    {
      "epoch": 0.4842115848753017,
      "grad_norm": 20.5,
      "learning_rate": 1.90315768302494e-05,
      "loss": 1.2609,
      "step": 9630
    },
    {
      "epoch": 0.48471440064360416,
      "grad_norm": 12.8125,
      "learning_rate": 1.9030571198712792e-05,
      "loss": 0.9516,
      "step": 9640
    },
    {
      "epoch": 0.4852172164119067,
      "grad_norm": 33.0,
      "learning_rate": 1.9029565567176188e-05,
      "loss": 1.1764,
      "step": 9650
    },
    {
      "epoch": 0.48572003218020915,
      "grad_norm": 22.625,
      "learning_rate": 1.9028559935639584e-05,
      "loss": 1.2671,
      "step": 9660
    },
    {
      "epoch": 0.48622284794851167,
      "grad_norm": 23.0,
      "learning_rate": 1.9027554304102976e-05,
      "loss": 1.1002,
      "step": 9670
    },
    {
      "epoch": 0.48672566371681414,
      "grad_norm": 64.5,
      "learning_rate": 1.9026548672566376e-05,
      "loss": 1.049,
      "step": 9680
    },
    {
      "epoch": 0.48722847948511666,
      "grad_norm": 15.75,
      "learning_rate": 1.9025543041029768e-05,
      "loss": 0.9913,
      "step": 9690
    },
    {
      "epoch": 0.48773129525341913,
      "grad_norm": 49.0,
      "learning_rate": 1.9024537409493164e-05,
      "loss": 1.1387,
      "step": 9700
    },
    {
      "epoch": 0.48823411102172165,
      "grad_norm": 22.5,
      "learning_rate": 1.902353177795656e-05,
      "loss": 1.103,
      "step": 9710
    },
    {
      "epoch": 0.4887369267900241,
      "grad_norm": 23.375,
      "learning_rate": 1.9022526146419952e-05,
      "loss": 0.8626,
      "step": 9720
    },
    {
      "epoch": 0.48923974255832664,
      "grad_norm": 42.75,
      "learning_rate": 1.9021520514883348e-05,
      "loss": 1.0339,
      "step": 9730
    },
    {
      "epoch": 0.4897425583266291,
      "grad_norm": 14.8125,
      "learning_rate": 1.9020514883346744e-05,
      "loss": 0.8951,
      "step": 9740
    },
    {
      "epoch": 0.49024537409493163,
      "grad_norm": 34.25,
      "learning_rate": 1.9019509251810137e-05,
      "loss": 1.2779,
      "step": 9750
    },
    {
      "epoch": 0.4907481898632341,
      "grad_norm": 17.125,
      "learning_rate": 1.9018503620273532e-05,
      "loss": 1.1898,
      "step": 9760
    },
    {
      "epoch": 0.4912510056315366,
      "grad_norm": 19.125,
      "learning_rate": 1.901749798873693e-05,
      "loss": 0.9314,
      "step": 9770
    },
    {
      "epoch": 0.4917538213998391,
      "grad_norm": 22.125,
      "learning_rate": 1.901649235720032e-05,
      "loss": 0.9469,
      "step": 9780
    },
    {
      "epoch": 0.4922566371681416,
      "grad_norm": 30.375,
      "learning_rate": 1.901548672566372e-05,
      "loss": 1.0304,
      "step": 9790
    },
    {
      "epoch": 0.4927594529364441,
      "grad_norm": 10.625,
      "learning_rate": 1.9014481094127113e-05,
      "loss": 1.0151,
      "step": 9800
    },
    {
      "epoch": 0.4932622687047466,
      "grad_norm": 21.875,
      "learning_rate": 1.901347546259051e-05,
      "loss": 0.7649,
      "step": 9810
    },
    {
      "epoch": 0.49376508447304907,
      "grad_norm": 26.375,
      "learning_rate": 1.9012469831053904e-05,
      "loss": 0.9071,
      "step": 9820
    },
    {
      "epoch": 0.4942679002413516,
      "grad_norm": 14.3125,
      "learning_rate": 1.9011464199517297e-05,
      "loss": 1.0004,
      "step": 9830
    },
    {
      "epoch": 0.49477071600965405,
      "grad_norm": 8.5,
      "learning_rate": 1.9010458567980693e-05,
      "loss": 0.8321,
      "step": 9840
    },
    {
      "epoch": 0.4952735317779566,
      "grad_norm": 10.9375,
      "learning_rate": 1.900945293644409e-05,
      "loss": 1.0412,
      "step": 9850
    },
    {
      "epoch": 0.49577634754625904,
      "grad_norm": 26.0,
      "learning_rate": 1.900844730490748e-05,
      "loss": 0.8688,
      "step": 9860
    },
    {
      "epoch": 0.49627916331456157,
      "grad_norm": 12.5,
      "learning_rate": 1.900744167337088e-05,
      "loss": 0.8936,
      "step": 9870
    },
    {
      "epoch": 0.49678197908286403,
      "grad_norm": 11.875,
      "learning_rate": 1.9006436041834273e-05,
      "loss": 1.2581,
      "step": 9880
    },
    {
      "epoch": 0.49728479485116656,
      "grad_norm": 20.5,
      "learning_rate": 1.900543041029767e-05,
      "loss": 0.9878,
      "step": 9890
    },
    {
      "epoch": 0.497787610619469,
      "grad_norm": 21.125,
      "learning_rate": 1.9004424778761065e-05,
      "loss": 0.9876,
      "step": 9900
    },
    {
      "epoch": 0.49829042638777155,
      "grad_norm": 8.25,
      "learning_rate": 1.9003419147224457e-05,
      "loss": 0.9665,
      "step": 9910
    },
    {
      "epoch": 0.498793242156074,
      "grad_norm": 24.125,
      "learning_rate": 1.9002413515687853e-05,
      "loss": 1.3668,
      "step": 9920
    },
    {
      "epoch": 0.49929605792437654,
      "grad_norm": 6.875,
      "learning_rate": 1.900140788415125e-05,
      "loss": 0.7967,
      "step": 9930
    },
    {
      "epoch": 0.499798873692679,
      "grad_norm": 15.4375,
      "learning_rate": 1.900040225261464e-05,
      "loss": 0.9284,
      "step": 9940
    },
    {
      "epoch": 0.5003016894609815,
      "grad_norm": 18.5,
      "learning_rate": 1.899939662107804e-05,
      "loss": 0.9197,
      "step": 9950
    },
    {
      "epoch": 0.500804505229284,
      "grad_norm": 25.875,
      "learning_rate": 1.8998390989541433e-05,
      "loss": 0.9592,
      "step": 9960
    },
    {
      "epoch": 0.5013073209975865,
      "grad_norm": 28.0,
      "learning_rate": 1.899738535800483e-05,
      "loss": 1.2216,
      "step": 9970
    },
    {
      "epoch": 0.5018101367658889,
      "grad_norm": 20.75,
      "learning_rate": 1.8996379726468225e-05,
      "loss": 1.1536,
      "step": 9980
    },
    {
      "epoch": 0.5023129525341915,
      "grad_norm": 54.25,
      "learning_rate": 1.8995374094931617e-05,
      "loss": 1.3039,
      "step": 9990
    },
    {
      "epoch": 0.502815768302494,
      "grad_norm": 18.5,
      "learning_rate": 1.8994368463395013e-05,
      "loss": 0.9787,
      "step": 10000
    },
    {
      "epoch": 0.502815768302494,
      "eval_accuracy": 0.5106346058502726,
      "eval_loss": 1.0351945161819458,
      "eval_runtime": 466.2164,
      "eval_samples_per_second": 86.526,
      "eval_steps_per_second": 86.526,
      "step": 10000
    },
    {
      "epoch": 0.5033185840707964,
      "grad_norm": 19.75,
      "learning_rate": 1.899336283185841e-05,
      "loss": 0.9965,
      "step": 10010
    },
    {
      "epoch": 0.5038213998390989,
      "grad_norm": 21.125,
      "learning_rate": 1.89923572003218e-05,
      "loss": 1.0149,
      "step": 10020
    },
    {
      "epoch": 0.5043242156074015,
      "grad_norm": 5.09375,
      "learning_rate": 1.8991351568785197e-05,
      "loss": 0.9242,
      "step": 10030
    },
    {
      "epoch": 0.504827031375704,
      "grad_norm": 17.375,
      "learning_rate": 1.8990345937248593e-05,
      "loss": 1.2138,
      "step": 10040
    },
    {
      "epoch": 0.5053298471440064,
      "grad_norm": 5.9375,
      "learning_rate": 1.898934030571199e-05,
      "loss": 0.8992,
      "step": 10050
    },
    {
      "epoch": 0.5058326629123089,
      "grad_norm": 13.0,
      "learning_rate": 1.8988334674175385e-05,
      "loss": 1.0119,
      "step": 10060
    },
    {
      "epoch": 0.5063354786806115,
      "grad_norm": 22.0,
      "learning_rate": 1.8987329042638778e-05,
      "loss": 0.798,
      "step": 10070
    },
    {
      "epoch": 0.5068382944489139,
      "grad_norm": 5.59375,
      "learning_rate": 1.8986323411102173e-05,
      "loss": 1.0676,
      "step": 10080
    },
    {
      "epoch": 0.5073411102172164,
      "grad_norm": 52.5,
      "learning_rate": 1.898531777956557e-05,
      "loss": 1.0114,
      "step": 10090
    },
    {
      "epoch": 0.5078439259855189,
      "grad_norm": 14.3125,
      "learning_rate": 1.8984312148028962e-05,
      "loss": 1.0609,
      "step": 10100
    },
    {
      "epoch": 0.5083467417538214,
      "grad_norm": 76.0,
      "learning_rate": 1.8983306516492358e-05,
      "loss": 0.9045,
      "step": 10110
    },
    {
      "epoch": 0.5088495575221239,
      "grad_norm": 60.0,
      "learning_rate": 1.8982300884955754e-05,
      "loss": 1.2703,
      "step": 10120
    },
    {
      "epoch": 0.5093523732904264,
      "grad_norm": 44.0,
      "learning_rate": 1.898129525341915e-05,
      "loss": 0.9816,
      "step": 10130
    },
    {
      "epoch": 0.5098551890587288,
      "grad_norm": 11.3125,
      "learning_rate": 1.8980289621882545e-05,
      "loss": 1.0559,
      "step": 10140
    },
    {
      "epoch": 0.5103580048270314,
      "grad_norm": 11.875,
      "learning_rate": 1.8979283990345938e-05,
      "loss": 0.7811,
      "step": 10150
    },
    {
      "epoch": 0.5108608205953339,
      "grad_norm": 7.46875,
      "learning_rate": 1.8978278358809334e-05,
      "loss": 0.9885,
      "step": 10160
    },
    {
      "epoch": 0.5113636363636364,
      "grad_norm": 23.625,
      "learning_rate": 1.897727272727273e-05,
      "loss": 1.1783,
      "step": 10170
    },
    {
      "epoch": 0.5118664521319388,
      "grad_norm": 21.25,
      "learning_rate": 1.8976267095736122e-05,
      "loss": 1.1508,
      "step": 10180
    },
    {
      "epoch": 0.5123692679002414,
      "grad_norm": 14.4375,
      "learning_rate": 1.8975261464199518e-05,
      "loss": 0.7945,
      "step": 10190
    },
    {
      "epoch": 0.5128720836685439,
      "grad_norm": 38.75,
      "learning_rate": 1.8974255832662914e-05,
      "loss": 1.182,
      "step": 10200
    },
    {
      "epoch": 0.5133748994368463,
      "grad_norm": 19.375,
      "learning_rate": 1.897325020112631e-05,
      "loss": 0.8987,
      "step": 10210
    },
    {
      "epoch": 0.5138777152051488,
      "grad_norm": 10.1875,
      "learning_rate": 1.8972244569589706e-05,
      "loss": 0.994,
      "step": 10220
    },
    {
      "epoch": 0.5143805309734514,
      "grad_norm": 21.25,
      "learning_rate": 1.8971238938053098e-05,
      "loss": 0.9795,
      "step": 10230
    },
    {
      "epoch": 0.5148833467417538,
      "grad_norm": 31.0,
      "learning_rate": 1.8970233306516494e-05,
      "loss": 1.0142,
      "step": 10240
    },
    {
      "epoch": 0.5153861625100563,
      "grad_norm": 27.625,
      "learning_rate": 1.896922767497989e-05,
      "loss": 1.1256,
      "step": 10250
    },
    {
      "epoch": 0.5158889782783588,
      "grad_norm": 11.4375,
      "learning_rate": 1.8968222043443282e-05,
      "loss": 1.1286,
      "step": 10260
    },
    {
      "epoch": 0.5163917940466614,
      "grad_norm": 23.625,
      "learning_rate": 1.8967216411906678e-05,
      "loss": 1.1225,
      "step": 10270
    },
    {
      "epoch": 0.5168946098149638,
      "grad_norm": 47.75,
      "learning_rate": 1.8966210780370074e-05,
      "loss": 1.25,
      "step": 10280
    },
    {
      "epoch": 0.5173974255832663,
      "grad_norm": 33.25,
      "learning_rate": 1.896520514883347e-05,
      "loss": 1.1116,
      "step": 10290
    },
    {
      "epoch": 0.5179002413515688,
      "grad_norm": 22.5,
      "learning_rate": 1.8964199517296862e-05,
      "loss": 0.6778,
      "step": 10300
    },
    {
      "epoch": 0.5184030571198712,
      "grad_norm": 48.25,
      "learning_rate": 1.896319388576026e-05,
      "loss": 0.957,
      "step": 10310
    },
    {
      "epoch": 0.5189058728881738,
      "grad_norm": 4.5,
      "learning_rate": 1.8962188254223654e-05,
      "loss": 1.1111,
      "step": 10320
    },
    {
      "epoch": 0.5194086886564763,
      "grad_norm": 43.0,
      "learning_rate": 1.896118262268705e-05,
      "loss": 1.0707,
      "step": 10330
    },
    {
      "epoch": 0.5199115044247787,
      "grad_norm": 21.625,
      "learning_rate": 1.8960176991150443e-05,
      "loss": 0.8648,
      "step": 10340
    },
    {
      "epoch": 0.5204143201930812,
      "grad_norm": 38.75,
      "learning_rate": 1.895917135961384e-05,
      "loss": 0.766,
      "step": 10350
    },
    {
      "epoch": 0.5209171359613838,
      "grad_norm": 12.0,
      "learning_rate": 1.8958165728077234e-05,
      "loss": 1.0474,
      "step": 10360
    },
    {
      "epoch": 0.5214199517296862,
      "grad_norm": 37.75,
      "learning_rate": 1.895716009654063e-05,
      "loss": 1.0527,
      "step": 10370
    },
    {
      "epoch": 0.5219227674979887,
      "grad_norm": 5.0625,
      "learning_rate": 1.8956154465004023e-05,
      "loss": 1.2607,
      "step": 10380
    },
    {
      "epoch": 0.5224255832662912,
      "grad_norm": 8.25,
      "learning_rate": 1.895514883346742e-05,
      "loss": 0.8445,
      "step": 10390
    },
    {
      "epoch": 0.5229283990345938,
      "grad_norm": 26.875,
      "learning_rate": 1.8954143201930815e-05,
      "loss": 1.0285,
      "step": 10400
    },
    {
      "epoch": 0.5234312148028962,
      "grad_norm": 28.0,
      "learning_rate": 1.895313757039421e-05,
      "loss": 1.4137,
      "step": 10410
    },
    {
      "epoch": 0.5239340305711987,
      "grad_norm": 6.5625,
      "learning_rate": 1.8952131938857603e-05,
      "loss": 1.0679,
      "step": 10420
    },
    {
      "epoch": 0.5244368463395012,
      "grad_norm": 17.0,
      "learning_rate": 1.8951126307321e-05,
      "loss": 0.9416,
      "step": 10430
    },
    {
      "epoch": 0.5249396621078037,
      "grad_norm": 65.0,
      "learning_rate": 1.8950120675784395e-05,
      "loss": 1.0251,
      "step": 10440
    },
    {
      "epoch": 0.5254424778761062,
      "grad_norm": 14.3125,
      "learning_rate": 1.894911504424779e-05,
      "loss": 1.2048,
      "step": 10450
    },
    {
      "epoch": 0.5259452936444087,
      "grad_norm": 12.1875,
      "learning_rate": 1.8948109412711183e-05,
      "loss": 0.9678,
      "step": 10460
    },
    {
      "epoch": 0.5264481094127111,
      "grad_norm": 28.25,
      "learning_rate": 1.894710378117458e-05,
      "loss": 1.3121,
      "step": 10470
    },
    {
      "epoch": 0.5269509251810137,
      "grad_norm": 18.125,
      "learning_rate": 1.8946098149637975e-05,
      "loss": 1.1789,
      "step": 10480
    },
    {
      "epoch": 0.5274537409493162,
      "grad_norm": 33.75,
      "learning_rate": 1.894509251810137e-05,
      "loss": 0.9325,
      "step": 10490
    },
    {
      "epoch": 0.5279565567176187,
      "grad_norm": 10.75,
      "learning_rate": 1.8944086886564763e-05,
      "loss": 1.1319,
      "step": 10500
    },
    {
      "epoch": 0.5279565567176187,
      "eval_accuracy": 0.5112543381259296,
      "eval_loss": 1.0339479446411133,
      "eval_runtime": 465.1461,
      "eval_samples_per_second": 86.725,
      "eval_steps_per_second": 86.725,
      "step": 10500
    },
    {
      "epoch": 0.5284593724859211,
      "grad_norm": 9.75,
      "learning_rate": 1.894308125502816e-05,
      "loss": 1.4224,
      "step": 10510
    },
    {
      "epoch": 0.5289621882542237,
      "grad_norm": 10.1875,
      "learning_rate": 1.8942075623491555e-05,
      "loss": 0.937,
      "step": 10520
    },
    {
      "epoch": 0.5294650040225262,
      "grad_norm": 20.0,
      "learning_rate": 1.894106999195495e-05,
      "loss": 0.9158,
      "step": 10530
    },
    {
      "epoch": 0.5299678197908286,
      "grad_norm": 6.84375,
      "learning_rate": 1.8940064360418343e-05,
      "loss": 1.0714,
      "step": 10540
    },
    {
      "epoch": 0.5304706355591311,
      "grad_norm": 25.125,
      "learning_rate": 1.893905872888174e-05,
      "loss": 1.2289,
      "step": 10550
    },
    {
      "epoch": 0.5309734513274337,
      "grad_norm": 70.0,
      "learning_rate": 1.8938053097345135e-05,
      "loss": 1.3075,
      "step": 10560
    },
    {
      "epoch": 0.5314762670957361,
      "grad_norm": 33.25,
      "learning_rate": 1.8937047465808528e-05,
      "loss": 1.114,
      "step": 10570
    },
    {
      "epoch": 0.5319790828640386,
      "grad_norm": 11.75,
      "learning_rate": 1.8936041834271923e-05,
      "loss": 0.8508,
      "step": 10580
    },
    {
      "epoch": 0.5324818986323411,
      "grad_norm": 25.5,
      "learning_rate": 1.893503620273532e-05,
      "loss": 0.9953,
      "step": 10590
    },
    {
      "epoch": 0.5329847144006437,
      "grad_norm": 39.25,
      "learning_rate": 1.8934030571198715e-05,
      "loss": 1.2255,
      "step": 10600
    },
    {
      "epoch": 0.5334875301689461,
      "grad_norm": 32.75,
      "learning_rate": 1.893302493966211e-05,
      "loss": 1.0123,
      "step": 10610
    },
    {
      "epoch": 0.5339903459372486,
      "grad_norm": 21.125,
      "learning_rate": 1.8932019308125504e-05,
      "loss": 0.9988,
      "step": 10620
    },
    {
      "epoch": 0.5344931617055511,
      "grad_norm": 40.0,
      "learning_rate": 1.89310136765889e-05,
      "loss": 0.9863,
      "step": 10630
    },
    {
      "epoch": 0.5349959774738536,
      "grad_norm": 14.875,
      "learning_rate": 1.8930008045052295e-05,
      "loss": 0.8027,
      "step": 10640
    },
    {
      "epoch": 0.5354987932421561,
      "grad_norm": 29.0,
      "learning_rate": 1.8929002413515688e-05,
      "loss": 1.098,
      "step": 10650
    },
    {
      "epoch": 0.5360016090104586,
      "grad_norm": 35.75,
      "learning_rate": 1.8927996781979087e-05,
      "loss": 1.0168,
      "step": 10660
    },
    {
      "epoch": 0.536504424778761,
      "grad_norm": 28.375,
      "learning_rate": 1.892699115044248e-05,
      "loss": 1.1876,
      "step": 10670
    },
    {
      "epoch": 0.5370072405470635,
      "grad_norm": 41.5,
      "learning_rate": 1.8925985518905875e-05,
      "loss": 1.1681,
      "step": 10680
    },
    {
      "epoch": 0.5375100563153661,
      "grad_norm": 25.625,
      "learning_rate": 1.892497988736927e-05,
      "loss": 0.8655,
      "step": 10690
    },
    {
      "epoch": 0.5380128720836685,
      "grad_norm": 15.0,
      "learning_rate": 1.8923974255832664e-05,
      "loss": 0.802,
      "step": 10700
    },
    {
      "epoch": 0.538515687851971,
      "grad_norm": 37.25,
      "learning_rate": 1.892296862429606e-05,
      "loss": 1.0533,
      "step": 10710
    },
    {
      "epoch": 0.5390185036202735,
      "grad_norm": 14.75,
      "learning_rate": 1.8921962992759456e-05,
      "loss": 1.1517,
      "step": 10720
    },
    {
      "epoch": 0.5395213193885761,
      "grad_norm": 10.0625,
      "learning_rate": 1.8920957361222848e-05,
      "loss": 0.7469,
      "step": 10730
    },
    {
      "epoch": 0.5400241351568785,
      "grad_norm": 12.9375,
      "learning_rate": 1.8919951729686247e-05,
      "loss": 0.8551,
      "step": 10740
    },
    {
      "epoch": 0.540526950925181,
      "grad_norm": 16.75,
      "learning_rate": 1.891894609814964e-05,
      "loss": 0.9784,
      "step": 10750
    },
    {
      "epoch": 0.5410297666934835,
      "grad_norm": 25.25,
      "learning_rate": 1.8917940466613036e-05,
      "loss": 0.6296,
      "step": 10760
    },
    {
      "epoch": 0.541532582461786,
      "grad_norm": 45.25,
      "learning_rate": 1.891693483507643e-05,
      "loss": 0.9257,
      "step": 10770
    },
    {
      "epoch": 0.5420353982300885,
      "grad_norm": 13.0625,
      "learning_rate": 1.8915929203539824e-05,
      "loss": 1.2737,
      "step": 10780
    },
    {
      "epoch": 0.542538213998391,
      "grad_norm": 60.75,
      "learning_rate": 1.891492357200322e-05,
      "loss": 1.1031,
      "step": 10790
    },
    {
      "epoch": 0.5430410297666934,
      "grad_norm": 21.0,
      "learning_rate": 1.8913917940466616e-05,
      "loss": 1.1281,
      "step": 10800
    },
    {
      "epoch": 0.543543845534996,
      "grad_norm": 88.5,
      "learning_rate": 1.8912912308930008e-05,
      "loss": 1.237,
      "step": 10810
    },
    {
      "epoch": 0.5440466613032985,
      "grad_norm": 51.0,
      "learning_rate": 1.8911906677393404e-05,
      "loss": 1.1597,
      "step": 10820
    },
    {
      "epoch": 0.544549477071601,
      "grad_norm": 6.875,
      "learning_rate": 1.89109010458568e-05,
      "loss": 0.9978,
      "step": 10830
    },
    {
      "epoch": 0.5450522928399034,
      "grad_norm": 42.5,
      "learning_rate": 1.8909895414320193e-05,
      "loss": 1.0994,
      "step": 10840
    },
    {
      "epoch": 0.545555108608206,
      "grad_norm": 5.71875,
      "learning_rate": 1.8908889782783592e-05,
      "loss": 0.7362,
      "step": 10850
    },
    {
      "epoch": 0.5460579243765085,
      "grad_norm": 19.5,
      "learning_rate": 1.8907884151246984e-05,
      "loss": 1.0743,
      "step": 10860
    },
    {
      "epoch": 0.5465607401448109,
      "grad_norm": 49.25,
      "learning_rate": 1.890687851971038e-05,
      "loss": 1.1288,
      "step": 10870
    },
    {
      "epoch": 0.5470635559131134,
      "grad_norm": 33.25,
      "learning_rate": 1.8905872888173776e-05,
      "loss": 1.2567,
      "step": 10880
    },
    {
      "epoch": 0.547566371681416,
      "grad_norm": 9.8125,
      "learning_rate": 1.890486725663717e-05,
      "loss": 0.8506,
      "step": 10890
    },
    {
      "epoch": 0.5480691874497184,
      "grad_norm": 19.0,
      "learning_rate": 1.8903861625100564e-05,
      "loss": 0.8912,
      "step": 10900
    },
    {
      "epoch": 0.5485720032180209,
      "grad_norm": 24.0,
      "learning_rate": 1.890285599356396e-05,
      "loss": 0.8131,
      "step": 10910
    },
    {
      "epoch": 0.5490748189863234,
      "grad_norm": 12.9375,
      "learning_rate": 1.8901850362027353e-05,
      "loss": 0.6741,
      "step": 10920
    },
    {
      "epoch": 0.549577634754626,
      "grad_norm": 62.75,
      "learning_rate": 1.8900844730490752e-05,
      "loss": 1.4142,
      "step": 10930
    },
    {
      "epoch": 0.5500804505229284,
      "grad_norm": 9.1875,
      "learning_rate": 1.8899839098954145e-05,
      "loss": 1.1467,
      "step": 10940
    },
    {
      "epoch": 0.5505832662912309,
      "grad_norm": 14.375,
      "learning_rate": 1.889883346741754e-05,
      "loss": 0.9077,
      "step": 10950
    },
    {
      "epoch": 0.5510860820595334,
      "grad_norm": 8.0,
      "learning_rate": 1.8897827835880936e-05,
      "loss": 0.8574,
      "step": 10960
    },
    {
      "epoch": 0.5515888978278359,
      "grad_norm": 30.25,
      "learning_rate": 1.889682220434433e-05,
      "loss": 0.7793,
      "step": 10970
    },
    {
      "epoch": 0.5520917135961384,
      "grad_norm": 36.0,
      "learning_rate": 1.8895816572807725e-05,
      "loss": 0.936,
      "step": 10980
    },
    {
      "epoch": 0.5525945293644409,
      "grad_norm": 6.6875,
      "learning_rate": 1.889481094127112e-05,
      "loss": 0.7789,
      "step": 10990
    },
    {
      "epoch": 0.5530973451327433,
      "grad_norm": 7.3125,
      "learning_rate": 1.8893805309734513e-05,
      "loss": 0.9668,
      "step": 11000
    },
    {
      "epoch": 0.5530973451327433,
      "eval_accuracy": 0.5116757560733763,
      "eval_loss": 1.0370365381240845,
      "eval_runtime": 464.6848,
      "eval_samples_per_second": 86.812,
      "eval_steps_per_second": 86.812,
      "step": 11000
    },
    {
      "epoch": 0.5536001609010458,
      "grad_norm": 15.8125,
      "learning_rate": 1.8892799678197912e-05,
      "loss": 0.8382,
      "step": 11010
    },
    {
      "epoch": 0.5541029766693484,
      "grad_norm": 41.5,
      "learning_rate": 1.8891794046661305e-05,
      "loss": 0.9494,
      "step": 11020
    },
    {
      "epoch": 0.5546057924376508,
      "grad_norm": 5.5625,
      "learning_rate": 1.8890788415124697e-05,
      "loss": 0.9633,
      "step": 11030
    },
    {
      "epoch": 0.5551086082059533,
      "grad_norm": 5.46875,
      "learning_rate": 1.8889782783588097e-05,
      "loss": 1.0201,
      "step": 11040
    },
    {
      "epoch": 0.5556114239742558,
      "grad_norm": 57.0,
      "learning_rate": 1.888877715205149e-05,
      "loss": 1.0862,
      "step": 11050
    },
    {
      "epoch": 0.5561142397425584,
      "grad_norm": 54.75,
      "learning_rate": 1.8887771520514885e-05,
      "loss": 1.0075,
      "step": 11060
    },
    {
      "epoch": 0.5566170555108608,
      "grad_norm": 32.25,
      "learning_rate": 1.888676588897828e-05,
      "loss": 1.0024,
      "step": 11070
    },
    {
      "epoch": 0.5571198712791633,
      "grad_norm": 58.25,
      "learning_rate": 1.8885760257441673e-05,
      "loss": 0.9603,
      "step": 11080
    },
    {
      "epoch": 0.5576226870474658,
      "grad_norm": 4.71875,
      "learning_rate": 1.888475462590507e-05,
      "loss": 1.2026,
      "step": 11090
    },
    {
      "epoch": 0.5581255028157683,
      "grad_norm": 11.5,
      "learning_rate": 1.8883748994368465e-05,
      "loss": 1.1603,
      "step": 11100
    },
    {
      "epoch": 0.5586283185840708,
      "grad_norm": 25.25,
      "learning_rate": 1.8882743362831858e-05,
      "loss": 0.9529,
      "step": 11110
    },
    {
      "epoch": 0.5591311343523733,
      "grad_norm": 24.0,
      "learning_rate": 1.8881737731295257e-05,
      "loss": 1.1152,
      "step": 11120
    },
    {
      "epoch": 0.5596339501206757,
      "grad_norm": 10.3125,
      "learning_rate": 1.888073209975865e-05,
      "loss": 0.7266,
      "step": 11130
    },
    {
      "epoch": 0.5601367658889783,
      "grad_norm": 22.5,
      "learning_rate": 1.8879726468222045e-05,
      "loss": 0.9547,
      "step": 11140
    },
    {
      "epoch": 0.5606395816572808,
      "grad_norm": 35.75,
      "learning_rate": 1.887872083668544e-05,
      "loss": 0.9877,
      "step": 11150
    },
    {
      "epoch": 0.5611423974255833,
      "grad_norm": 15.125,
      "learning_rate": 1.8877715205148834e-05,
      "loss": 1.116,
      "step": 11160
    },
    {
      "epoch": 0.5616452131938857,
      "grad_norm": 61.25,
      "learning_rate": 1.887670957361223e-05,
      "loss": 1.3199,
      "step": 11170
    },
    {
      "epoch": 0.5621480289621883,
      "grad_norm": 17.125,
      "learning_rate": 1.8875703942075625e-05,
      "loss": 1.2451,
      "step": 11180
    },
    {
      "epoch": 0.5626508447304908,
      "grad_norm": 32.0,
      "learning_rate": 1.8874698310539018e-05,
      "loss": 1.0004,
      "step": 11190
    },
    {
      "epoch": 0.5631536604987932,
      "grad_norm": 13.5625,
      "learning_rate": 1.8873692679002417e-05,
      "loss": 0.9459,
      "step": 11200
    },
    {
      "epoch": 0.5636564762670957,
      "grad_norm": 28.5,
      "learning_rate": 1.887268704746581e-05,
      "loss": 1.4365,
      "step": 11210
    },
    {
      "epoch": 0.5641592920353983,
      "grad_norm": 19.875,
      "learning_rate": 1.8871681415929205e-05,
      "loss": 0.9857,
      "step": 11220
    },
    {
      "epoch": 0.5646621078037007,
      "grad_norm": 8.0,
      "learning_rate": 1.88706757843926e-05,
      "loss": 1.1586,
      "step": 11230
    },
    {
      "epoch": 0.5651649235720032,
      "grad_norm": 15.5,
      "learning_rate": 1.8869670152855994e-05,
      "loss": 1.0178,
      "step": 11240
    },
    {
      "epoch": 0.5656677393403057,
      "grad_norm": 34.75,
      "learning_rate": 1.886866452131939e-05,
      "loss": 0.9332,
      "step": 11250
    },
    {
      "epoch": 0.5661705551086083,
      "grad_norm": 30.25,
      "learning_rate": 1.8867658889782786e-05,
      "loss": 0.961,
      "step": 11260
    },
    {
      "epoch": 0.5666733708769107,
      "grad_norm": 18.75,
      "learning_rate": 1.8866653258246178e-05,
      "loss": 1.2204,
      "step": 11270
    },
    {
      "epoch": 0.5671761866452132,
      "grad_norm": 13.5,
      "learning_rate": 1.8865647626709574e-05,
      "loss": 1.2005,
      "step": 11280
    },
    {
      "epoch": 0.5676790024135157,
      "grad_norm": 14.0,
      "learning_rate": 1.886464199517297e-05,
      "loss": 0.7524,
      "step": 11290
    },
    {
      "epoch": 0.5681818181818182,
      "grad_norm": 6.90625,
      "learning_rate": 1.8863636363636366e-05,
      "loss": 0.8488,
      "step": 11300
    },
    {
      "epoch": 0.5686846339501207,
      "grad_norm": 39.5,
      "learning_rate": 1.886263073209976e-05,
      "loss": 1.0165,
      "step": 11310
    },
    {
      "epoch": 0.5691874497184232,
      "grad_norm": 9.8125,
      "learning_rate": 1.8861625100563154e-05,
      "loss": 0.7513,
      "step": 11320
    },
    {
      "epoch": 0.5696902654867256,
      "grad_norm": 24.0,
      "learning_rate": 1.886061946902655e-05,
      "loss": 0.7737,
      "step": 11330
    },
    {
      "epoch": 0.5701930812550282,
      "grad_norm": 15.1875,
      "learning_rate": 1.8859613837489946e-05,
      "loss": 1.0234,
      "step": 11340
    },
    {
      "epoch": 0.5706958970233307,
      "grad_norm": 41.75,
      "learning_rate": 1.8858608205953338e-05,
      "loss": 0.9288,
      "step": 11350
    },
    {
      "epoch": 0.5711987127916331,
      "grad_norm": 25.375,
      "learning_rate": 1.8857602574416734e-05,
      "loss": 1.0888,
      "step": 11360
    },
    {
      "epoch": 0.5717015285599356,
      "grad_norm": 14.75,
      "learning_rate": 1.885659694288013e-05,
      "loss": 0.9278,
      "step": 11370
    },
    {
      "epoch": 0.5722043443282381,
      "grad_norm": 36.25,
      "learning_rate": 1.8855591311343526e-05,
      "loss": 1.1609,
      "step": 11380
    },
    {
      "epoch": 0.5727071600965407,
      "grad_norm": 19.0,
      "learning_rate": 1.8854585679806922e-05,
      "loss": 1.3852,
      "step": 11390
    },
    {
      "epoch": 0.5732099758648431,
      "grad_norm": 6.65625,
      "learning_rate": 1.8853580048270314e-05,
      "loss": 0.68,
      "step": 11400
    },
    {
      "epoch": 0.5737127916331456,
      "grad_norm": 46.75,
      "learning_rate": 1.885257441673371e-05,
      "loss": 1.2676,
      "step": 11410
    },
    {
      "epoch": 0.5742156074014481,
      "grad_norm": 16.625,
      "learning_rate": 1.8851568785197106e-05,
      "loss": 0.8167,
      "step": 11420
    },
    {
      "epoch": 0.5747184231697506,
      "grad_norm": 30.0,
      "learning_rate": 1.88505631536605e-05,
      "loss": 0.8557,
      "step": 11430
    },
    {
      "epoch": 0.5752212389380531,
      "grad_norm": 43.75,
      "learning_rate": 1.8849557522123894e-05,
      "loss": 0.9238,
      "step": 11440
    },
    {
      "epoch": 0.5757240547063556,
      "grad_norm": 21.625,
      "learning_rate": 1.884855189058729e-05,
      "loss": 1.3541,
      "step": 11450
    },
    {
      "epoch": 0.576226870474658,
      "grad_norm": 42.0,
      "learning_rate": 1.8847546259050686e-05,
      "loss": 1.3038,
      "step": 11460
    },
    {
      "epoch": 0.5767296862429606,
      "grad_norm": 35.25,
      "learning_rate": 1.8846540627514082e-05,
      "loss": 0.7772,
      "step": 11470
    },
    {
      "epoch": 0.5772325020112631,
      "grad_norm": 6.0625,
      "learning_rate": 1.8845534995977475e-05,
      "loss": 0.9913,
      "step": 11480
    },
    {
      "epoch": 0.5777353177795655,
      "grad_norm": 51.75,
      "learning_rate": 1.884452936444087e-05,
      "loss": 1.3605,
      "step": 11490
    },
    {
      "epoch": 0.578238133547868,
      "grad_norm": 14.25,
      "learning_rate": 1.8843523732904266e-05,
      "loss": 1.1211,
      "step": 11500
    },
    {
      "epoch": 0.578238133547868,
      "eval_accuracy": 0.5115270203272186,
      "eval_loss": 1.036556601524353,
      "eval_runtime": 465.9401,
      "eval_samples_per_second": 86.578,
      "eval_steps_per_second": 86.578,
      "step": 11500
    },
    {
      "epoch": 0.5787409493161706,
      "grad_norm": 9.25,
      "learning_rate": 1.884251810136766e-05,
      "loss": 1.1897,
      "step": 11510
    },
    {
      "epoch": 0.5792437650844731,
      "grad_norm": 6.03125,
      "learning_rate": 1.8841512469831055e-05,
      "loss": 0.9827,
      "step": 11520
    },
    {
      "epoch": 0.5797465808527755,
      "grad_norm": 6.59375,
      "learning_rate": 1.884050683829445e-05,
      "loss": 0.8371,
      "step": 11530
    },
    {
      "epoch": 0.580249396621078,
      "grad_norm": 29.75,
      "learning_rate": 1.8839501206757846e-05,
      "loss": 1.1436,
      "step": 11540
    },
    {
      "epoch": 0.5807522123893806,
      "grad_norm": 24.5,
      "learning_rate": 1.883849557522124e-05,
      "loss": 0.9333,
      "step": 11550
    },
    {
      "epoch": 0.581255028157683,
      "grad_norm": 17.0,
      "learning_rate": 1.8837489943684635e-05,
      "loss": 1.0105,
      "step": 11560
    },
    {
      "epoch": 0.5817578439259855,
      "grad_norm": 13.3125,
      "learning_rate": 1.883648431214803e-05,
      "loss": 1.0159,
      "step": 11570
    },
    {
      "epoch": 0.582260659694288,
      "grad_norm": 52.5,
      "learning_rate": 1.8835478680611427e-05,
      "loss": 1.0718,
      "step": 11580
    },
    {
      "epoch": 0.5827634754625906,
      "grad_norm": 30.125,
      "learning_rate": 1.883447304907482e-05,
      "loss": 0.8349,
      "step": 11590
    },
    {
      "epoch": 0.583266291230893,
      "grad_norm": 11.0625,
      "learning_rate": 1.8833467417538215e-05,
      "loss": 0.8935,
      "step": 11600
    },
    {
      "epoch": 0.5837691069991955,
      "grad_norm": 25.875,
      "learning_rate": 1.883246178600161e-05,
      "loss": 1.0757,
      "step": 11610
    },
    {
      "epoch": 0.584271922767498,
      "grad_norm": 13.4375,
      "learning_rate": 1.8831456154465007e-05,
      "loss": 0.7796,
      "step": 11620
    },
    {
      "epoch": 0.5847747385358005,
      "grad_norm": 20.875,
      "learning_rate": 1.88304505229284e-05,
      "loss": 0.935,
      "step": 11630
    },
    {
      "epoch": 0.585277554304103,
      "grad_norm": 43.5,
      "learning_rate": 1.8829444891391795e-05,
      "loss": 0.9787,
      "step": 11640
    },
    {
      "epoch": 0.5857803700724055,
      "grad_norm": 12.125,
      "learning_rate": 1.882843925985519e-05,
      "loss": 0.8575,
      "step": 11650
    },
    {
      "epoch": 0.5862831858407079,
      "grad_norm": 17.375,
      "learning_rate": 1.8827433628318587e-05,
      "loss": 0.7759,
      "step": 11660
    },
    {
      "epoch": 0.5867860016090105,
      "grad_norm": 39.5,
      "learning_rate": 1.882642799678198e-05,
      "loss": 1.0498,
      "step": 11670
    },
    {
      "epoch": 0.587288817377313,
      "grad_norm": 21.0,
      "learning_rate": 1.8825422365245375e-05,
      "loss": 1.0365,
      "step": 11680
    },
    {
      "epoch": 0.5877916331456154,
      "grad_norm": 14.5,
      "learning_rate": 1.882441673370877e-05,
      "loss": 0.995,
      "step": 11690
    },
    {
      "epoch": 0.5882944489139179,
      "grad_norm": 97.0,
      "learning_rate": 1.8823411102172167e-05,
      "loss": 1.3053,
      "step": 11700
    },
    {
      "epoch": 0.5887972646822205,
      "grad_norm": 15.75,
      "learning_rate": 1.882240547063556e-05,
      "loss": 0.7906,
      "step": 11710
    },
    {
      "epoch": 0.589300080450523,
      "grad_norm": 37.75,
      "learning_rate": 1.8821399839098955e-05,
      "loss": 1.1526,
      "step": 11720
    },
    {
      "epoch": 0.5898028962188254,
      "grad_norm": 25.625,
      "learning_rate": 1.882039420756235e-05,
      "loss": 0.8825,
      "step": 11730
    },
    {
      "epoch": 0.5903057119871279,
      "grad_norm": 15.5,
      "learning_rate": 1.8819388576025747e-05,
      "loss": 1.1241,
      "step": 11740
    },
    {
      "epoch": 0.5908085277554304,
      "grad_norm": 17.375,
      "learning_rate": 1.881838294448914e-05,
      "loss": 1.0366,
      "step": 11750
    },
    {
      "epoch": 0.5913113435237329,
      "grad_norm": 29.5,
      "learning_rate": 1.8817377312952535e-05,
      "loss": 1.1958,
      "step": 11760
    },
    {
      "epoch": 0.5918141592920354,
      "grad_norm": 19.125,
      "learning_rate": 1.881637168141593e-05,
      "loss": 1.2505,
      "step": 11770
    },
    {
      "epoch": 0.5923169750603379,
      "grad_norm": 31.75,
      "learning_rate": 1.8815366049879327e-05,
      "loss": 0.9719,
      "step": 11780
    },
    {
      "epoch": 0.5928197908286403,
      "grad_norm": 9.3125,
      "learning_rate": 1.881436041834272e-05,
      "loss": 0.9268,
      "step": 11790
    },
    {
      "epoch": 0.5933226065969429,
      "grad_norm": 8.5,
      "learning_rate": 1.8813354786806116e-05,
      "loss": 0.8984,
      "step": 11800
    },
    {
      "epoch": 0.5938254223652454,
      "grad_norm": 17.875,
      "learning_rate": 1.881234915526951e-05,
      "loss": 1.0292,
      "step": 11810
    },
    {
      "epoch": 0.5943282381335478,
      "grad_norm": 13.1875,
      "learning_rate": 1.8811343523732904e-05,
      "loss": 1.1356,
      "step": 11820
    },
    {
      "epoch": 0.5948310539018503,
      "grad_norm": 25.875,
      "learning_rate": 1.88103378921963e-05,
      "loss": 1.0693,
      "step": 11830
    },
    {
      "epoch": 0.5953338696701529,
      "grad_norm": 19.0,
      "learning_rate": 1.8809332260659696e-05,
      "loss": 1.1038,
      "step": 11840
    },
    {
      "epoch": 0.5958366854384554,
      "grad_norm": 31.5,
      "learning_rate": 1.880832662912309e-05,
      "loss": 0.7758,
      "step": 11850
    },
    {
      "epoch": 0.5963395012067578,
      "grad_norm": 15.125,
      "learning_rate": 1.8807320997586487e-05,
      "loss": 1.2754,
      "step": 11860
    },
    {
      "epoch": 0.5968423169750603,
      "grad_norm": 26.25,
      "learning_rate": 1.880631536604988e-05,
      "loss": 1.1808,
      "step": 11870
    },
    {
      "epoch": 0.5973451327433629,
      "grad_norm": 15.1875,
      "learning_rate": 1.8805309734513276e-05,
      "loss": 1.0024,
      "step": 11880
    },
    {
      "epoch": 0.5978479485116653,
      "grad_norm": 52.5,
      "learning_rate": 1.8804304102976672e-05,
      "loss": 0.8926,
      "step": 11890
    },
    {
      "epoch": 0.5983507642799678,
      "grad_norm": 13.75,
      "learning_rate": 1.8803298471440064e-05,
      "loss": 1.1999,
      "step": 11900
    },
    {
      "epoch": 0.5988535800482703,
      "grad_norm": 18.875,
      "learning_rate": 1.880229283990346e-05,
      "loss": 0.8841,
      "step": 11910
    },
    {
      "epoch": 0.5993563958165729,
      "grad_norm": 46.25,
      "learning_rate": 1.8801287208366856e-05,
      "loss": 1.0838,
      "step": 11920
    },
    {
      "epoch": 0.5998592115848753,
      "grad_norm": 15.8125,
      "learning_rate": 1.8800281576830252e-05,
      "loss": 0.9178,
      "step": 11930
    },
    {
      "epoch": 0.6003620273531778,
      "grad_norm": 44.75,
      "learning_rate": 1.8799275945293648e-05,
      "loss": 1.2286,
      "step": 11940
    },
    {
      "epoch": 0.6008648431214803,
      "grad_norm": 3.921875,
      "learning_rate": 1.879827031375704e-05,
      "loss": 0.9036,
      "step": 11950
    },
    {
      "epoch": 0.6013676588897828,
      "grad_norm": 3.953125,
      "learning_rate": 1.8797264682220436e-05,
      "loss": 1.0707,
      "step": 11960
    },
    {
      "epoch": 0.6018704746580853,
      "grad_norm": 12.75,
      "learning_rate": 1.8796259050683832e-05,
      "loss": 1.1303,
      "step": 11970
    },
    {
      "epoch": 0.6023732904263878,
      "grad_norm": 41.5,
      "learning_rate": 1.8795253419147224e-05,
      "loss": 0.815,
      "step": 11980
    },
    {
      "epoch": 0.6028761061946902,
      "grad_norm": 30.0,
      "learning_rate": 1.879424778761062e-05,
      "loss": 1.1546,
      "step": 11990
    },
    {
      "epoch": 0.6033789219629928,
      "grad_norm": 62.75,
      "learning_rate": 1.8793242156074016e-05,
      "loss": 0.9474,
      "step": 12000
    },
    {
      "epoch": 0.6033789219629928,
      "eval_accuracy": 0.5115518096182449,
      "eval_loss": 1.0344663858413696,
      "eval_runtime": 464.9744,
      "eval_samples_per_second": 86.757,
      "eval_steps_per_second": 86.757,
      "step": 12000
    },
    {
      "epoch": 0.6038817377312953,
      "grad_norm": 16.375,
      "learning_rate": 1.8792236524537412e-05,
      "loss": 0.9432,
      "step": 12010
    },
    {
      "epoch": 0.6043845534995977,
      "grad_norm": 8.625,
      "learning_rate": 1.8791230893000808e-05,
      "loss": 0.7113,
      "step": 12020
    },
    {
      "epoch": 0.6048873692679002,
      "grad_norm": 8.75,
      "learning_rate": 1.87902252614642e-05,
      "loss": 0.9072,
      "step": 12030
    },
    {
      "epoch": 0.6053901850362028,
      "grad_norm": 7.0,
      "learning_rate": 1.8789219629927596e-05,
      "loss": 0.7794,
      "step": 12040
    },
    {
      "epoch": 0.6058930008045053,
      "grad_norm": 41.0,
      "learning_rate": 1.8788213998390992e-05,
      "loss": 1.088,
      "step": 12050
    },
    {
      "epoch": 0.6063958165728077,
      "grad_norm": 13.0625,
      "learning_rate": 1.8787208366854385e-05,
      "loss": 1.1198,
      "step": 12060
    },
    {
      "epoch": 0.6068986323411102,
      "grad_norm": 29.25,
      "learning_rate": 1.878620273531778e-05,
      "loss": 1.0594,
      "step": 12070
    },
    {
      "epoch": 0.6074014481094127,
      "grad_norm": 9.6875,
      "learning_rate": 1.8785197103781176e-05,
      "loss": 1.0723,
      "step": 12080
    },
    {
      "epoch": 0.6079042638777152,
      "grad_norm": 32.25,
      "learning_rate": 1.878419147224457e-05,
      "loss": 1.0384,
      "step": 12090
    },
    {
      "epoch": 0.6084070796460177,
      "grad_norm": 5.09375,
      "learning_rate": 1.8783185840707968e-05,
      "loss": 0.9803,
      "step": 12100
    },
    {
      "epoch": 0.6089098954143202,
      "grad_norm": 18.625,
      "learning_rate": 1.878218020917136e-05,
      "loss": 1.1226,
      "step": 12110
    },
    {
      "epoch": 0.6094127111826226,
      "grad_norm": 11.375,
      "learning_rate": 1.8781174577634757e-05,
      "loss": 0.6218,
      "step": 12120
    },
    {
      "epoch": 0.6099155269509252,
      "grad_norm": 35.5,
      "learning_rate": 1.8780168946098152e-05,
      "loss": 0.934,
      "step": 12130
    },
    {
      "epoch": 0.6104183427192277,
      "grad_norm": 31.375,
      "learning_rate": 1.8779163314561545e-05,
      "loss": 1.2066,
      "step": 12140
    },
    {
      "epoch": 0.6109211584875301,
      "grad_norm": 19.0,
      "learning_rate": 1.877815768302494e-05,
      "loss": 1.0526,
      "step": 12150
    },
    {
      "epoch": 0.6114239742558326,
      "grad_norm": 10.8125,
      "learning_rate": 1.8777152051488337e-05,
      "loss": 0.8265,
      "step": 12160
    },
    {
      "epoch": 0.6119267900241352,
      "grad_norm": 60.25,
      "learning_rate": 1.877614641995173e-05,
      "loss": 1.4159,
      "step": 12170
    },
    {
      "epoch": 0.6124296057924377,
      "grad_norm": 42.5,
      "learning_rate": 1.877514078841513e-05,
      "loss": 0.9598,
      "step": 12180
    },
    {
      "epoch": 0.6129324215607401,
      "grad_norm": 31.0,
      "learning_rate": 1.877413515687852e-05,
      "loss": 0.9494,
      "step": 12190
    },
    {
      "epoch": 0.6134352373290426,
      "grad_norm": 21.875,
      "learning_rate": 1.8773129525341917e-05,
      "loss": 1.0357,
      "step": 12200
    },
    {
      "epoch": 0.6139380530973452,
      "grad_norm": 15.25,
      "learning_rate": 1.8772123893805313e-05,
      "loss": 0.728,
      "step": 12210
    },
    {
      "epoch": 0.6144408688656476,
      "grad_norm": 7.1875,
      "learning_rate": 1.8771118262268705e-05,
      "loss": 1.0115,
      "step": 12220
    },
    {
      "epoch": 0.6149436846339501,
      "grad_norm": 44.5,
      "learning_rate": 1.87701126307321e-05,
      "loss": 1.0542,
      "step": 12230
    },
    {
      "epoch": 0.6154465004022526,
      "grad_norm": 45.25,
      "learning_rate": 1.8769106999195497e-05,
      "loss": 1.0544,
      "step": 12240
    },
    {
      "epoch": 0.6159493161705552,
      "grad_norm": 12.0,
      "learning_rate": 1.876810136765889e-05,
      "loss": 1.2804,
      "step": 12250
    },
    {
      "epoch": 0.6164521319388576,
      "grad_norm": 7.28125,
      "learning_rate": 1.876709573612229e-05,
      "loss": 1.0787,
      "step": 12260
    },
    {
      "epoch": 0.6169549477071601,
      "grad_norm": 22.25,
      "learning_rate": 1.876609010458568e-05,
      "loss": 0.9921,
      "step": 12270
    },
    {
      "epoch": 0.6174577634754626,
      "grad_norm": 17.25,
      "learning_rate": 1.8765084473049077e-05,
      "loss": 0.9897,
      "step": 12280
    },
    {
      "epoch": 0.6179605792437651,
      "grad_norm": 24.25,
      "learning_rate": 1.8764078841512473e-05,
      "loss": 1.0232,
      "step": 12290
    },
    {
      "epoch": 0.6184633950120676,
      "grad_norm": 30.75,
      "learning_rate": 1.8763073209975865e-05,
      "loss": 0.999,
      "step": 12300
    },
    {
      "epoch": 0.6189662107803701,
      "grad_norm": 32.75,
      "learning_rate": 1.876206757843926e-05,
      "loss": 1.2868,
      "step": 12310
    },
    {
      "epoch": 0.6194690265486725,
      "grad_norm": 11.8125,
      "learning_rate": 1.8761061946902657e-05,
      "loss": 0.7433,
      "step": 12320
    },
    {
      "epoch": 0.6199718423169751,
      "grad_norm": 19.5,
      "learning_rate": 1.876005631536605e-05,
      "loss": 0.8507,
      "step": 12330
    },
    {
      "epoch": 0.6204746580852776,
      "grad_norm": 24.125,
      "learning_rate": 1.8759050683829446e-05,
      "loss": 1.0319,
      "step": 12340
    },
    {
      "epoch": 0.62097747385358,
      "grad_norm": 26.125,
      "learning_rate": 1.875804505229284e-05,
      "loss": 1.0654,
      "step": 12350
    },
    {
      "epoch": 0.6214802896218825,
      "grad_norm": 9.625,
      "learning_rate": 1.8757039420756234e-05,
      "loss": 0.8278,
      "step": 12360
    },
    {
      "epoch": 0.6219831053901851,
      "grad_norm": 18.5,
      "learning_rate": 1.8756033789219633e-05,
      "loss": 0.9691,
      "step": 12370
    },
    {
      "epoch": 0.6224859211584876,
      "grad_norm": 26.625,
      "learning_rate": 1.8755028157683026e-05,
      "loss": 1.111,
      "step": 12380
    },
    {
      "epoch": 0.62298873692679,
      "grad_norm": 19.5,
      "learning_rate": 1.875402252614642e-05,
      "loss": 0.8015,
      "step": 12390
    },
    {
      "epoch": 0.6234915526950925,
      "grad_norm": 86.0,
      "learning_rate": 1.8753016894609817e-05,
      "loss": 1.2869,
      "step": 12400
    },
    {
      "epoch": 0.6239943684633951,
      "grad_norm": 38.25,
      "learning_rate": 1.875201126307321e-05,
      "loss": 1.0032,
      "step": 12410
    },
    {
      "epoch": 0.6244971842316975,
      "grad_norm": 56.25,
      "learning_rate": 1.8751005631536606e-05,
      "loss": 1.1519,
      "step": 12420
    },
    {
      "epoch": 0.625,
      "grad_norm": 17.625,
      "learning_rate": 1.8750000000000002e-05,
      "loss": 1.2147,
      "step": 12430
    },
    {
      "epoch": 0.6255028157683025,
      "grad_norm": 65.0,
      "learning_rate": 1.8748994368463394e-05,
      "loss": 1.0602,
      "step": 12440
    },
    {
      "epoch": 0.6260056315366049,
      "grad_norm": 21.5,
      "learning_rate": 1.8747988736926794e-05,
      "loss": 1.0031,
      "step": 12450
    },
    {
      "epoch": 0.6265084473049075,
      "grad_norm": 41.25,
      "learning_rate": 1.8746983105390186e-05,
      "loss": 1.2617,
      "step": 12460
    },
    {
      "epoch": 0.62701126307321,
      "grad_norm": 30.875,
      "learning_rate": 1.8745977473853582e-05,
      "loss": 1.0568,
      "step": 12470
    },
    {
      "epoch": 0.6275140788415124,
      "grad_norm": 52.25,
      "learning_rate": 1.8744971842316978e-05,
      "loss": 1.2168,
      "step": 12480
    },
    {
      "epoch": 0.6280168946098149,
      "grad_norm": 6.875,
      "learning_rate": 1.874396621078037e-05,
      "loss": 0.9234,
      "step": 12490
    },
    {
      "epoch": 0.6285197103781175,
      "grad_norm": 5.3125,
      "learning_rate": 1.8742960579243766e-05,
      "loss": 0.6878,
      "step": 12500
    },
    {
      "epoch": 0.6285197103781175,
      "eval_accuracy": 0.5118740704015865,
      "eval_loss": 1.033707618713379,
      "eval_runtime": 462.8188,
      "eval_samples_per_second": 87.162,
      "eval_steps_per_second": 87.162,
      "step": 12500
    },
    {
      "epoch": 0.62902252614642,
      "grad_norm": 26.0,
      "learning_rate": 1.8741954947707162e-05,
      "loss": 1.2172,
      "step": 12510
    },
    {
      "epoch": 0.6295253419147224,
      "grad_norm": 19.5,
      "learning_rate": 1.8740949316170554e-05,
      "loss": 0.9748,
      "step": 12520
    },
    {
      "epoch": 0.6300281576830249,
      "grad_norm": 8.875,
      "learning_rate": 1.8739943684633954e-05,
      "loss": 0.826,
      "step": 12530
    },
    {
      "epoch": 0.6305309734513275,
      "grad_norm": 7.78125,
      "learning_rate": 1.8738938053097346e-05,
      "loss": 0.8234,
      "step": 12540
    },
    {
      "epoch": 0.6310337892196299,
      "grad_norm": 12.8125,
      "learning_rate": 1.8737932421560742e-05,
      "loss": 0.9227,
      "step": 12550
    },
    {
      "epoch": 0.6315366049879324,
      "grad_norm": 17.0,
      "learning_rate": 1.8736926790024138e-05,
      "loss": 1.1403,
      "step": 12560
    },
    {
      "epoch": 0.6320394207562349,
      "grad_norm": 19.125,
      "learning_rate": 1.873592115848753e-05,
      "loss": 0.8703,
      "step": 12570
    },
    {
      "epoch": 0.6325422365245374,
      "grad_norm": 50.5,
      "learning_rate": 1.8734915526950926e-05,
      "loss": 1.0712,
      "step": 12580
    },
    {
      "epoch": 0.6330450522928399,
      "grad_norm": 15.5625,
      "learning_rate": 1.8733909895414322e-05,
      "loss": 1.3061,
      "step": 12590
    },
    {
      "epoch": 0.6335478680611424,
      "grad_norm": 7.1875,
      "learning_rate": 1.8732904263877715e-05,
      "loss": 1.1146,
      "step": 12600
    },
    {
      "epoch": 0.6340506838294448,
      "grad_norm": 7.65625,
      "learning_rate": 1.873189863234111e-05,
      "loss": 1.1465,
      "step": 12610
    },
    {
      "epoch": 0.6345534995977474,
      "grad_norm": 6.65625,
      "learning_rate": 1.8730893000804507e-05,
      "loss": 0.9772,
      "step": 12620
    },
    {
      "epoch": 0.6350563153660499,
      "grad_norm": 19.0,
      "learning_rate": 1.8729887369267902e-05,
      "loss": 0.9507,
      "step": 12630
    },
    {
      "epoch": 0.6355591311343524,
      "grad_norm": 63.75,
      "learning_rate": 1.8728881737731298e-05,
      "loss": 1.1855,
      "step": 12640
    },
    {
      "epoch": 0.6360619469026548,
      "grad_norm": 32.75,
      "learning_rate": 1.872787610619469e-05,
      "loss": 0.9497,
      "step": 12650
    },
    {
      "epoch": 0.6365647626709574,
      "grad_norm": 7.9375,
      "learning_rate": 1.8726870474658087e-05,
      "loss": 0.9029,
      "step": 12660
    },
    {
      "epoch": 0.6370675784392599,
      "grad_norm": 37.75,
      "learning_rate": 1.8725864843121483e-05,
      "loss": 1.1439,
      "step": 12670
    },
    {
      "epoch": 0.6375703942075623,
      "grad_norm": 20.5,
      "learning_rate": 1.8724859211584875e-05,
      "loss": 0.8592,
      "step": 12680
    },
    {
      "epoch": 0.6380732099758648,
      "grad_norm": 17.125,
      "learning_rate": 1.872385358004827e-05,
      "loss": 0.7343,
      "step": 12690
    },
    {
      "epoch": 0.6385760257441674,
      "grad_norm": 5.25,
      "learning_rate": 1.8722847948511667e-05,
      "loss": 1.2031,
      "step": 12700
    },
    {
      "epoch": 0.6390788415124699,
      "grad_norm": 56.5,
      "learning_rate": 1.8721842316975063e-05,
      "loss": 1.0962,
      "step": 12710
    },
    {
      "epoch": 0.6395816572807723,
      "grad_norm": 15.4375,
      "learning_rate": 1.872083668543846e-05,
      "loss": 0.843,
      "step": 12720
    },
    {
      "epoch": 0.6400844730490748,
      "grad_norm": 16.0,
      "learning_rate": 1.871983105390185e-05,
      "loss": 0.8351,
      "step": 12730
    },
    {
      "epoch": 0.6405872888173774,
      "grad_norm": 17.125,
      "learning_rate": 1.8718825422365247e-05,
      "loss": 1.245,
      "step": 12740
    },
    {
      "epoch": 0.6410901045856798,
      "grad_norm": 43.5,
      "learning_rate": 1.8717819790828643e-05,
      "loss": 1.4917,
      "step": 12750
    },
    {
      "epoch": 0.6415929203539823,
      "grad_norm": 38.0,
      "learning_rate": 1.8716814159292035e-05,
      "loss": 1.0593,
      "step": 12760
    },
    {
      "epoch": 0.6420957361222848,
      "grad_norm": 41.0,
      "learning_rate": 1.871580852775543e-05,
      "loss": 1.1859,
      "step": 12770
    },
    {
      "epoch": 0.6425985518905873,
      "grad_norm": 15.75,
      "learning_rate": 1.8714802896218827e-05,
      "loss": 1.4027,
      "step": 12780
    },
    {
      "epoch": 0.6431013676588898,
      "grad_norm": 34.5,
      "learning_rate": 1.8713797264682223e-05,
      "loss": 1.0395,
      "step": 12790
    },
    {
      "epoch": 0.6436041834271923,
      "grad_norm": 4.59375,
      "learning_rate": 1.871279163314562e-05,
      "loss": 1.0036,
      "step": 12800
    },
    {
      "epoch": 0.6441069991954947,
      "grad_norm": 8.8125,
      "learning_rate": 1.871178600160901e-05,
      "loss": 1.1277,
      "step": 12810
    },
    {
      "epoch": 0.6446098149637972,
      "grad_norm": 18.0,
      "learning_rate": 1.8710780370072407e-05,
      "loss": 1.2169,
      "step": 12820
    },
    {
      "epoch": 0.6451126307320998,
      "grad_norm": 11.375,
      "learning_rate": 1.8709774738535803e-05,
      "loss": 1.0882,
      "step": 12830
    },
    {
      "epoch": 0.6456154465004023,
      "grad_norm": 34.75,
      "learning_rate": 1.8708769106999196e-05,
      "loss": 1.0605,
      "step": 12840
    },
    {
      "epoch": 0.6461182622687047,
      "grad_norm": 8.9375,
      "learning_rate": 1.870776347546259e-05,
      "loss": 1.3268,
      "step": 12850
    },
    {
      "epoch": 0.6466210780370072,
      "grad_norm": 38.25,
      "learning_rate": 1.8706757843925987e-05,
      "loss": 0.823,
      "step": 12860
    },
    {
      "epoch": 0.6471238938053098,
      "grad_norm": 31.125,
      "learning_rate": 1.8705752212389383e-05,
      "loss": 1.1289,
      "step": 12870
    },
    {
      "epoch": 0.6476267095736122,
      "grad_norm": 23.375,
      "learning_rate": 1.8704746580852776e-05,
      "loss": 1.1454,
      "step": 12880
    },
    {
      "epoch": 0.6481295253419147,
      "grad_norm": 12.1875,
      "learning_rate": 1.870374094931617e-05,
      "loss": 0.7924,
      "step": 12890
    },
    {
      "epoch": 0.6486323411102172,
      "grad_norm": 20.5,
      "learning_rate": 1.8702735317779567e-05,
      "loss": 1.272,
      "step": 12900
    },
    {
      "epoch": 0.6491351568785197,
      "grad_norm": 19.25,
      "learning_rate": 1.8701729686242963e-05,
      "loss": 0.981,
      "step": 12910
    },
    {
      "epoch": 0.6496379726468222,
      "grad_norm": 45.0,
      "learning_rate": 1.8700724054706356e-05,
      "loss": 1.0984,
      "step": 12920
    },
    {
      "epoch": 0.6501407884151247,
      "grad_norm": 34.25,
      "learning_rate": 1.869971842316975e-05,
      "loss": 0.986,
      "step": 12930
    },
    {
      "epoch": 0.6506436041834271,
      "grad_norm": 17.375,
      "learning_rate": 1.8698712791633148e-05,
      "loss": 0.7826,
      "step": 12940
    },
    {
      "epoch": 0.6511464199517297,
      "grad_norm": 64.0,
      "learning_rate": 1.8697707160096543e-05,
      "loss": 0.9625,
      "step": 12950
    },
    {
      "epoch": 0.6516492357200322,
      "grad_norm": 19.625,
      "learning_rate": 1.8696701528559936e-05,
      "loss": 1.0627,
      "step": 12960
    },
    {
      "epoch": 0.6521520514883347,
      "grad_norm": 4.71875,
      "learning_rate": 1.8695695897023332e-05,
      "loss": 0.9697,
      "step": 12970
    },
    {
      "epoch": 0.6526548672566371,
      "grad_norm": 7.09375,
      "learning_rate": 1.8694690265486728e-05,
      "loss": 0.7422,
      "step": 12980
    },
    {
      "epoch": 0.6531576830249397,
      "grad_norm": 11.75,
      "learning_rate": 1.8693684633950124e-05,
      "loss": 0.9628,
      "step": 12990
    },
    {
      "epoch": 0.6536604987932422,
      "grad_norm": 9.125,
      "learning_rate": 1.8692679002413516e-05,
      "loss": 0.9721,
      "step": 13000
    },
    {
      "epoch": 0.6536604987932422,
      "eval_accuracy": 0.5121963311849281,
      "eval_loss": 1.031802773475647,
      "eval_runtime": 462.7587,
      "eval_samples_per_second": 87.173,
      "eval_steps_per_second": 87.173,
      "step": 13000
    },
    {
      "epoch": 0.6541633145615446,
      "grad_norm": 22.625,
      "learning_rate": 1.8691673370876912e-05,
      "loss": 0.8199,
      "step": 13010
    },
    {
      "epoch": 0.6546661303298471,
      "grad_norm": 94.0,
      "learning_rate": 1.8690667739340308e-05,
      "loss": 1.2703,
      "step": 13020
    },
    {
      "epoch": 0.6551689460981497,
      "grad_norm": 16.25,
      "learning_rate": 1.8689662107803704e-05,
      "loss": 0.7051,
      "step": 13030
    },
    {
      "epoch": 0.6556717618664522,
      "grad_norm": 31.0,
      "learning_rate": 1.8688656476267096e-05,
      "loss": 0.9251,
      "step": 13040
    },
    {
      "epoch": 0.6561745776347546,
      "grad_norm": 27.625,
      "learning_rate": 1.8687650844730492e-05,
      "loss": 0.7904,
      "step": 13050
    },
    {
      "epoch": 0.6566773934030571,
      "grad_norm": 15.0,
      "learning_rate": 1.8686645213193888e-05,
      "loss": 0.7843,
      "step": 13060
    },
    {
      "epoch": 0.6571802091713597,
      "grad_norm": 18.875,
      "learning_rate": 1.8685639581657284e-05,
      "loss": 0.8107,
      "step": 13070
    },
    {
      "epoch": 0.6576830249396621,
      "grad_norm": 92.0,
      "learning_rate": 1.8684633950120676e-05,
      "loss": 1.1434,
      "step": 13080
    },
    {
      "epoch": 0.6581858407079646,
      "grad_norm": 39.5,
      "learning_rate": 1.8683628318584072e-05,
      "loss": 1.1716,
      "step": 13090
    },
    {
      "epoch": 0.6586886564762671,
      "grad_norm": 39.5,
      "learning_rate": 1.8682622687047468e-05,
      "loss": 0.805,
      "step": 13100
    },
    {
      "epoch": 0.6591914722445696,
      "grad_norm": 9.625,
      "learning_rate": 1.8681617055510864e-05,
      "loss": 1.1102,
      "step": 13110
    },
    {
      "epoch": 0.6596942880128721,
      "grad_norm": 24.625,
      "learning_rate": 1.8680611423974256e-05,
      "loss": 0.7388,
      "step": 13120
    },
    {
      "epoch": 0.6601971037811746,
      "grad_norm": 21.5,
      "learning_rate": 1.8679605792437652e-05,
      "loss": 1.1485,
      "step": 13130
    },
    {
      "epoch": 0.660699919549477,
      "grad_norm": 35.5,
      "learning_rate": 1.8678600160901048e-05,
      "loss": 1.0864,
      "step": 13140
    },
    {
      "epoch": 0.6612027353177795,
      "grad_norm": 19.625,
      "learning_rate": 1.867759452936444e-05,
      "loss": 1.1206,
      "step": 13150
    },
    {
      "epoch": 0.6617055510860821,
      "grad_norm": 32.75,
      "learning_rate": 1.8676588897827837e-05,
      "loss": 0.8133,
      "step": 13160
    },
    {
      "epoch": 0.6622083668543846,
      "grad_norm": 5.8125,
      "learning_rate": 1.8675583266291232e-05,
      "loss": 1.0973,
      "step": 13170
    },
    {
      "epoch": 0.662711182622687,
      "grad_norm": 52.0,
      "learning_rate": 1.8674577634754628e-05,
      "loss": 1.2342,
      "step": 13180
    },
    {
      "epoch": 0.6632139983909895,
      "grad_norm": 29.125,
      "learning_rate": 1.8673572003218024e-05,
      "loss": 1.0618,
      "step": 13190
    },
    {
      "epoch": 0.6637168141592921,
      "grad_norm": 23.5,
      "learning_rate": 1.8672566371681417e-05,
      "loss": 0.867,
      "step": 13200
    },
    {
      "epoch": 0.6642196299275945,
      "grad_norm": 9.1875,
      "learning_rate": 1.8671560740144813e-05,
      "loss": 0.9183,
      "step": 13210
    },
    {
      "epoch": 0.664722445695897,
      "grad_norm": 44.25,
      "learning_rate": 1.867055510860821e-05,
      "loss": 0.9544,
      "step": 13220
    },
    {
      "epoch": 0.6652252614641995,
      "grad_norm": 7.40625,
      "learning_rate": 1.86695494770716e-05,
      "loss": 0.6139,
      "step": 13230
    },
    {
      "epoch": 0.665728077232502,
      "grad_norm": 47.0,
      "learning_rate": 1.8668543845534997e-05,
      "loss": 1.2582,
      "step": 13240
    },
    {
      "epoch": 0.6662308930008045,
      "grad_norm": 33.25,
      "learning_rate": 1.8667538213998393e-05,
      "loss": 1.0179,
      "step": 13250
    },
    {
      "epoch": 0.666733708769107,
      "grad_norm": 62.0,
      "learning_rate": 1.866653258246179e-05,
      "loss": 1.1932,
      "step": 13260
    },
    {
      "epoch": 0.6672365245374094,
      "grad_norm": 8.125,
      "learning_rate": 1.8665526950925184e-05,
      "loss": 0.9243,
      "step": 13270
    },
    {
      "epoch": 0.667739340305712,
      "grad_norm": 7.125,
      "learning_rate": 1.8664521319388577e-05,
      "loss": 1.0498,
      "step": 13280
    },
    {
      "epoch": 0.6682421560740145,
      "grad_norm": 13.3125,
      "learning_rate": 1.8663515687851973e-05,
      "loss": 1.031,
      "step": 13290
    },
    {
      "epoch": 0.668744971842317,
      "grad_norm": 6.3125,
      "learning_rate": 1.866251005631537e-05,
      "loss": 0.9012,
      "step": 13300
    },
    {
      "epoch": 0.6692477876106194,
      "grad_norm": 6.0625,
      "learning_rate": 1.866150442477876e-05,
      "loss": 1.1456,
      "step": 13310
    },
    {
      "epoch": 0.669750603378922,
      "grad_norm": 24.125,
      "learning_rate": 1.8660498793242157e-05,
      "loss": 0.8877,
      "step": 13320
    },
    {
      "epoch": 0.6702534191472245,
      "grad_norm": 11.375,
      "learning_rate": 1.8659493161705553e-05,
      "loss": 0.8331,
      "step": 13330
    },
    {
      "epoch": 0.6707562349155269,
      "grad_norm": 37.25,
      "learning_rate": 1.865848753016895e-05,
      "loss": 0.9967,
      "step": 13340
    },
    {
      "epoch": 0.6712590506838294,
      "grad_norm": 62.75,
      "learning_rate": 1.8657481898632345e-05,
      "loss": 0.8972,
      "step": 13350
    },
    {
      "epoch": 0.671761866452132,
      "grad_norm": 25.5,
      "learning_rate": 1.8656476267095737e-05,
      "loss": 1.3336,
      "step": 13360
    },
    {
      "epoch": 0.6722646822204345,
      "grad_norm": 15.75,
      "learning_rate": 1.8655470635559133e-05,
      "loss": 0.9743,
      "step": 13370
    },
    {
      "epoch": 0.6727674979887369,
      "grad_norm": 23.625,
      "learning_rate": 1.865446500402253e-05,
      "loss": 1.0329,
      "step": 13380
    },
    {
      "epoch": 0.6732703137570394,
      "grad_norm": 39.5,
      "learning_rate": 1.865345937248592e-05,
      "loss": 1.1817,
      "step": 13390
    },
    {
      "epoch": 0.673773129525342,
      "grad_norm": 11.4375,
      "learning_rate": 1.8652453740949317e-05,
      "loss": 0.7289,
      "step": 13400
    },
    {
      "epoch": 0.6742759452936444,
      "grad_norm": 25.0,
      "learning_rate": 1.8651448109412713e-05,
      "loss": 1.2439,
      "step": 13410
    },
    {
      "epoch": 0.6747787610619469,
      "grad_norm": 15.375,
      "learning_rate": 1.8650442477876106e-05,
      "loss": 1.1813,
      "step": 13420
    },
    {
      "epoch": 0.6752815768302494,
      "grad_norm": 102.0,
      "learning_rate": 1.8649436846339505e-05,
      "loss": 0.9507,
      "step": 13430
    },
    {
      "epoch": 0.6757843925985519,
      "grad_norm": 7.65625,
      "learning_rate": 1.8648431214802897e-05,
      "loss": 1.2928,
      "step": 13440
    },
    {
      "epoch": 0.6762872083668544,
      "grad_norm": 52.25,
      "learning_rate": 1.8647425583266293e-05,
      "loss": 0.8708,
      "step": 13450
    },
    {
      "epoch": 0.6767900241351569,
      "grad_norm": 21.875,
      "learning_rate": 1.864641995172969e-05,
      "loss": 0.7471,
      "step": 13460
    },
    {
      "epoch": 0.6772928399034593,
      "grad_norm": 29.625,
      "learning_rate": 1.864541432019308e-05,
      "loss": 0.8003,
      "step": 13470
    },
    {
      "epoch": 0.6777956556717619,
      "grad_norm": 6.0625,
      "learning_rate": 1.8644408688656478e-05,
      "loss": 0.8372,
      "step": 13480
    },
    {
      "epoch": 0.6782984714400644,
      "grad_norm": 14.5625,
      "learning_rate": 1.8643403057119873e-05,
      "loss": 0.9016,
      "step": 13490
    },
    {
      "epoch": 0.6788012872083669,
      "grad_norm": 14.4375,
      "learning_rate": 1.8642397425583266e-05,
      "loss": 0.9227,
      "step": 13500
    },
    {
      "epoch": 0.6788012872083669,
      "eval_accuracy": 0.510932077342588,
      "eval_loss": 1.0323444604873657,
      "eval_runtime": 461.9217,
      "eval_samples_per_second": 87.331,
      "eval_steps_per_second": 87.331,
      "step": 13500
    },
    {
      "epoch": 0.6793041029766693,
      "grad_norm": 27.375,
      "learning_rate": 1.8641391794046665e-05,
      "loss": 0.9622,
      "step": 13510
    },
    {
      "epoch": 0.6798069187449718,
      "grad_norm": 9.125,
      "learning_rate": 1.8640386162510058e-05,
      "loss": 0.8834,
      "step": 13520
    },
    {
      "epoch": 0.6803097345132744,
      "grad_norm": 6.34375,
      "learning_rate": 1.8639380530973454e-05,
      "loss": 0.9499,
      "step": 13530
    },
    {
      "epoch": 0.6808125502815768,
      "grad_norm": 9.625,
      "learning_rate": 1.863837489943685e-05,
      "loss": 0.9415,
      "step": 13540
    },
    {
      "epoch": 0.6813153660498793,
      "grad_norm": 43.25,
      "learning_rate": 1.8637369267900242e-05,
      "loss": 1.1731,
      "step": 13550
    },
    {
      "epoch": 0.6818181818181818,
      "grad_norm": 15.6875,
      "learning_rate": 1.8636363636363638e-05,
      "loss": 1.2233,
      "step": 13560
    },
    {
      "epoch": 0.6823209975864843,
      "grad_norm": 10.5625,
      "learning_rate": 1.8635358004827034e-05,
      "loss": 1.0328,
      "step": 13570
    },
    {
      "epoch": 0.6828238133547868,
      "grad_norm": 10.8125,
      "learning_rate": 1.8634352373290426e-05,
      "loss": 1.1615,
      "step": 13580
    },
    {
      "epoch": 0.6833266291230893,
      "grad_norm": 20.0,
      "learning_rate": 1.8633346741753825e-05,
      "loss": 0.7219,
      "step": 13590
    },
    {
      "epoch": 0.6838294448913917,
      "grad_norm": 37.0,
      "learning_rate": 1.8632341110217218e-05,
      "loss": 1.289,
      "step": 13600
    },
    {
      "epoch": 0.6843322606596943,
      "grad_norm": 29.25,
      "learning_rate": 1.8631335478680614e-05,
      "loss": 0.871,
      "step": 13610
    },
    {
      "epoch": 0.6848350764279968,
      "grad_norm": 15.3125,
      "learning_rate": 1.863032984714401e-05,
      "loss": 1.0844,
      "step": 13620
    },
    {
      "epoch": 0.6853378921962993,
      "grad_norm": 7.78125,
      "learning_rate": 1.8629324215607402e-05,
      "loss": 0.801,
      "step": 13630
    },
    {
      "epoch": 0.6858407079646017,
      "grad_norm": 11.875,
      "learning_rate": 1.8628318584070798e-05,
      "loss": 0.7915,
      "step": 13640
    },
    {
      "epoch": 0.6863435237329043,
      "grad_norm": 31.25,
      "learning_rate": 1.8627312952534194e-05,
      "loss": 1.0958,
      "step": 13650
    },
    {
      "epoch": 0.6868463395012068,
      "grad_norm": 80.0,
      "learning_rate": 1.8626307320997586e-05,
      "loss": 1.0332,
      "step": 13660
    },
    {
      "epoch": 0.6873491552695092,
      "grad_norm": 34.75,
      "learning_rate": 1.8625301689460982e-05,
      "loss": 1.3167,
      "step": 13670
    },
    {
      "epoch": 0.6878519710378117,
      "grad_norm": 31.375,
      "learning_rate": 1.8624296057924378e-05,
      "loss": 1.2158,
      "step": 13680
    },
    {
      "epoch": 0.6883547868061143,
      "grad_norm": 25.75,
      "learning_rate": 1.862329042638777e-05,
      "loss": 0.9274,
      "step": 13690
    },
    {
      "epoch": 0.6888576025744167,
      "grad_norm": 9.8125,
      "learning_rate": 1.862228479485117e-05,
      "loss": 0.7301,
      "step": 13700
    },
    {
      "epoch": 0.6893604183427192,
      "grad_norm": 24.875,
      "learning_rate": 1.8621279163314562e-05,
      "loss": 1.0061,
      "step": 13710
    },
    {
      "epoch": 0.6898632341110217,
      "grad_norm": 25.75,
      "learning_rate": 1.862027353177796e-05,
      "loss": 1.0051,
      "step": 13720
    },
    {
      "epoch": 0.6903660498793243,
      "grad_norm": 12.125,
      "learning_rate": 1.8619267900241354e-05,
      "loss": 1.0593,
      "step": 13730
    },
    {
      "epoch": 0.6908688656476267,
      "grad_norm": 8.9375,
      "learning_rate": 1.8618262268704747e-05,
      "loss": 0.5666,
      "step": 13740
    },
    {
      "epoch": 0.6913716814159292,
      "grad_norm": 35.0,
      "learning_rate": 1.8617256637168143e-05,
      "loss": 1.0133,
      "step": 13750
    },
    {
      "epoch": 0.6918744971842317,
      "grad_norm": 5.90625,
      "learning_rate": 1.861625100563154e-05,
      "loss": 0.7495,
      "step": 13760
    },
    {
      "epoch": 0.6923773129525342,
      "grad_norm": 17.5,
      "learning_rate": 1.861524537409493e-05,
      "loss": 1.0428,
      "step": 13770
    },
    {
      "epoch": 0.6928801287208367,
      "grad_norm": 58.5,
      "learning_rate": 1.861423974255833e-05,
      "loss": 1.2406,
      "step": 13780
    },
    {
      "epoch": 0.6933829444891392,
      "grad_norm": 31.0,
      "learning_rate": 1.8613234111021723e-05,
      "loss": 1.0103,
      "step": 13790
    },
    {
      "epoch": 0.6938857602574416,
      "grad_norm": 5.9375,
      "learning_rate": 1.861222847948512e-05,
      "loss": 1.1257,
      "step": 13800
    },
    {
      "epoch": 0.6943885760257442,
      "grad_norm": 54.0,
      "learning_rate": 1.8611222847948514e-05,
      "loss": 0.9919,
      "step": 13810
    },
    {
      "epoch": 0.6948913917940467,
      "grad_norm": 44.0,
      "learning_rate": 1.8610217216411907e-05,
      "loss": 0.9218,
      "step": 13820
    },
    {
      "epoch": 0.6953942075623492,
      "grad_norm": 25.125,
      "learning_rate": 1.8609211584875303e-05,
      "loss": 0.9679,
      "step": 13830
    },
    {
      "epoch": 0.6958970233306516,
      "grad_norm": 17.125,
      "learning_rate": 1.86082059533387e-05,
      "loss": 1.0946,
      "step": 13840
    },
    {
      "epoch": 0.6963998390989542,
      "grad_norm": 19.125,
      "learning_rate": 1.860720032180209e-05,
      "loss": 0.8294,
      "step": 13850
    },
    {
      "epoch": 0.6969026548672567,
      "grad_norm": 34.5,
      "learning_rate": 1.860619469026549e-05,
      "loss": 1.1483,
      "step": 13860
    },
    {
      "epoch": 0.6974054706355591,
      "grad_norm": 18.25,
      "learning_rate": 1.8605189058728883e-05,
      "loss": 0.9998,
      "step": 13870
    },
    {
      "epoch": 0.6979082864038616,
      "grad_norm": 53.5,
      "learning_rate": 1.860418342719228e-05,
      "loss": 1.0154,
      "step": 13880
    },
    {
      "epoch": 0.6984111021721641,
      "grad_norm": 25.0,
      "learning_rate": 1.8603177795655675e-05,
      "loss": 1.1831,
      "step": 13890
    },
    {
      "epoch": 0.6989139179404666,
      "grad_norm": 23.125,
      "learning_rate": 1.8602172164119067e-05,
      "loss": 0.9966,
      "step": 13900
    },
    {
      "epoch": 0.6994167337087691,
      "grad_norm": 11.9375,
      "learning_rate": 1.8601166532582463e-05,
      "loss": 0.7847,
      "step": 13910
    },
    {
      "epoch": 0.6999195494770716,
      "grad_norm": 15.125,
      "learning_rate": 1.860016090104586e-05,
      "loss": 0.9223,
      "step": 13920
    },
    {
      "epoch": 0.700422365245374,
      "grad_norm": 12.5625,
      "learning_rate": 1.859915526950925e-05,
      "loss": 1.0408,
      "step": 13930
    },
    {
      "epoch": 0.7009251810136766,
      "grad_norm": 58.0,
      "learning_rate": 1.8598149637972647e-05,
      "loss": 0.9072,
      "step": 13940
    },
    {
      "epoch": 0.7014279967819791,
      "grad_norm": 28.125,
      "learning_rate": 1.8597144006436043e-05,
      "loss": 0.6444,
      "step": 13950
    },
    {
      "epoch": 0.7019308125502816,
      "grad_norm": 32.0,
      "learning_rate": 1.8596138374899436e-05,
      "loss": 0.8683,
      "step": 13960
    },
    {
      "epoch": 0.702433628318584,
      "grad_norm": 10.0,
      "learning_rate": 1.8595132743362835e-05,
      "loss": 0.9581,
      "step": 13970
    },
    {
      "epoch": 0.7029364440868866,
      "grad_norm": 18.0,
      "learning_rate": 1.8594127111826227e-05,
      "loss": 1.1096,
      "step": 13980
    },
    {
      "epoch": 0.7034392598551891,
      "grad_norm": 19.875,
      "learning_rate": 1.8593121480289623e-05,
      "loss": 0.9573,
      "step": 13990
    },
    {
      "epoch": 0.7039420756234915,
      "grad_norm": 7.5,
      "learning_rate": 1.859211584875302e-05,
      "loss": 1.1005,
      "step": 14000
    },
    {
      "epoch": 0.7039420756234915,
      "eval_accuracy": 0.5113534952900347,
      "eval_loss": 1.030100703239441,
      "eval_runtime": 463.2207,
      "eval_samples_per_second": 87.086,
      "eval_steps_per_second": 87.086,
      "step": 14000
    },
    {
      "epoch": 0.704444891391794,
      "grad_norm": 41.75,
      "learning_rate": 1.8591110217216412e-05,
      "loss": 1.1537,
      "step": 14010
    },
    {
      "epoch": 0.7049477071600966,
      "grad_norm": 5.8125,
      "learning_rate": 1.8590104585679808e-05,
      "loss": 1.0649,
      "step": 14020
    },
    {
      "epoch": 0.705450522928399,
      "grad_norm": 14.125,
      "learning_rate": 1.8589098954143203e-05,
      "loss": 0.9864,
      "step": 14030
    },
    {
      "epoch": 0.7059533386967015,
      "grad_norm": 42.5,
      "learning_rate": 1.8588093322606596e-05,
      "loss": 0.8748,
      "step": 14040
    },
    {
      "epoch": 0.706456154465004,
      "grad_norm": 27.25,
      "learning_rate": 1.8587087691069995e-05,
      "loss": 0.966,
      "step": 14050
    },
    {
      "epoch": 0.7069589702333066,
      "grad_norm": 30.0,
      "learning_rate": 1.8586082059533388e-05,
      "loss": 0.9762,
      "step": 14060
    },
    {
      "epoch": 0.707461786001609,
      "grad_norm": 57.5,
      "learning_rate": 1.8585076427996784e-05,
      "loss": 1.528,
      "step": 14070
    },
    {
      "epoch": 0.7079646017699115,
      "grad_norm": 62.25,
      "learning_rate": 1.858407079646018e-05,
      "loss": 0.8634,
      "step": 14080
    },
    {
      "epoch": 0.708467417538214,
      "grad_norm": 18.875,
      "learning_rate": 1.8583065164923572e-05,
      "loss": 0.808,
      "step": 14090
    },
    {
      "epoch": 0.7089702333065165,
      "grad_norm": 12.375,
      "learning_rate": 1.8582059533386968e-05,
      "loss": 0.95,
      "step": 14100
    },
    {
      "epoch": 0.709473049074819,
      "grad_norm": 24.875,
      "learning_rate": 1.8581053901850364e-05,
      "loss": 0.8798,
      "step": 14110
    },
    {
      "epoch": 0.7099758648431215,
      "grad_norm": 5.15625,
      "learning_rate": 1.8580048270313756e-05,
      "loss": 0.7978,
      "step": 14120
    },
    {
      "epoch": 0.7104786806114239,
      "grad_norm": 37.5,
      "learning_rate": 1.8579042638777155e-05,
      "loss": 1.123,
      "step": 14130
    },
    {
      "epoch": 0.7109814963797265,
      "grad_norm": 18.25,
      "learning_rate": 1.8578037007240548e-05,
      "loss": 1.1545,
      "step": 14140
    },
    {
      "epoch": 0.711484312148029,
      "grad_norm": 34.0,
      "learning_rate": 1.8577031375703944e-05,
      "loss": 1.0842,
      "step": 14150
    },
    {
      "epoch": 0.7119871279163315,
      "grad_norm": 10.0,
      "learning_rate": 1.857602574416734e-05,
      "loss": 1.0354,
      "step": 14160
    },
    {
      "epoch": 0.7124899436846339,
      "grad_norm": 12.0,
      "learning_rate": 1.8575020112630732e-05,
      "loss": 0.7939,
      "step": 14170
    },
    {
      "epoch": 0.7129927594529365,
      "grad_norm": 13.0625,
      "learning_rate": 1.8574014481094128e-05,
      "loss": 1.1055,
      "step": 14180
    },
    {
      "epoch": 0.713495575221239,
      "grad_norm": 32.25,
      "learning_rate": 1.8573008849557524e-05,
      "loss": 0.9832,
      "step": 14190
    },
    {
      "epoch": 0.7139983909895414,
      "grad_norm": 16.5,
      "learning_rate": 1.8572003218020916e-05,
      "loss": 0.8253,
      "step": 14200
    },
    {
      "epoch": 0.7145012067578439,
      "grad_norm": 7.65625,
      "learning_rate": 1.8570997586484312e-05,
      "loss": 1.021,
      "step": 14210
    },
    {
      "epoch": 0.7150040225261464,
      "grad_norm": 22.875,
      "learning_rate": 1.8569991954947708e-05,
      "loss": 0.8142,
      "step": 14220
    },
    {
      "epoch": 0.7155068382944489,
      "grad_norm": 9.0625,
      "learning_rate": 1.8568986323411104e-05,
      "loss": 1.4501,
      "step": 14230
    },
    {
      "epoch": 0.7160096540627514,
      "grad_norm": 19.375,
      "learning_rate": 1.85679806918745e-05,
      "loss": 0.8696,
      "step": 14240
    },
    {
      "epoch": 0.7165124698310539,
      "grad_norm": 31.75,
      "learning_rate": 1.8566975060337892e-05,
      "loss": 0.8813,
      "step": 14250
    },
    {
      "epoch": 0.7170152855993563,
      "grad_norm": 8.9375,
      "learning_rate": 1.856596942880129e-05,
      "loss": 0.9293,
      "step": 14260
    },
    {
      "epoch": 0.7175181013676589,
      "grad_norm": 16.75,
      "learning_rate": 1.8564963797264684e-05,
      "loss": 0.8417,
      "step": 14270
    },
    {
      "epoch": 0.7180209171359614,
      "grad_norm": 39.75,
      "learning_rate": 1.856395816572808e-05,
      "loss": 1.0728,
      "step": 14280
    },
    {
      "epoch": 0.7185237329042639,
      "grad_norm": 11.625,
      "learning_rate": 1.8562952534191473e-05,
      "loss": 1.0961,
      "step": 14290
    },
    {
      "epoch": 0.7190265486725663,
      "grad_norm": 35.75,
      "learning_rate": 1.856194690265487e-05,
      "loss": 0.8579,
      "step": 14300
    },
    {
      "epoch": 0.7195293644408689,
      "grad_norm": 21.875,
      "learning_rate": 1.8560941271118264e-05,
      "loss": 1.2716,
      "step": 14310
    },
    {
      "epoch": 0.7200321802091714,
      "grad_norm": 21.375,
      "learning_rate": 1.855993563958166e-05,
      "loss": 0.9268,
      "step": 14320
    },
    {
      "epoch": 0.7205349959774738,
      "grad_norm": 5.5,
      "learning_rate": 1.8558930008045053e-05,
      "loss": 0.933,
      "step": 14330
    },
    {
      "epoch": 0.7210378117457763,
      "grad_norm": 38.0,
      "learning_rate": 1.855792437650845e-05,
      "loss": 1.3161,
      "step": 14340
    },
    {
      "epoch": 0.7215406275140789,
      "grad_norm": 90.0,
      "learning_rate": 1.8556918744971844e-05,
      "loss": 1.1467,
      "step": 14350
    },
    {
      "epoch": 0.7220434432823813,
      "grad_norm": 12.125,
      "learning_rate": 1.855591311343524e-05,
      "loss": 1.2752,
      "step": 14360
    },
    {
      "epoch": 0.7225462590506838,
      "grad_norm": 5.78125,
      "learning_rate": 1.8554907481898633e-05,
      "loss": 0.8789,
      "step": 14370
    },
    {
      "epoch": 0.7230490748189863,
      "grad_norm": 24.0,
      "learning_rate": 1.855390185036203e-05,
      "loss": 1.0457,
      "step": 14380
    },
    {
      "epoch": 0.7235518905872889,
      "grad_norm": 11.375,
      "learning_rate": 1.8552896218825425e-05,
      "loss": 0.7788,
      "step": 14390
    },
    {
      "epoch": 0.7240547063555913,
      "grad_norm": 59.0,
      "learning_rate": 1.855189058728882e-05,
      "loss": 1.1851,
      "step": 14400
    },
    {
      "epoch": 0.7245575221238938,
      "grad_norm": 9.0,
      "learning_rate": 1.8550884955752213e-05,
      "loss": 0.8034,
      "step": 14410
    },
    {
      "epoch": 0.7250603378921963,
      "grad_norm": 8.625,
      "learning_rate": 1.854987932421561e-05,
      "loss": 1.301,
      "step": 14420
    },
    {
      "epoch": 0.7255631536604988,
      "grad_norm": 25.625,
      "learning_rate": 1.8548873692679005e-05,
      "loss": 1.2742,
      "step": 14430
    },
    {
      "epoch": 0.7260659694288013,
      "grad_norm": 7.5625,
      "learning_rate": 1.85478680611424e-05,
      "loss": 0.9219,
      "step": 14440
    },
    {
      "epoch": 0.7265687851971038,
      "grad_norm": 21.0,
      "learning_rate": 1.8546862429605793e-05,
      "loss": 0.9952,
      "step": 14450
    },
    {
      "epoch": 0.7270716009654062,
      "grad_norm": 17.125,
      "learning_rate": 1.854585679806919e-05,
      "loss": 0.9337,
      "step": 14460
    },
    {
      "epoch": 0.7275744167337088,
      "grad_norm": 20.875,
      "learning_rate": 1.8544851166532585e-05,
      "loss": 1.1636,
      "step": 14470
    },
    {
      "epoch": 0.7280772325020113,
      "grad_norm": 10.125,
      "learning_rate": 1.8543845534995977e-05,
      "loss": 0.8086,
      "step": 14480
    },
    {
      "epoch": 0.7285800482703138,
      "grad_norm": 22.25,
      "learning_rate": 1.8542839903459373e-05,
      "loss": 0.8197,
      "step": 14490
    },
    {
      "epoch": 0.7290828640386162,
      "grad_norm": 45.0,
      "learning_rate": 1.854183427192277e-05,
      "loss": 1.087,
      "step": 14500
    },
    {
      "epoch": 0.7290828640386162,
      "eval_accuracy": 0.5115518096182449,
      "eval_loss": 1.0304903984069824,
      "eval_runtime": 462.3806,
      "eval_samples_per_second": 87.244,
      "eval_steps_per_second": 87.244,
      "step": 14500
    },
    {
      "epoch": 0.7295856798069188,
      "grad_norm": 23.875,
      "learning_rate": 1.8540828640386165e-05,
      "loss": 0.9876,
      "step": 14510
    },
    {
      "epoch": 0.7300884955752213,
      "grad_norm": 10.5625,
      "learning_rate": 1.853982300884956e-05,
      "loss": 1.0546,
      "step": 14520
    },
    {
      "epoch": 0.7305913113435237,
      "grad_norm": 82.5,
      "learning_rate": 1.8538817377312953e-05,
      "loss": 1.1393,
      "step": 14530
    },
    {
      "epoch": 0.7310941271118262,
      "grad_norm": 33.5,
      "learning_rate": 1.853781174577635e-05,
      "loss": 1.0272,
      "step": 14540
    },
    {
      "epoch": 0.7315969428801288,
      "grad_norm": 21.125,
      "learning_rate": 1.8536806114239745e-05,
      "loss": 0.9209,
      "step": 14550
    },
    {
      "epoch": 0.7320997586484312,
      "grad_norm": 62.0,
      "learning_rate": 1.8535800482703138e-05,
      "loss": 1.0285,
      "step": 14560
    },
    {
      "epoch": 0.7326025744167337,
      "grad_norm": 51.75,
      "learning_rate": 1.8534794851166533e-05,
      "loss": 0.9036,
      "step": 14570
    },
    {
      "epoch": 0.7331053901850362,
      "grad_norm": 13.625,
      "learning_rate": 1.853378921962993e-05,
      "loss": 0.9901,
      "step": 14580
    },
    {
      "epoch": 0.7336082059533386,
      "grad_norm": 23.75,
      "learning_rate": 1.8532783588093325e-05,
      "loss": 1.007,
      "step": 14590
    },
    {
      "epoch": 0.7341110217216412,
      "grad_norm": 6.71875,
      "learning_rate": 1.853177795655672e-05,
      "loss": 1.4053,
      "step": 14600
    },
    {
      "epoch": 0.7346138374899437,
      "grad_norm": 11.625,
      "learning_rate": 1.8530772325020114e-05,
      "loss": 0.9813,
      "step": 14610
    },
    {
      "epoch": 0.7351166532582462,
      "grad_norm": 8.5,
      "learning_rate": 1.852976669348351e-05,
      "loss": 1.1478,
      "step": 14620
    },
    {
      "epoch": 0.7356194690265486,
      "grad_norm": 16.125,
      "learning_rate": 1.8528761061946905e-05,
      "loss": 1.1858,
      "step": 14630
    },
    {
      "epoch": 0.7361222847948512,
      "grad_norm": 17.875,
      "learning_rate": 1.8527755430410298e-05,
      "loss": 0.9545,
      "step": 14640
    },
    {
      "epoch": 0.7366251005631537,
      "grad_norm": 31.5,
      "learning_rate": 1.8526749798873694e-05,
      "loss": 0.8781,
      "step": 14650
    },
    {
      "epoch": 0.7371279163314561,
      "grad_norm": 44.5,
      "learning_rate": 1.852574416733709e-05,
      "loss": 0.9256,
      "step": 14660
    },
    {
      "epoch": 0.7376307320997586,
      "grad_norm": 19.25,
      "learning_rate": 1.8524738535800486e-05,
      "loss": 0.8726,
      "step": 14670
    },
    {
      "epoch": 0.7381335478680612,
      "grad_norm": 10.8125,
      "learning_rate": 1.852373290426388e-05,
      "loss": 1.0437,
      "step": 14680
    },
    {
      "epoch": 0.7386363636363636,
      "grad_norm": 6.84375,
      "learning_rate": 1.8522727272727274e-05,
      "loss": 0.8256,
      "step": 14690
    },
    {
      "epoch": 0.7391391794046661,
      "grad_norm": 28.875,
      "learning_rate": 1.852172164119067e-05,
      "loss": 0.9751,
      "step": 14700
    },
    {
      "epoch": 0.7396419951729686,
      "grad_norm": 18.25,
      "learning_rate": 1.8520716009654066e-05,
      "loss": 0.931,
      "step": 14710
    },
    {
      "epoch": 0.7401448109412712,
      "grad_norm": 15.125,
      "learning_rate": 1.8519710378117458e-05,
      "loss": 0.9793,
      "step": 14720
    },
    {
      "epoch": 0.7406476267095736,
      "grad_norm": 9.25,
      "learning_rate": 1.8518704746580854e-05,
      "loss": 1.0711,
      "step": 14730
    },
    {
      "epoch": 0.7411504424778761,
      "grad_norm": 60.5,
      "learning_rate": 1.851769911504425e-05,
      "loss": 1.0638,
      "step": 14740
    },
    {
      "epoch": 0.7416532582461786,
      "grad_norm": 17.75,
      "learning_rate": 1.8516693483507642e-05,
      "loss": 0.9586,
      "step": 14750
    },
    {
      "epoch": 0.7421560740144811,
      "grad_norm": 14.5625,
      "learning_rate": 1.851568785197104e-05,
      "loss": 1.0803,
      "step": 14760
    },
    {
      "epoch": 0.7426588897827836,
      "grad_norm": 13.125,
      "learning_rate": 1.8514682220434434e-05,
      "loss": 0.7279,
      "step": 14770
    },
    {
      "epoch": 0.7431617055510861,
      "grad_norm": 8.5625,
      "learning_rate": 1.851367658889783e-05,
      "loss": 1.303,
      "step": 14780
    },
    {
      "epoch": 0.7436645213193885,
      "grad_norm": 36.75,
      "learning_rate": 1.8512670957361226e-05,
      "loss": 0.731,
      "step": 14790
    },
    {
      "epoch": 0.7441673370876911,
      "grad_norm": 30.5,
      "learning_rate": 1.851166532582462e-05,
      "loss": 1.1614,
      "step": 14800
    },
    {
      "epoch": 0.7446701528559936,
      "grad_norm": 14.0625,
      "learning_rate": 1.8510659694288014e-05,
      "loss": 0.6634,
      "step": 14810
    },
    {
      "epoch": 0.745172968624296,
      "grad_norm": 14.4375,
      "learning_rate": 1.850965406275141e-05,
      "loss": 1.0988,
      "step": 14820
    },
    {
      "epoch": 0.7456757843925985,
      "grad_norm": 24.75,
      "learning_rate": 1.8508648431214803e-05,
      "loss": 0.9588,
      "step": 14830
    },
    {
      "epoch": 0.7461786001609011,
      "grad_norm": 6.34375,
      "learning_rate": 1.8507642799678202e-05,
      "loss": 1.0428,
      "step": 14840
    },
    {
      "epoch": 0.7466814159292036,
      "grad_norm": 75.0,
      "learning_rate": 1.8506637168141594e-05,
      "loss": 1.1366,
      "step": 14850
    },
    {
      "epoch": 0.747184231697506,
      "grad_norm": 12.75,
      "learning_rate": 1.850563153660499e-05,
      "loss": 0.8348,
      "step": 14860
    },
    {
      "epoch": 0.7476870474658085,
      "grad_norm": 9.4375,
      "learning_rate": 1.8504625905068386e-05,
      "loss": 1.0447,
      "step": 14870
    },
    {
      "epoch": 0.7481898632341111,
      "grad_norm": 29.375,
      "learning_rate": 1.850362027353178e-05,
      "loss": 1.116,
      "step": 14880
    },
    {
      "epoch": 0.7486926790024135,
      "grad_norm": 33.0,
      "learning_rate": 1.8502614641995175e-05,
      "loss": 0.9166,
      "step": 14890
    },
    {
      "epoch": 0.749195494770716,
      "grad_norm": 37.25,
      "learning_rate": 1.850160901045857e-05,
      "loss": 1.0141,
      "step": 14900
    },
    {
      "epoch": 0.7496983105390185,
      "grad_norm": 11.6875,
      "learning_rate": 1.8500603378921963e-05,
      "loss": 0.8626,
      "step": 14910
    },
    {
      "epoch": 0.7502011263073209,
      "grad_norm": 7.4375,
      "learning_rate": 1.8499597747385362e-05,
      "loss": 0.7202,
      "step": 14920
    },
    {
      "epoch": 0.7507039420756235,
      "grad_norm": 37.25,
      "learning_rate": 1.8498592115848755e-05,
      "loss": 1.3069,
      "step": 14930
    },
    {
      "epoch": 0.751206757843926,
      "grad_norm": 45.25,
      "learning_rate": 1.849758648431215e-05,
      "loss": 0.9123,
      "step": 14940
    },
    {
      "epoch": 0.7517095736122285,
      "grad_norm": 12.1875,
      "learning_rate": 1.8496580852775546e-05,
      "loss": 0.9235,
      "step": 14950
    },
    {
      "epoch": 0.7522123893805309,
      "grad_norm": 31.875,
      "learning_rate": 1.849557522123894e-05,
      "loss": 1.4413,
      "step": 14960
    },
    {
      "epoch": 0.7527152051488335,
      "grad_norm": 16.875,
      "learning_rate": 1.8494569589702335e-05,
      "loss": 1.074,
      "step": 14970
    },
    {
      "epoch": 0.753218020917136,
      "grad_norm": 7.375,
      "learning_rate": 1.849356395816573e-05,
      "loss": 0.9867,
      "step": 14980
    },
    {
      "epoch": 0.7537208366854384,
      "grad_norm": 6.5625,
      "learning_rate": 1.8492558326629123e-05,
      "loss": 0.7498,
      "step": 14990
    },
    {
      "epoch": 0.7542236524537409,
      "grad_norm": 53.75,
      "learning_rate": 1.849155269509252e-05,
      "loss": 1.045,
      "step": 15000
    },
    {
      "epoch": 0.7542236524537409,
      "eval_accuracy": 0.5123450669310857,
      "eval_loss": 1.0296765565872192,
      "eval_runtime": 462.1831,
      "eval_samples_per_second": 87.281,
      "eval_steps_per_second": 87.281,
      "step": 15000
    },
    {
      "epoch": 0.7547264682220435,
      "grad_norm": 34.25,
      "learning_rate": 1.8490547063555915e-05,
      "loss": 1.3088,
      "step": 15010
    },
    {
      "epoch": 0.755229283990346,
      "grad_norm": 61.0,
      "learning_rate": 1.8489541432019307e-05,
      "loss": 0.9974,
      "step": 15020
    },
    {
      "epoch": 0.7557320997586484,
      "grad_norm": 27.5,
      "learning_rate": 1.8488535800482707e-05,
      "loss": 0.7487,
      "step": 15030
    },
    {
      "epoch": 0.7562349155269509,
      "grad_norm": 19.75,
      "learning_rate": 1.84875301689461e-05,
      "loss": 0.7547,
      "step": 15040
    },
    {
      "epoch": 0.7567377312952535,
      "grad_norm": 16.0,
      "learning_rate": 1.8486524537409495e-05,
      "loss": 0.9791,
      "step": 15050
    },
    {
      "epoch": 0.7572405470635559,
      "grad_norm": 22.75,
      "learning_rate": 1.848551890587289e-05,
      "loss": 1.3102,
      "step": 15060
    },
    {
      "epoch": 0.7577433628318584,
      "grad_norm": 12.8125,
      "learning_rate": 1.8484513274336283e-05,
      "loss": 0.8489,
      "step": 15070
    },
    {
      "epoch": 0.7582461786001609,
      "grad_norm": 38.5,
      "learning_rate": 1.848350764279968e-05,
      "loss": 1.138,
      "step": 15080
    },
    {
      "epoch": 0.7587489943684634,
      "grad_norm": 8.1875,
      "learning_rate": 1.8482502011263075e-05,
      "loss": 0.9184,
      "step": 15090
    },
    {
      "epoch": 0.7592518101367659,
      "grad_norm": 27.5,
      "learning_rate": 1.8481496379726468e-05,
      "loss": 0.9464,
      "step": 15100
    },
    {
      "epoch": 0.7597546259050684,
      "grad_norm": 28.75,
      "learning_rate": 1.8480490748189867e-05,
      "loss": 1.0403,
      "step": 15110
    },
    {
      "epoch": 0.7602574416733708,
      "grad_norm": 23.625,
      "learning_rate": 1.847948511665326e-05,
      "loss": 0.9567,
      "step": 15120
    },
    {
      "epoch": 0.7607602574416734,
      "grad_norm": 6.65625,
      "learning_rate": 1.8478479485116655e-05,
      "loss": 0.6304,
      "step": 15130
    },
    {
      "epoch": 0.7612630732099759,
      "grad_norm": 73.0,
      "learning_rate": 1.847747385358005e-05,
      "loss": 1.1861,
      "step": 15140
    },
    {
      "epoch": 0.7617658889782783,
      "grad_norm": 12.125,
      "learning_rate": 1.8476468222043444e-05,
      "loss": 1.237,
      "step": 15150
    },
    {
      "epoch": 0.7622687047465808,
      "grad_norm": 9.0625,
      "learning_rate": 1.847546259050684e-05,
      "loss": 1.0577,
      "step": 15160
    },
    {
      "epoch": 0.7627715205148834,
      "grad_norm": 37.75,
      "learning_rate": 1.8474456958970235e-05,
      "loss": 0.951,
      "step": 15170
    },
    {
      "epoch": 0.7632743362831859,
      "grad_norm": 11.8125,
      "learning_rate": 1.8473451327433628e-05,
      "loss": 1.2675,
      "step": 15180
    },
    {
      "epoch": 0.7637771520514883,
      "grad_norm": 14.625,
      "learning_rate": 1.8472445695897027e-05,
      "loss": 0.8958,
      "step": 15190
    },
    {
      "epoch": 0.7642799678197908,
      "grad_norm": 19.0,
      "learning_rate": 1.847144006436042e-05,
      "loss": 1.01,
      "step": 15200
    },
    {
      "epoch": 0.7647827835880934,
      "grad_norm": 26.125,
      "learning_rate": 1.8470434432823816e-05,
      "loss": 1.1511,
      "step": 15210
    },
    {
      "epoch": 0.7652855993563958,
      "grad_norm": 52.0,
      "learning_rate": 1.846942880128721e-05,
      "loss": 0.8142,
      "step": 15220
    },
    {
      "epoch": 0.7657884151246983,
      "grad_norm": 19.625,
      "learning_rate": 1.8468423169750604e-05,
      "loss": 0.9245,
      "step": 15230
    },
    {
      "epoch": 0.7662912308930008,
      "grad_norm": 12.5625,
      "learning_rate": 1.8467417538214e-05,
      "loss": 0.983,
      "step": 15240
    },
    {
      "epoch": 0.7667940466613034,
      "grad_norm": 34.75,
      "learning_rate": 1.8466411906677396e-05,
      "loss": 0.7909,
      "step": 15250
    },
    {
      "epoch": 0.7672968624296058,
      "grad_norm": 19.75,
      "learning_rate": 1.8465406275140788e-05,
      "loss": 1.0142,
      "step": 15260
    },
    {
      "epoch": 0.7677996781979083,
      "grad_norm": 17.875,
      "learning_rate": 1.8464400643604184e-05,
      "loss": 1.0054,
      "step": 15270
    },
    {
      "epoch": 0.7683024939662108,
      "grad_norm": 5.09375,
      "learning_rate": 1.846339501206758e-05,
      "loss": 0.9838,
      "step": 15280
    },
    {
      "epoch": 0.7688053097345132,
      "grad_norm": 9.625,
      "learning_rate": 1.8462389380530972e-05,
      "loss": 1.2101,
      "step": 15290
    },
    {
      "epoch": 0.7693081255028158,
      "grad_norm": 40.5,
      "learning_rate": 1.846138374899437e-05,
      "loss": 0.8731,
      "step": 15300
    },
    {
      "epoch": 0.7698109412711183,
      "grad_norm": 8.125,
      "learning_rate": 1.8460378117457764e-05,
      "loss": 0.838,
      "step": 15310
    },
    {
      "epoch": 0.7703137570394207,
      "grad_norm": 9.9375,
      "learning_rate": 1.845937248592116e-05,
      "loss": 0.9281,
      "step": 15320
    },
    {
      "epoch": 0.7708165728077232,
      "grad_norm": 9.0,
      "learning_rate": 1.8458366854384556e-05,
      "loss": 0.7506,
      "step": 15330
    },
    {
      "epoch": 0.7713193885760258,
      "grad_norm": 30.125,
      "learning_rate": 1.845736122284795e-05,
      "loss": 1.3884,
      "step": 15340
    },
    {
      "epoch": 0.7718222043443282,
      "grad_norm": 45.75,
      "learning_rate": 1.8456355591311344e-05,
      "loss": 0.9122,
      "step": 15350
    },
    {
      "epoch": 0.7723250201126307,
      "grad_norm": 24.75,
      "learning_rate": 1.845534995977474e-05,
      "loss": 1.3035,
      "step": 15360
    },
    {
      "epoch": 0.7728278358809332,
      "grad_norm": 39.5,
      "learning_rate": 1.8454344328238133e-05,
      "loss": 0.9214,
      "step": 15370
    },
    {
      "epoch": 0.7733306516492358,
      "grad_norm": 25.25,
      "learning_rate": 1.8453338696701532e-05,
      "loss": 0.9201,
      "step": 15380
    },
    {
      "epoch": 0.7738334674175382,
      "grad_norm": 11.0625,
      "learning_rate": 1.8452333065164924e-05,
      "loss": 0.9759,
      "step": 15390
    },
    {
      "epoch": 0.7743362831858407,
      "grad_norm": 13.0625,
      "learning_rate": 1.845132743362832e-05,
      "loss": 1.1604,
      "step": 15400
    },
    {
      "epoch": 0.7748390989541432,
      "grad_norm": 30.625,
      "learning_rate": 1.8450321802091716e-05,
      "loss": 1.281,
      "step": 15410
    },
    {
      "epoch": 0.7753419147224457,
      "grad_norm": 5.9375,
      "learning_rate": 1.844931617055511e-05,
      "loss": 1.1496,
      "step": 15420
    },
    {
      "epoch": 0.7758447304907482,
      "grad_norm": 37.75,
      "learning_rate": 1.8448310539018505e-05,
      "loss": 0.9043,
      "step": 15430
    },
    {
      "epoch": 0.7763475462590507,
      "grad_norm": 5.84375,
      "learning_rate": 1.84473049074819e-05,
      "loss": 1.1902,
      "step": 15440
    },
    {
      "epoch": 0.7768503620273531,
      "grad_norm": 14.5,
      "learning_rate": 1.8446299275945293e-05,
      "loss": 1.0369,
      "step": 15450
    },
    {
      "epoch": 0.7773531777956557,
      "grad_norm": 9.0625,
      "learning_rate": 1.8445293644408692e-05,
      "loss": 0.7428,
      "step": 15460
    },
    {
      "epoch": 0.7778559935639582,
      "grad_norm": 27.125,
      "learning_rate": 1.8444288012872085e-05,
      "loss": 1.0334,
      "step": 15470
    },
    {
      "epoch": 0.7783588093322606,
      "grad_norm": 25.125,
      "learning_rate": 1.844328238133548e-05,
      "loss": 1.0755,
      "step": 15480
    },
    {
      "epoch": 0.7788616251005631,
      "grad_norm": 25.125,
      "learning_rate": 1.8442276749798876e-05,
      "loss": 0.9872,
      "step": 15490
    },
    {
      "epoch": 0.7793644408688657,
      "grad_norm": 10.6875,
      "learning_rate": 1.844127111826227e-05,
      "loss": 1.0176,
      "step": 15500
    },
    {
      "epoch": 0.7793644408688657,
      "eval_accuracy": 0.5121715418939018,
      "eval_loss": 1.0302051305770874,
      "eval_runtime": 461.3059,
      "eval_samples_per_second": 87.447,
      "eval_steps_per_second": 87.447,
      "step": 15500
    },
    {
      "epoch": 0.7798672566371682,
      "grad_norm": 13.125,
      "learning_rate": 1.8440265486725665e-05,
      "loss": 0.9188,
      "step": 15510
    },
    {
      "epoch": 0.7803700724054706,
      "grad_norm": 29.875,
      "learning_rate": 1.843925985518906e-05,
      "loss": 0.8956,
      "step": 15520
    },
    {
      "epoch": 0.7808728881737731,
      "grad_norm": 29.625,
      "learning_rate": 1.8438254223652453e-05,
      "loss": 1.0843,
      "step": 15530
    },
    {
      "epoch": 0.7813757039420757,
      "grad_norm": 9.0625,
      "learning_rate": 1.843724859211585e-05,
      "loss": 0.7549,
      "step": 15540
    },
    {
      "epoch": 0.7818785197103781,
      "grad_norm": 69.5,
      "learning_rate": 1.8436242960579245e-05,
      "loss": 1.3544,
      "step": 15550
    },
    {
      "epoch": 0.7823813354786806,
      "grad_norm": 5.90625,
      "learning_rate": 1.843523732904264e-05,
      "loss": 0.8588,
      "step": 15560
    },
    {
      "epoch": 0.7828841512469831,
      "grad_norm": 47.75,
      "learning_rate": 1.8434231697506037e-05,
      "loss": 1.0913,
      "step": 15570
    },
    {
      "epoch": 0.7833869670152857,
      "grad_norm": 15.4375,
      "learning_rate": 1.843322606596943e-05,
      "loss": 0.9026,
      "step": 15580
    },
    {
      "epoch": 0.7838897827835881,
      "grad_norm": 46.5,
      "learning_rate": 1.8432220434432825e-05,
      "loss": 0.791,
      "step": 15590
    },
    {
      "epoch": 0.7843925985518906,
      "grad_norm": 17.75,
      "learning_rate": 1.843121480289622e-05,
      "loss": 1.2057,
      "step": 15600
    },
    {
      "epoch": 0.784895414320193,
      "grad_norm": 31.625,
      "learning_rate": 1.8430209171359613e-05,
      "loss": 0.6408,
      "step": 15610
    },
    {
      "epoch": 0.7853982300884956,
      "grad_norm": 25.25,
      "learning_rate": 1.842920353982301e-05,
      "loss": 0.9845,
      "step": 15620
    },
    {
      "epoch": 0.7859010458567981,
      "grad_norm": 39.75,
      "learning_rate": 1.8428197908286405e-05,
      "loss": 1.3269,
      "step": 15630
    },
    {
      "epoch": 0.7864038616251006,
      "grad_norm": 6.90625,
      "learning_rate": 1.84271922767498e-05,
      "loss": 1.187,
      "step": 15640
    },
    {
      "epoch": 0.786906677393403,
      "grad_norm": 23.875,
      "learning_rate": 1.8426186645213197e-05,
      "loss": 1.2489,
      "step": 15650
    },
    {
      "epoch": 0.7874094931617055,
      "grad_norm": 13.6875,
      "learning_rate": 1.842518101367659e-05,
      "loss": 1.0186,
      "step": 15660
    },
    {
      "epoch": 0.7879123089300081,
      "grad_norm": 14.5625,
      "learning_rate": 1.8424175382139985e-05,
      "loss": 0.8269,
      "step": 15670
    },
    {
      "epoch": 0.7884151246983105,
      "grad_norm": 37.5,
      "learning_rate": 1.842316975060338e-05,
      "loss": 1.2449,
      "step": 15680
    },
    {
      "epoch": 0.788917940466613,
      "grad_norm": 8.25,
      "learning_rate": 1.8422164119066774e-05,
      "loss": 0.943,
      "step": 15690
    },
    {
      "epoch": 0.7894207562349155,
      "grad_norm": 19.25,
      "learning_rate": 1.842115848753017e-05,
      "loss": 1.4987,
      "step": 15700
    },
    {
      "epoch": 0.789923572003218,
      "grad_norm": 40.25,
      "learning_rate": 1.8420152855993565e-05,
      "loss": 1.0566,
      "step": 15710
    },
    {
      "epoch": 0.7904263877715205,
      "grad_norm": 11.3125,
      "learning_rate": 1.841914722445696e-05,
      "loss": 0.9932,
      "step": 15720
    },
    {
      "epoch": 0.790929203539823,
      "grad_norm": 13.875,
      "learning_rate": 1.8418141592920357e-05,
      "loss": 0.747,
      "step": 15730
    },
    {
      "epoch": 0.7914320193081255,
      "grad_norm": 52.25,
      "learning_rate": 1.841713596138375e-05,
      "loss": 1.1804,
      "step": 15740
    },
    {
      "epoch": 0.791934835076428,
      "grad_norm": 7.21875,
      "learning_rate": 1.8416130329847146e-05,
      "loss": 1.0235,
      "step": 15750
    },
    {
      "epoch": 0.7924376508447305,
      "grad_norm": 24.75,
      "learning_rate": 1.841512469831054e-05,
      "loss": 0.9489,
      "step": 15760
    },
    {
      "epoch": 0.792940466613033,
      "grad_norm": 67.5,
      "learning_rate": 1.8414119066773934e-05,
      "loss": 0.8536,
      "step": 15770
    },
    {
      "epoch": 0.7934432823813354,
      "grad_norm": 36.5,
      "learning_rate": 1.841311343523733e-05,
      "loss": 0.9196,
      "step": 15780
    },
    {
      "epoch": 0.793946098149638,
      "grad_norm": 59.5,
      "learning_rate": 1.8412107803700726e-05,
      "loss": 1.2421,
      "step": 15790
    },
    {
      "epoch": 0.7944489139179405,
      "grad_norm": 6.53125,
      "learning_rate": 1.841110217216412e-05,
      "loss": 0.9862,
      "step": 15800
    },
    {
      "epoch": 0.794951729686243,
      "grad_norm": 10.25,
      "learning_rate": 1.8410096540627514e-05,
      "loss": 0.8791,
      "step": 15810
    },
    {
      "epoch": 0.7954545454545454,
      "grad_norm": 30.875,
      "learning_rate": 1.840909090909091e-05,
      "loss": 0.9496,
      "step": 15820
    },
    {
      "epoch": 0.795957361222848,
      "grad_norm": 31.875,
      "learning_rate": 1.8408085277554306e-05,
      "loss": 1.3875,
      "step": 15830
    },
    {
      "epoch": 0.7964601769911505,
      "grad_norm": 5.84375,
      "learning_rate": 1.8407079646017702e-05,
      "loss": 1.1654,
      "step": 15840
    },
    {
      "epoch": 0.7969629927594529,
      "grad_norm": 26.75,
      "learning_rate": 1.8406074014481094e-05,
      "loss": 0.8994,
      "step": 15850
    },
    {
      "epoch": 0.7974658085277554,
      "grad_norm": 60.0,
      "learning_rate": 1.840506838294449e-05,
      "loss": 1.2769,
      "step": 15860
    },
    {
      "epoch": 0.797968624296058,
      "grad_norm": 22.625,
      "learning_rate": 1.8404062751407886e-05,
      "loss": 0.949,
      "step": 15870
    },
    {
      "epoch": 0.7984714400643604,
      "grad_norm": 14.5625,
      "learning_rate": 1.8403057119871282e-05,
      "loss": 1.2673,
      "step": 15880
    },
    {
      "epoch": 0.7989742558326629,
      "grad_norm": 9.4375,
      "learning_rate": 1.8402051488334674e-05,
      "loss": 0.7867,
      "step": 15890
    },
    {
      "epoch": 0.7994770716009654,
      "grad_norm": 88.0,
      "learning_rate": 1.840104585679807e-05,
      "loss": 1.1961,
      "step": 15900
    },
    {
      "epoch": 0.799979887369268,
      "grad_norm": 12.125,
      "learning_rate": 1.8400040225261466e-05,
      "loss": 0.9253,
      "step": 15910
    },
    {
      "epoch": 0.8004827031375704,
      "grad_norm": 83.0,
      "learning_rate": 1.8399034593724862e-05,
      "loss": 1.0468,
      "step": 15920
    },
    {
      "epoch": 0.8009855189058729,
      "grad_norm": 9.75,
      "learning_rate": 1.8398028962188254e-05,
      "loss": 0.9049,
      "step": 15930
    },
    {
      "epoch": 0.8014883346741754,
      "grad_norm": 11.4375,
      "learning_rate": 1.839702333065165e-05,
      "loss": 1.0809,
      "step": 15940
    },
    {
      "epoch": 0.8019911504424779,
      "grad_norm": 13.1875,
      "learning_rate": 1.8396017699115046e-05,
      "loss": 0.9479,
      "step": 15950
    },
    {
      "epoch": 0.8024939662107804,
      "grad_norm": 6.59375,
      "learning_rate": 1.8395012067578442e-05,
      "loss": 0.9082,
      "step": 15960
    },
    {
      "epoch": 0.8029967819790829,
      "grad_norm": 52.5,
      "learning_rate": 1.8394006436041835e-05,
      "loss": 1.0834,
      "step": 15970
    },
    {
      "epoch": 0.8034995977473853,
      "grad_norm": 17.5,
      "learning_rate": 1.839300080450523e-05,
      "loss": 0.9888,
      "step": 15980
    },
    {
      "epoch": 0.8040024135156878,
      "grad_norm": 14.9375,
      "learning_rate": 1.8391995172968626e-05,
      "loss": 0.8679,
      "step": 15990
    },
    {
      "epoch": 0.8045052292839904,
      "grad_norm": 56.75,
      "learning_rate": 1.8390989541432022e-05,
      "loss": 1.0969,
      "step": 16000
    },
    {
      "epoch": 0.8045052292839904,
      "eval_accuracy": 0.5121219633118492,
      "eval_loss": 1.0300852060317993,
      "eval_runtime": 463.0004,
      "eval_samples_per_second": 87.127,
      "eval_steps_per_second": 87.127,
      "step": 16000
    },
    {
      "epoch": 0.8050080450522928,
      "grad_norm": 25.125,
      "learning_rate": 1.8389983909895418e-05,
      "loss": 1.0316,
      "step": 16010
    },
    {
      "epoch": 0.8055108608205953,
      "grad_norm": 10.25,
      "learning_rate": 1.838897827835881e-05,
      "loss": 0.8322,
      "step": 16020
    },
    {
      "epoch": 0.8060136765888978,
      "grad_norm": 88.5,
      "learning_rate": 1.8387972646822206e-05,
      "loss": 1.2985,
      "step": 16030
    },
    {
      "epoch": 0.8065164923572004,
      "grad_norm": 52.5,
      "learning_rate": 1.8386967015285602e-05,
      "loss": 1.0409,
      "step": 16040
    },
    {
      "epoch": 0.8070193081255028,
      "grad_norm": 79.0,
      "learning_rate": 1.8385961383748995e-05,
      "loss": 0.9334,
      "step": 16050
    },
    {
      "epoch": 0.8075221238938053,
      "grad_norm": 13.5625,
      "learning_rate": 1.838495575221239e-05,
      "loss": 1.0365,
      "step": 16060
    },
    {
      "epoch": 0.8080249396621078,
      "grad_norm": 21.125,
      "learning_rate": 1.8383950120675787e-05,
      "loss": 1.1605,
      "step": 16070
    },
    {
      "epoch": 0.8085277554304103,
      "grad_norm": 19.625,
      "learning_rate": 1.838294448913918e-05,
      "loss": 1.0843,
      "step": 16080
    },
    {
      "epoch": 0.8090305711987128,
      "grad_norm": 25.125,
      "learning_rate": 1.838193885760258e-05,
      "loss": 1.1091,
      "step": 16090
    },
    {
      "epoch": 0.8095333869670153,
      "grad_norm": 45.5,
      "learning_rate": 1.838093322606597e-05,
      "loss": 1.0225,
      "step": 16100
    },
    {
      "epoch": 0.8100362027353177,
      "grad_norm": 4.46875,
      "learning_rate": 1.8379927594529367e-05,
      "loss": 1.1937,
      "step": 16110
    },
    {
      "epoch": 0.8105390185036203,
      "grad_norm": 20.75,
      "learning_rate": 1.8378921962992763e-05,
      "loss": 0.9881,
      "step": 16120
    },
    {
      "epoch": 0.8110418342719228,
      "grad_norm": 28.75,
      "learning_rate": 1.8377916331456155e-05,
      "loss": 1.2092,
      "step": 16130
    },
    {
      "epoch": 0.8115446500402252,
      "grad_norm": 14.25,
      "learning_rate": 1.837691069991955e-05,
      "loss": 0.8522,
      "step": 16140
    },
    {
      "epoch": 0.8120474658085277,
      "grad_norm": 48.5,
      "learning_rate": 1.8375905068382947e-05,
      "loss": 0.7155,
      "step": 16150
    },
    {
      "epoch": 0.8125502815768303,
      "grad_norm": 22.0,
      "learning_rate": 1.837489943684634e-05,
      "loss": 0.9676,
      "step": 16160
    },
    {
      "epoch": 0.8130530973451328,
      "grad_norm": 26.75,
      "learning_rate": 1.837389380530974e-05,
      "loss": 0.8515,
      "step": 16170
    },
    {
      "epoch": 0.8135559131134352,
      "grad_norm": 6.6875,
      "learning_rate": 1.837288817377313e-05,
      "loss": 0.8802,
      "step": 16180
    },
    {
      "epoch": 0.8140587288817377,
      "grad_norm": 45.5,
      "learning_rate": 1.8371882542236527e-05,
      "loss": 1.3041,
      "step": 16190
    },
    {
      "epoch": 0.8145615446500403,
      "grad_norm": 40.5,
      "learning_rate": 1.8370876910699923e-05,
      "loss": 0.9752,
      "step": 16200
    },
    {
      "epoch": 0.8150643604183427,
      "grad_norm": 15.1875,
      "learning_rate": 1.8369871279163315e-05,
      "loss": 0.8569,
      "step": 16210
    },
    {
      "epoch": 0.8155671761866452,
      "grad_norm": 50.5,
      "learning_rate": 1.836886564762671e-05,
      "loss": 1.1208,
      "step": 16220
    },
    {
      "epoch": 0.8160699919549477,
      "grad_norm": 11.0625,
      "learning_rate": 1.8367860016090107e-05,
      "loss": 1.0629,
      "step": 16230
    },
    {
      "epoch": 0.8165728077232502,
      "grad_norm": 64.5,
      "learning_rate": 1.83668543845535e-05,
      "loss": 1.1629,
      "step": 16240
    },
    {
      "epoch": 0.8170756234915527,
      "grad_norm": 19.0,
      "learning_rate": 1.83658487530169e-05,
      "loss": 0.7962,
      "step": 16250
    },
    {
      "epoch": 0.8175784392598552,
      "grad_norm": 23.75,
      "learning_rate": 1.836484312148029e-05,
      "loss": 1.5389,
      "step": 16260
    },
    {
      "epoch": 0.8180812550281577,
      "grad_norm": 15.3125,
      "learning_rate": 1.8363837489943687e-05,
      "loss": 0.838,
      "step": 16270
    },
    {
      "epoch": 0.8185840707964602,
      "grad_norm": 35.5,
      "learning_rate": 1.8362831858407083e-05,
      "loss": 0.9173,
      "step": 16280
    },
    {
      "epoch": 0.8190868865647627,
      "grad_norm": 23.0,
      "learning_rate": 1.8361826226870476e-05,
      "loss": 1.2171,
      "step": 16290
    },
    {
      "epoch": 0.8195897023330652,
      "grad_norm": 33.25,
      "learning_rate": 1.836082059533387e-05,
      "loss": 1.1236,
      "step": 16300
    },
    {
      "epoch": 0.8200925181013676,
      "grad_norm": 28.75,
      "learning_rate": 1.8359814963797267e-05,
      "loss": 0.9621,
      "step": 16310
    },
    {
      "epoch": 0.8205953338696702,
      "grad_norm": 3.640625,
      "learning_rate": 1.835880933226066e-05,
      "loss": 1.053,
      "step": 16320
    },
    {
      "epoch": 0.8210981496379727,
      "grad_norm": 41.75,
      "learning_rate": 1.8357803700724056e-05,
      "loss": 1.2255,
      "step": 16330
    },
    {
      "epoch": 0.8216009654062751,
      "grad_norm": 14.1875,
      "learning_rate": 1.835679806918745e-05,
      "loss": 1.0903,
      "step": 16340
    },
    {
      "epoch": 0.8221037811745776,
      "grad_norm": 46.25,
      "learning_rate": 1.8355792437650844e-05,
      "loss": 1.199,
      "step": 16350
    },
    {
      "epoch": 0.8226065969428801,
      "grad_norm": 18.375,
      "learning_rate": 1.8354786806114243e-05,
      "loss": 1.2123,
      "step": 16360
    },
    {
      "epoch": 0.8231094127111827,
      "grad_norm": 31.5,
      "learning_rate": 1.8353781174577636e-05,
      "loss": 1.1022,
      "step": 16370
    },
    {
      "epoch": 0.8236122284794851,
      "grad_norm": 13.0625,
      "learning_rate": 1.8352775543041032e-05,
      "loss": 1.0532,
      "step": 16380
    },
    {
      "epoch": 0.8241150442477876,
      "grad_norm": 27.125,
      "learning_rate": 1.8351769911504428e-05,
      "loss": 0.9988,
      "step": 16390
    },
    {
      "epoch": 0.82461786001609,
      "grad_norm": 18.625,
      "learning_rate": 1.835076427996782e-05,
      "loss": 0.9286,
      "step": 16400
    },
    {
      "epoch": 0.8251206757843926,
      "grad_norm": 5.28125,
      "learning_rate": 1.8349758648431216e-05,
      "loss": 1.0479,
      "step": 16410
    },
    {
      "epoch": 0.8256234915526951,
      "grad_norm": 26.75,
      "learning_rate": 1.8348753016894612e-05,
      "loss": 0.8992,
      "step": 16420
    },
    {
      "epoch": 0.8261263073209976,
      "grad_norm": 35.25,
      "learning_rate": 1.8347747385358004e-05,
      "loss": 0.8922,
      "step": 16430
    },
    {
      "epoch": 0.8266291230893,
      "grad_norm": 5.28125,
      "learning_rate": 1.8346741753821404e-05,
      "loss": 0.6916,
      "step": 16440
    },
    {
      "epoch": 0.8271319388576026,
      "grad_norm": 47.5,
      "learning_rate": 1.8345736122284796e-05,
      "loss": 0.9853,
      "step": 16450
    },
    {
      "epoch": 0.8276347546259051,
      "grad_norm": 25.375,
      "learning_rate": 1.8344730490748192e-05,
      "loss": 0.8593,
      "step": 16460
    },
    {
      "epoch": 0.8281375703942075,
      "grad_norm": 27.25,
      "learning_rate": 1.8343724859211588e-05,
      "loss": 1.0337,
      "step": 16470
    },
    {
      "epoch": 0.82864038616251,
      "grad_norm": 19.75,
      "learning_rate": 1.834271922767498e-05,
      "loss": 1.1097,
      "step": 16480
    },
    {
      "epoch": 0.8291432019308126,
      "grad_norm": 42.75,
      "learning_rate": 1.8341713596138376e-05,
      "loss": 1.3658,
      "step": 16490
    },
    {
      "epoch": 0.8296460176991151,
      "grad_norm": 25.25,
      "learning_rate": 1.8340707964601772e-05,
      "loss": 1.1933,
      "step": 16500
    },
    {
      "epoch": 0.8296460176991151,
      "eval_accuracy": 0.5119236489836391,
      "eval_loss": 1.0301820039749146,
      "eval_runtime": 461.9591,
      "eval_samples_per_second": 87.324,
      "eval_steps_per_second": 87.324,
      "step": 16500
    },
    {
      "epoch": 0.8301488334674175,
      "grad_norm": 48.5,
      "learning_rate": 1.8339702333065165e-05,
      "loss": 1.3353,
      "step": 16510
    },
    {
      "epoch": 0.83065164923572,
      "grad_norm": 16.125,
      "learning_rate": 1.8338696701528564e-05,
      "loss": 0.914,
      "step": 16520
    },
    {
      "epoch": 0.8311544650040226,
      "grad_norm": 16.125,
      "learning_rate": 1.8337691069991956e-05,
      "loss": 0.9255,
      "step": 16530
    },
    {
      "epoch": 0.831657280772325,
      "grad_norm": 26.0,
      "learning_rate": 1.833668543845535e-05,
      "loss": 0.8681,
      "step": 16540
    },
    {
      "epoch": 0.8321600965406275,
      "grad_norm": 16.375,
      "learning_rate": 1.8335679806918748e-05,
      "loss": 1.0199,
      "step": 16550
    },
    {
      "epoch": 0.83266291230893,
      "grad_norm": 60.5,
      "learning_rate": 1.833467417538214e-05,
      "loss": 1.3848,
      "step": 16560
    },
    {
      "epoch": 0.8331657280772325,
      "grad_norm": 63.75,
      "learning_rate": 1.8333668543845536e-05,
      "loss": 0.9827,
      "step": 16570
    },
    {
      "epoch": 0.833668543845535,
      "grad_norm": 11.125,
      "learning_rate": 1.8332662912308932e-05,
      "loss": 0.875,
      "step": 16580
    },
    {
      "epoch": 0.8341713596138375,
      "grad_norm": 5.9375,
      "learning_rate": 1.8331657280772325e-05,
      "loss": 1.0749,
      "step": 16590
    },
    {
      "epoch": 0.83467417538214,
      "grad_norm": 19.75,
      "learning_rate": 1.833065164923572e-05,
      "loss": 1.0424,
      "step": 16600
    },
    {
      "epoch": 0.8351769911504425,
      "grad_norm": 14.5625,
      "learning_rate": 1.8329646017699117e-05,
      "loss": 1.331,
      "step": 16610
    },
    {
      "epoch": 0.835679806918745,
      "grad_norm": 27.5,
      "learning_rate": 1.832864038616251e-05,
      "loss": 1.025,
      "step": 16620
    },
    {
      "epoch": 0.8361826226870475,
      "grad_norm": 15.8125,
      "learning_rate": 1.832763475462591e-05,
      "loss": 0.8597,
      "step": 16630
    },
    {
      "epoch": 0.8366854384553499,
      "grad_norm": 10.375,
      "learning_rate": 1.83266291230893e-05,
      "loss": 1.2479,
      "step": 16640
    },
    {
      "epoch": 0.8371882542236525,
      "grad_norm": 16.375,
      "learning_rate": 1.8325623491552697e-05,
      "loss": 1.3217,
      "step": 16650
    },
    {
      "epoch": 0.837691069991955,
      "grad_norm": 5.90625,
      "learning_rate": 1.8324617860016093e-05,
      "loss": 1.0049,
      "step": 16660
    },
    {
      "epoch": 0.8381938857602574,
      "grad_norm": 43.75,
      "learning_rate": 1.8323612228479485e-05,
      "loss": 1.0463,
      "step": 16670
    },
    {
      "epoch": 0.8386967015285599,
      "grad_norm": 25.5,
      "learning_rate": 1.832260659694288e-05,
      "loss": 1.3906,
      "step": 16680
    },
    {
      "epoch": 0.8391995172968625,
      "grad_norm": 19.375,
      "learning_rate": 1.8321600965406277e-05,
      "loss": 0.9686,
      "step": 16690
    },
    {
      "epoch": 0.839702333065165,
      "grad_norm": 8.4375,
      "learning_rate": 1.832059533386967e-05,
      "loss": 1.2019,
      "step": 16700
    },
    {
      "epoch": 0.8402051488334674,
      "grad_norm": 4.0625,
      "learning_rate": 1.831958970233307e-05,
      "loss": 0.9921,
      "step": 16710
    },
    {
      "epoch": 0.8407079646017699,
      "grad_norm": 13.125,
      "learning_rate": 1.831858407079646e-05,
      "loss": 0.9354,
      "step": 16720
    },
    {
      "epoch": 0.8412107803700724,
      "grad_norm": 15.3125,
      "learning_rate": 1.8317578439259857e-05,
      "loss": 1.0542,
      "step": 16730
    },
    {
      "epoch": 0.8417135961383749,
      "grad_norm": 46.75,
      "learning_rate": 1.8316572807723253e-05,
      "loss": 0.9702,
      "step": 16740
    },
    {
      "epoch": 0.8422164119066774,
      "grad_norm": 20.125,
      "learning_rate": 1.8315567176186645e-05,
      "loss": 0.9636,
      "step": 16750
    },
    {
      "epoch": 0.8427192276749799,
      "grad_norm": 26.875,
      "learning_rate": 1.831456154465004e-05,
      "loss": 0.9355,
      "step": 16760
    },
    {
      "epoch": 0.8432220434432823,
      "grad_norm": 17.625,
      "learning_rate": 1.8313555913113437e-05,
      "loss": 1.1685,
      "step": 16770
    },
    {
      "epoch": 0.8437248592115849,
      "grad_norm": 5.6875,
      "learning_rate": 1.831255028157683e-05,
      "loss": 0.9408,
      "step": 16780
    },
    {
      "epoch": 0.8442276749798874,
      "grad_norm": 31.0,
      "learning_rate": 1.8311544650040225e-05,
      "loss": 1.2311,
      "step": 16790
    },
    {
      "epoch": 0.8447304907481898,
      "grad_norm": 10.8125,
      "learning_rate": 1.831053901850362e-05,
      "loss": 1.2096,
      "step": 16800
    },
    {
      "epoch": 0.8452333065164923,
      "grad_norm": 9.6875,
      "learning_rate": 1.8309533386967017e-05,
      "loss": 1.1112,
      "step": 16810
    },
    {
      "epoch": 0.8457361222847949,
      "grad_norm": 26.625,
      "learning_rate": 1.8308527755430413e-05,
      "loss": 0.9405,
      "step": 16820
    },
    {
      "epoch": 0.8462389380530974,
      "grad_norm": 22.5,
      "learning_rate": 1.8307522123893806e-05,
      "loss": 0.9714,
      "step": 16830
    },
    {
      "epoch": 0.8467417538213998,
      "grad_norm": 22.125,
      "learning_rate": 1.83065164923572e-05,
      "loss": 0.9878,
      "step": 16840
    },
    {
      "epoch": 0.8472445695897023,
      "grad_norm": 3.65625,
      "learning_rate": 1.8305510860820597e-05,
      "loss": 0.9349,
      "step": 16850
    },
    {
      "epoch": 0.8477473853580049,
      "grad_norm": 7.375,
      "learning_rate": 1.830450522928399e-05,
      "loss": 0.968,
      "step": 16860
    },
    {
      "epoch": 0.8482502011263073,
      "grad_norm": 53.0,
      "learning_rate": 1.8303499597747386e-05,
      "loss": 1.1667,
      "step": 16870
    },
    {
      "epoch": 0.8487530168946098,
      "grad_norm": 53.25,
      "learning_rate": 1.830249396621078e-05,
      "loss": 1.1114,
      "step": 16880
    },
    {
      "epoch": 0.8492558326629123,
      "grad_norm": 23.5,
      "learning_rate": 1.8301488334674178e-05,
      "loss": 0.874,
      "step": 16890
    },
    {
      "epoch": 0.8497586484312148,
      "grad_norm": 16.875,
      "learning_rate": 1.8300482703137573e-05,
      "loss": 1.2056,
      "step": 16900
    },
    {
      "epoch": 0.8502614641995173,
      "grad_norm": 7.59375,
      "learning_rate": 1.8299477071600966e-05,
      "loss": 1.0254,
      "step": 16910
    },
    {
      "epoch": 0.8507642799678198,
      "grad_norm": 60.75,
      "learning_rate": 1.8298471440064362e-05,
      "loss": 1.2557,
      "step": 16920
    },
    {
      "epoch": 0.8512670957361222,
      "grad_norm": 10.875,
      "learning_rate": 1.8297465808527758e-05,
      "loss": 1.1131,
      "step": 16930
    },
    {
      "epoch": 0.8517699115044248,
      "grad_norm": 14.3125,
      "learning_rate": 1.829646017699115e-05,
      "loss": 0.8695,
      "step": 16940
    },
    {
      "epoch": 0.8522727272727273,
      "grad_norm": 59.25,
      "learning_rate": 1.8295454545454546e-05,
      "loss": 0.6936,
      "step": 16950
    },
    {
      "epoch": 0.8527755430410298,
      "grad_norm": 6.71875,
      "learning_rate": 1.8294448913917942e-05,
      "loss": 0.8377,
      "step": 16960
    },
    {
      "epoch": 0.8532783588093322,
      "grad_norm": 11.125,
      "learning_rate": 1.8293443282381338e-05,
      "loss": 1.0102,
      "step": 16970
    },
    {
      "epoch": 0.8537811745776348,
      "grad_norm": 14.0625,
      "learning_rate": 1.8292437650844734e-05,
      "loss": 0.9527,
      "step": 16980
    },
    {
      "epoch": 0.8542839903459373,
      "grad_norm": 15.8125,
      "learning_rate": 1.8291432019308126e-05,
      "loss": 0.9743,
      "step": 16990
    },
    {
      "epoch": 0.8547868061142397,
      "grad_norm": 21.375,
      "learning_rate": 1.8290426387771522e-05,
      "loss": 1.1504,
      "step": 17000
    },
    {
      "epoch": 0.8547868061142397,
      "eval_accuracy": 0.5123698562221121,
      "eval_loss": 1.0280652046203613,
      "eval_runtime": 462.4558,
      "eval_samples_per_second": 87.23,
      "eval_steps_per_second": 87.23,
      "step": 17000
    },
    {
      "epoch": 0.8552896218825422,
      "grad_norm": 24.0,
      "learning_rate": 1.8289420756234918e-05,
      "loss": 0.8505,
      "step": 17010
    },
    {
      "epoch": 0.8557924376508448,
      "grad_norm": 28.0,
      "learning_rate": 1.828841512469831e-05,
      "loss": 1.005,
      "step": 17020
    },
    {
      "epoch": 0.8562952534191473,
      "grad_norm": 6.5625,
      "learning_rate": 1.8287409493161706e-05,
      "loss": 0.8718,
      "step": 17030
    },
    {
      "epoch": 0.8567980691874497,
      "grad_norm": 10.5625,
      "learning_rate": 1.8286403861625102e-05,
      "loss": 1.1798,
      "step": 17040
    },
    {
      "epoch": 0.8573008849557522,
      "grad_norm": 20.125,
      "learning_rate": 1.8285398230088498e-05,
      "loss": 0.9529,
      "step": 17050
    },
    {
      "epoch": 0.8578037007240547,
      "grad_norm": 26.5,
      "learning_rate": 1.828439259855189e-05,
      "loss": 1.2492,
      "step": 17060
    },
    {
      "epoch": 0.8583065164923572,
      "grad_norm": 11.8125,
      "learning_rate": 1.8283386967015286e-05,
      "loss": 1.4582,
      "step": 17070
    },
    {
      "epoch": 0.8588093322606597,
      "grad_norm": 19.125,
      "learning_rate": 1.8282381335478682e-05,
      "loss": 0.6868,
      "step": 17080
    },
    {
      "epoch": 0.8593121480289622,
      "grad_norm": 13.125,
      "learning_rate": 1.8281375703942078e-05,
      "loss": 0.9855,
      "step": 17090
    },
    {
      "epoch": 0.8598149637972646,
      "grad_norm": 22.625,
      "learning_rate": 1.828037007240547e-05,
      "loss": 1.0106,
      "step": 17100
    },
    {
      "epoch": 0.8603177795655672,
      "grad_norm": 7.5625,
      "learning_rate": 1.8279364440868867e-05,
      "loss": 1.0971,
      "step": 17110
    },
    {
      "epoch": 0.8608205953338697,
      "grad_norm": 25.25,
      "learning_rate": 1.8278358809332262e-05,
      "loss": 0.9079,
      "step": 17120
    },
    {
      "epoch": 0.8613234111021721,
      "grad_norm": 10.375,
      "learning_rate": 1.8277353177795658e-05,
      "loss": 1.0167,
      "step": 17130
    },
    {
      "epoch": 0.8618262268704746,
      "grad_norm": 12.0625,
      "learning_rate": 1.827634754625905e-05,
      "loss": 0.736,
      "step": 17140
    },
    {
      "epoch": 0.8623290426387772,
      "grad_norm": 5.75,
      "learning_rate": 1.8275341914722447e-05,
      "loss": 1.0892,
      "step": 17150
    },
    {
      "epoch": 0.8628318584070797,
      "grad_norm": 14.25,
      "learning_rate": 1.8274336283185843e-05,
      "loss": 1.3527,
      "step": 17160
    },
    {
      "epoch": 0.8633346741753821,
      "grad_norm": 4.6875,
      "learning_rate": 1.827333065164924e-05,
      "loss": 1.0316,
      "step": 17170
    },
    {
      "epoch": 0.8638374899436846,
      "grad_norm": 42.5,
      "learning_rate": 1.827232502011263e-05,
      "loss": 1.0999,
      "step": 17180
    },
    {
      "epoch": 0.8643403057119872,
      "grad_norm": 20.25,
      "learning_rate": 1.8271319388576027e-05,
      "loss": 1.0031,
      "step": 17190
    },
    {
      "epoch": 0.8648431214802896,
      "grad_norm": 8.5,
      "learning_rate": 1.8270313757039423e-05,
      "loss": 0.9769,
      "step": 17200
    },
    {
      "epoch": 0.8653459372485921,
      "grad_norm": 10.8125,
      "learning_rate": 1.826930812550282e-05,
      "loss": 1.2326,
      "step": 17210
    },
    {
      "epoch": 0.8658487530168946,
      "grad_norm": 22.375,
      "learning_rate": 1.826830249396621e-05,
      "loss": 1.0382,
      "step": 17220
    },
    {
      "epoch": 0.8663515687851971,
      "grad_norm": 12.625,
      "learning_rate": 1.8267296862429607e-05,
      "loss": 0.9285,
      "step": 17230
    },
    {
      "epoch": 0.8668543845534996,
      "grad_norm": 13.875,
      "learning_rate": 1.8266291230893003e-05,
      "loss": 1.0842,
      "step": 17240
    },
    {
      "epoch": 0.8673572003218021,
      "grad_norm": 68.0,
      "learning_rate": 1.82652855993564e-05,
      "loss": 1.1518,
      "step": 17250
    },
    {
      "epoch": 0.8678600160901045,
      "grad_norm": 31.375,
      "learning_rate": 1.826427996781979e-05,
      "loss": 1.4123,
      "step": 17260
    },
    {
      "epoch": 0.8683628318584071,
      "grad_norm": 17.75,
      "learning_rate": 1.8263274336283187e-05,
      "loss": 0.8724,
      "step": 17270
    },
    {
      "epoch": 0.8688656476267096,
      "grad_norm": 20.25,
      "learning_rate": 1.8262268704746583e-05,
      "loss": 1.1069,
      "step": 17280
    },
    {
      "epoch": 0.8693684633950121,
      "grad_norm": 36.5,
      "learning_rate": 1.826126307320998e-05,
      "loss": 0.9083,
      "step": 17290
    },
    {
      "epoch": 0.8698712791633145,
      "grad_norm": 11.6875,
      "learning_rate": 1.826025744167337e-05,
      "loss": 0.9378,
      "step": 17300
    },
    {
      "epoch": 0.8703740949316171,
      "grad_norm": 39.25,
      "learning_rate": 1.8259251810136767e-05,
      "loss": 1.1719,
      "step": 17310
    },
    {
      "epoch": 0.8708769106999196,
      "grad_norm": 11.25,
      "learning_rate": 1.8258246178600163e-05,
      "loss": 0.9659,
      "step": 17320
    },
    {
      "epoch": 0.871379726468222,
      "grad_norm": 11.75,
      "learning_rate": 1.8257240547063556e-05,
      "loss": 1.1138,
      "step": 17330
    },
    {
      "epoch": 0.8718825422365245,
      "grad_norm": 15.875,
      "learning_rate": 1.825623491552695e-05,
      "loss": 1.0494,
      "step": 17340
    },
    {
      "epoch": 0.8723853580048271,
      "grad_norm": 4.03125,
      "learning_rate": 1.8255229283990347e-05,
      "loss": 0.7788,
      "step": 17350
    },
    {
      "epoch": 0.8728881737731295,
      "grad_norm": 18.75,
      "learning_rate": 1.8254223652453743e-05,
      "loss": 1.1675,
      "step": 17360
    },
    {
      "epoch": 0.873390989541432,
      "grad_norm": 18.625,
      "learning_rate": 1.825321802091714e-05,
      "loss": 1.0899,
      "step": 17370
    },
    {
      "epoch": 0.8738938053097345,
      "grad_norm": 53.25,
      "learning_rate": 1.825221238938053e-05,
      "loss": 1.205,
      "step": 17380
    },
    {
      "epoch": 0.8743966210780371,
      "grad_norm": 15.875,
      "learning_rate": 1.8251206757843927e-05,
      "loss": 1.1858,
      "step": 17390
    },
    {
      "epoch": 0.8748994368463395,
      "grad_norm": 13.1875,
      "learning_rate": 1.8250201126307323e-05,
      "loss": 1.0256,
      "step": 17400
    },
    {
      "epoch": 0.875402252614642,
      "grad_norm": 13.75,
      "learning_rate": 1.8249195494770716e-05,
      "loss": 0.8975,
      "step": 17410
    },
    {
      "epoch": 0.8759050683829445,
      "grad_norm": 15.5,
      "learning_rate": 1.824818986323411e-05,
      "loss": 1.012,
      "step": 17420
    },
    {
      "epoch": 0.8764078841512469,
      "grad_norm": 32.75,
      "learning_rate": 1.8247184231697508e-05,
      "loss": 0.7981,
      "step": 17430
    },
    {
      "epoch": 0.8769106999195495,
      "grad_norm": 51.5,
      "learning_rate": 1.8246178600160903e-05,
      "loss": 1.0029,
      "step": 17440
    },
    {
      "epoch": 0.877413515687852,
      "grad_norm": 26.5,
      "learning_rate": 1.82451729686243e-05,
      "loss": 1.0312,
      "step": 17450
    },
    {
      "epoch": 0.8779163314561544,
      "grad_norm": 30.0,
      "learning_rate": 1.8244167337087692e-05,
      "loss": 1.1181,
      "step": 17460
    },
    {
      "epoch": 0.8784191472244569,
      "grad_norm": 52.0,
      "learning_rate": 1.8243161705551088e-05,
      "loss": 1.1908,
      "step": 17470
    },
    {
      "epoch": 0.8789219629927595,
      "grad_norm": 15.5625,
      "learning_rate": 1.8242156074014484e-05,
      "loss": 1.0125,
      "step": 17480
    },
    {
      "epoch": 0.879424778761062,
      "grad_norm": 14.25,
      "learning_rate": 1.8241150442477876e-05,
      "loss": 0.8435,
      "step": 17490
    },
    {
      "epoch": 0.8799275945293644,
      "grad_norm": 18.5,
      "learning_rate": 1.8240144810941272e-05,
      "loss": 1.1229,
      "step": 17500
    },
    {
      "epoch": 0.8799275945293644,
      "eval_accuracy": 0.5119236489836391,
      "eval_loss": 1.0284544229507446,
      "eval_runtime": 462.5487,
      "eval_samples_per_second": 87.212,
      "eval_steps_per_second": 87.212,
      "step": 17500
    },
    {
      "epoch": 0.8804304102976669,
      "grad_norm": 48.5,
      "learning_rate": 1.8239139179404668e-05,
      "loss": 0.91,
      "step": 17510
    },
    {
      "epoch": 0.8809332260659695,
      "grad_norm": 22.75,
      "learning_rate": 1.8238133547868064e-05,
      "loss": 0.9585,
      "step": 17520
    },
    {
      "epoch": 0.8814360418342719,
      "grad_norm": 25.25,
      "learning_rate": 1.823712791633146e-05,
      "loss": 1.0184,
      "step": 17530
    },
    {
      "epoch": 0.8819388576025744,
      "grad_norm": 26.0,
      "learning_rate": 1.8236122284794852e-05,
      "loss": 0.8615,
      "step": 17540
    },
    {
      "epoch": 0.8824416733708769,
      "grad_norm": 21.875,
      "learning_rate": 1.8235116653258248e-05,
      "loss": 1.1638,
      "step": 17550
    },
    {
      "epoch": 0.8829444891391794,
      "grad_norm": 15.375,
      "learning_rate": 1.8234111021721644e-05,
      "loss": 1.1136,
      "step": 17560
    },
    {
      "epoch": 0.8834473049074819,
      "grad_norm": 27.5,
      "learning_rate": 1.8233105390185036e-05,
      "loss": 0.9081,
      "step": 17570
    },
    {
      "epoch": 0.8839501206757844,
      "grad_norm": 35.75,
      "learning_rate": 1.8232099758648432e-05,
      "loss": 0.8577,
      "step": 17580
    },
    {
      "epoch": 0.8844529364440868,
      "grad_norm": 20.75,
      "learning_rate": 1.8231094127111828e-05,
      "loss": 1.1257,
      "step": 17590
    },
    {
      "epoch": 0.8849557522123894,
      "grad_norm": 11.0,
      "learning_rate": 1.823008849557522e-05,
      "loss": 1.0196,
      "step": 17600
    },
    {
      "epoch": 0.8854585679806919,
      "grad_norm": 53.0,
      "learning_rate": 1.822908286403862e-05,
      "loss": 0.9297,
      "step": 17610
    },
    {
      "epoch": 0.8859613837489944,
      "grad_norm": 26.375,
      "learning_rate": 1.8228077232502012e-05,
      "loss": 1.1288,
      "step": 17620
    },
    {
      "epoch": 0.8864641995172968,
      "grad_norm": 31.625,
      "learning_rate": 1.8227071600965408e-05,
      "loss": 1.2786,
      "step": 17630
    },
    {
      "epoch": 0.8869670152855994,
      "grad_norm": 21.5,
      "learning_rate": 1.8226065969428804e-05,
      "loss": 1.1805,
      "step": 17640
    },
    {
      "epoch": 0.8874698310539019,
      "grad_norm": 5.71875,
      "learning_rate": 1.8225060337892197e-05,
      "loss": 1.0016,
      "step": 17650
    },
    {
      "epoch": 0.8879726468222043,
      "grad_norm": 32.25,
      "learning_rate": 1.8224054706355592e-05,
      "loss": 1.0454,
      "step": 17660
    },
    {
      "epoch": 0.8884754625905068,
      "grad_norm": 17.0,
      "learning_rate": 1.822304907481899e-05,
      "loss": 0.8345,
      "step": 17670
    },
    {
      "epoch": 0.8889782783588094,
      "grad_norm": 24.25,
      "learning_rate": 1.822204344328238e-05,
      "loss": 1.0321,
      "step": 17680
    },
    {
      "epoch": 0.8894810941271118,
      "grad_norm": 17.125,
      "learning_rate": 1.822103781174578e-05,
      "loss": 0.8696,
      "step": 17690
    },
    {
      "epoch": 0.8899839098954143,
      "grad_norm": 23.625,
      "learning_rate": 1.8220032180209173e-05,
      "loss": 0.9015,
      "step": 17700
    },
    {
      "epoch": 0.8904867256637168,
      "grad_norm": 16.25,
      "learning_rate": 1.821902654867257e-05,
      "loss": 1.2547,
      "step": 17710
    },
    {
      "epoch": 0.8909895414320194,
      "grad_norm": 57.75,
      "learning_rate": 1.8218020917135964e-05,
      "loss": 1.5312,
      "step": 17720
    },
    {
      "epoch": 0.8914923572003218,
      "grad_norm": 15.4375,
      "learning_rate": 1.8217015285599357e-05,
      "loss": 1.0454,
      "step": 17730
    },
    {
      "epoch": 0.8919951729686243,
      "grad_norm": 55.75,
      "learning_rate": 1.8216009654062753e-05,
      "loss": 1.1093,
      "step": 17740
    },
    {
      "epoch": 0.8924979887369268,
      "grad_norm": 29.5,
      "learning_rate": 1.821500402252615e-05,
      "loss": 1.183,
      "step": 17750
    },
    {
      "epoch": 0.8930008045052292,
      "grad_norm": 31.25,
      "learning_rate": 1.821399839098954e-05,
      "loss": 1.0682,
      "step": 17760
    },
    {
      "epoch": 0.8935036202735318,
      "grad_norm": 30.75,
      "learning_rate": 1.821299275945294e-05,
      "loss": 1.3905,
      "step": 17770
    },
    {
      "epoch": 0.8940064360418343,
      "grad_norm": 6.96875,
      "learning_rate": 1.8211987127916333e-05,
      "loss": 0.8714,
      "step": 17780
    },
    {
      "epoch": 0.8945092518101367,
      "grad_norm": 30.375,
      "learning_rate": 1.821098149637973e-05,
      "loss": 1.4155,
      "step": 17790
    },
    {
      "epoch": 0.8950120675784392,
      "grad_norm": 47.5,
      "learning_rate": 1.8209975864843125e-05,
      "loss": 1.0382,
      "step": 17800
    },
    {
      "epoch": 0.8955148833467418,
      "grad_norm": 14.9375,
      "learning_rate": 1.8208970233306517e-05,
      "loss": 1.1908,
      "step": 17810
    },
    {
      "epoch": 0.8960176991150443,
      "grad_norm": 18.875,
      "learning_rate": 1.8207964601769913e-05,
      "loss": 1.1677,
      "step": 17820
    },
    {
      "epoch": 0.8965205148833467,
      "grad_norm": 12.875,
      "learning_rate": 1.820695897023331e-05,
      "loss": 0.9572,
      "step": 17830
    },
    {
      "epoch": 0.8970233306516492,
      "grad_norm": 59.5,
      "learning_rate": 1.82059533386967e-05,
      "loss": 1.2224,
      "step": 17840
    },
    {
      "epoch": 0.8975261464199518,
      "grad_norm": 24.5,
      "learning_rate": 1.8204947707160097e-05,
      "loss": 1.0429,
      "step": 17850
    },
    {
      "epoch": 0.8980289621882542,
      "grad_norm": 9.875,
      "learning_rate": 1.8203942075623493e-05,
      "loss": 1.0545,
      "step": 17860
    },
    {
      "epoch": 0.8985317779565567,
      "grad_norm": 18.0,
      "learning_rate": 1.8202936444086886e-05,
      "loss": 1.1099,
      "step": 17870
    },
    {
      "epoch": 0.8990345937248592,
      "grad_norm": 29.75,
      "learning_rate": 1.8201930812550285e-05,
      "loss": 1.2311,
      "step": 17880
    },
    {
      "epoch": 0.8995374094931617,
      "grad_norm": 14.625,
      "learning_rate": 1.8200925181013677e-05,
      "loss": 1.1047,
      "step": 17890
    },
    {
      "epoch": 0.9000402252614642,
      "grad_norm": 21.75,
      "learning_rate": 1.8199919549477073e-05,
      "loss": 1.2194,
      "step": 17900
    },
    {
      "epoch": 0.9005430410297667,
      "grad_norm": 11.5,
      "learning_rate": 1.819891391794047e-05,
      "loss": 0.8571,
      "step": 17910
    },
    {
      "epoch": 0.9010458567980691,
      "grad_norm": 50.5,
      "learning_rate": 1.819790828640386e-05,
      "loss": 1.098,
      "step": 17920
    },
    {
      "epoch": 0.9015486725663717,
      "grad_norm": 11.6875,
      "learning_rate": 1.8196902654867257e-05,
      "loss": 0.9747,
      "step": 17930
    },
    {
      "epoch": 0.9020514883346742,
      "grad_norm": 11.6875,
      "learning_rate": 1.8195897023330653e-05,
      "loss": 0.8156,
      "step": 17940
    },
    {
      "epoch": 0.9025543041029767,
      "grad_norm": 15.4375,
      "learning_rate": 1.8194891391794046e-05,
      "loss": 1.0256,
      "step": 17950
    },
    {
      "epoch": 0.9030571198712791,
      "grad_norm": 17.125,
      "learning_rate": 1.8193885760257445e-05,
      "loss": 0.9636,
      "step": 17960
    },
    {
      "epoch": 0.9035599356395817,
      "grad_norm": 11.4375,
      "learning_rate": 1.8192880128720838e-05,
      "loss": 0.7943,
      "step": 17970
    },
    {
      "epoch": 0.9040627514078842,
      "grad_norm": 9.1875,
      "learning_rate": 1.8191874497184233e-05,
      "loss": 0.8051,
      "step": 17980
    },
    {
      "epoch": 0.9045655671761866,
      "grad_norm": 17.25,
      "learning_rate": 1.819086886564763e-05,
      "loss": 0.8141,
      "step": 17990
    },
    {
      "epoch": 0.9050683829444891,
      "grad_norm": 57.25,
      "learning_rate": 1.8189863234111022e-05,
      "loss": 1.0643,
      "step": 18000
    },
    {
      "epoch": 0.9050683829444891,
      "eval_accuracy": 0.5120475954387704,
      "eval_loss": 1.0288959741592407,
      "eval_runtime": 462.556,
      "eval_samples_per_second": 87.211,
      "eval_steps_per_second": 87.211,
      "step": 18000
    },
    {
      "epoch": 0.9055711987127917,
      "grad_norm": 7.90625,
      "learning_rate": 1.8188857602574418e-05,
      "loss": 1.0145,
      "step": 18010
    },
    {
      "epoch": 0.9060740144810941,
      "grad_norm": 5.90625,
      "learning_rate": 1.8187851971037814e-05,
      "loss": 1.0173,
      "step": 18020
    },
    {
      "epoch": 0.9065768302493966,
      "grad_norm": 31.125,
      "learning_rate": 1.8186846339501206e-05,
      "loss": 1.1096,
      "step": 18030
    },
    {
      "epoch": 0.9070796460176991,
      "grad_norm": 10.8125,
      "learning_rate": 1.8185840707964605e-05,
      "loss": 0.7536,
      "step": 18040
    },
    {
      "epoch": 0.9075824617860017,
      "grad_norm": 12.75,
      "learning_rate": 1.8184835076427998e-05,
      "loss": 1.2761,
      "step": 18050
    },
    {
      "epoch": 0.9080852775543041,
      "grad_norm": 19.375,
      "learning_rate": 1.8183829444891394e-05,
      "loss": 0.6576,
      "step": 18060
    },
    {
      "epoch": 0.9085880933226066,
      "grad_norm": 15.75,
      "learning_rate": 1.818282381335479e-05,
      "loss": 0.8616,
      "step": 18070
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 6.5,
      "learning_rate": 1.8181818181818182e-05,
      "loss": 1.0466,
      "step": 18080
    },
    {
      "epoch": 0.9095937248592116,
      "grad_norm": 15.25,
      "learning_rate": 1.8180812550281578e-05,
      "loss": 0.9504,
      "step": 18090
    },
    {
      "epoch": 0.9100965406275141,
      "grad_norm": 27.0,
      "learning_rate": 1.8179806918744974e-05,
      "loss": 1.0633,
      "step": 18100
    },
    {
      "epoch": 0.9105993563958166,
      "grad_norm": 11.125,
      "learning_rate": 1.8178801287208366e-05,
      "loss": 0.9367,
      "step": 18110
    },
    {
      "epoch": 0.911102172164119,
      "grad_norm": 12.8125,
      "learning_rate": 1.8177795655671762e-05,
      "loss": 1.1117,
      "step": 18120
    },
    {
      "epoch": 0.9116049879324215,
      "grad_norm": 18.5,
      "learning_rate": 1.8176790024135158e-05,
      "loss": 0.9848,
      "step": 18130
    },
    {
      "epoch": 0.9121078037007241,
      "grad_norm": 28.625,
      "learning_rate": 1.8175784392598554e-05,
      "loss": 1.1417,
      "step": 18140
    },
    {
      "epoch": 0.9126106194690266,
      "grad_norm": 17.75,
      "learning_rate": 1.817477876106195e-05,
      "loss": 1.1899,
      "step": 18150
    },
    {
      "epoch": 0.913113435237329,
      "grad_norm": 6.40625,
      "learning_rate": 1.8173773129525342e-05,
      "loss": 0.8624,
      "step": 18160
    },
    {
      "epoch": 0.9136162510056315,
      "grad_norm": 26.5,
      "learning_rate": 1.8172767497988738e-05,
      "loss": 1.0761,
      "step": 18170
    },
    {
      "epoch": 0.9141190667739341,
      "grad_norm": 24.75,
      "learning_rate": 1.8171761866452134e-05,
      "loss": 0.9982,
      "step": 18180
    },
    {
      "epoch": 0.9146218825422365,
      "grad_norm": 6.90625,
      "learning_rate": 1.8170756234915527e-05,
      "loss": 0.8917,
      "step": 18190
    },
    {
      "epoch": 0.915124698310539,
      "grad_norm": 17.875,
      "learning_rate": 1.8169750603378922e-05,
      "loss": 0.7385,
      "step": 18200
    },
    {
      "epoch": 0.9156275140788415,
      "grad_norm": 14.9375,
      "learning_rate": 1.816874497184232e-05,
      "loss": 1.0182,
      "step": 18210
    },
    {
      "epoch": 0.916130329847144,
      "grad_norm": 65.5,
      "learning_rate": 1.8167739340305714e-05,
      "loss": 1.3689,
      "step": 18220
    },
    {
      "epoch": 0.9166331456154465,
      "grad_norm": 27.25,
      "learning_rate": 1.816673370876911e-05,
      "loss": 1.3457,
      "step": 18230
    },
    {
      "epoch": 0.917135961383749,
      "grad_norm": 27.5,
      "learning_rate": 1.8165728077232503e-05,
      "loss": 0.8542,
      "step": 18240
    },
    {
      "epoch": 0.9176387771520514,
      "grad_norm": 10.0625,
      "learning_rate": 1.81647224456959e-05,
      "loss": 0.7912,
      "step": 18250
    },
    {
      "epoch": 0.918141592920354,
      "grad_norm": 3.65625,
      "learning_rate": 1.8163716814159294e-05,
      "loss": 1.1645,
      "step": 18260
    },
    {
      "epoch": 0.9186444086886565,
      "grad_norm": 16.125,
      "learning_rate": 1.8162711182622687e-05,
      "loss": 0.7908,
      "step": 18270
    },
    {
      "epoch": 0.919147224456959,
      "grad_norm": 54.25,
      "learning_rate": 1.8161705551086083e-05,
      "loss": 0.9057,
      "step": 18280
    },
    {
      "epoch": 0.9196500402252614,
      "grad_norm": 22.5,
      "learning_rate": 1.816069991954948e-05,
      "loss": 1.2262,
      "step": 18290
    },
    {
      "epoch": 0.920152855993564,
      "grad_norm": 19.25,
      "learning_rate": 1.8159694288012874e-05,
      "loss": 0.9699,
      "step": 18300
    },
    {
      "epoch": 0.9206556717618665,
      "grad_norm": 20.5,
      "learning_rate": 1.815868865647627e-05,
      "loss": 1.0174,
      "step": 18310
    },
    {
      "epoch": 0.9211584875301689,
      "grad_norm": 49.0,
      "learning_rate": 1.8157683024939663e-05,
      "loss": 1.2709,
      "step": 18320
    },
    {
      "epoch": 0.9216613032984714,
      "grad_norm": 10.4375,
      "learning_rate": 1.815667739340306e-05,
      "loss": 0.9239,
      "step": 18330
    },
    {
      "epoch": 0.922164119066774,
      "grad_norm": 37.5,
      "learning_rate": 1.8155671761866455e-05,
      "loss": 1.1873,
      "step": 18340
    },
    {
      "epoch": 0.9226669348350764,
      "grad_norm": 27.75,
      "learning_rate": 1.8154666130329847e-05,
      "loss": 1.3248,
      "step": 18350
    },
    {
      "epoch": 0.9231697506033789,
      "grad_norm": 22.5,
      "learning_rate": 1.8153660498793243e-05,
      "loss": 0.7443,
      "step": 18360
    },
    {
      "epoch": 0.9236725663716814,
      "grad_norm": 3.359375,
      "learning_rate": 1.815265486725664e-05,
      "loss": 0.889,
      "step": 18370
    },
    {
      "epoch": 0.924175382139984,
      "grad_norm": 19.5,
      "learning_rate": 1.8151649235720035e-05,
      "loss": 1.0062,
      "step": 18380
    },
    {
      "epoch": 0.9246781979082864,
      "grad_norm": 31.875,
      "learning_rate": 1.8150643604183427e-05,
      "loss": 1.1468,
      "step": 18390
    },
    {
      "epoch": 0.9251810136765889,
      "grad_norm": 12.875,
      "learning_rate": 1.8149637972646823e-05,
      "loss": 1.0726,
      "step": 18400
    },
    {
      "epoch": 0.9256838294448914,
      "grad_norm": 46.75,
      "learning_rate": 1.814863234111022e-05,
      "loss": 0.909,
      "step": 18410
    },
    {
      "epoch": 0.9261866452131939,
      "grad_norm": 14.0,
      "learning_rate": 1.8147626709573615e-05,
      "loss": 0.8678,
      "step": 18420
    },
    {
      "epoch": 0.9266894609814964,
      "grad_norm": 5.65625,
      "learning_rate": 1.8146621078037007e-05,
      "loss": 0.8827,
      "step": 18430
    },
    {
      "epoch": 0.9271922767497989,
      "grad_norm": 17.5,
      "learning_rate": 1.8145615446500403e-05,
      "loss": 1.2191,
      "step": 18440
    },
    {
      "epoch": 0.9276950925181013,
      "grad_norm": 23.375,
      "learning_rate": 1.81446098149638e-05,
      "loss": 0.8359,
      "step": 18450
    },
    {
      "epoch": 0.9281979082864039,
      "grad_norm": 4.46875,
      "learning_rate": 1.8143604183427195e-05,
      "loss": 1.0543,
      "step": 18460
    },
    {
      "epoch": 0.9287007240547064,
      "grad_norm": 10.3125,
      "learning_rate": 1.8142598551890587e-05,
      "loss": 1.0665,
      "step": 18470
    },
    {
      "epoch": 0.9292035398230089,
      "grad_norm": 9.4375,
      "learning_rate": 1.8141592920353983e-05,
      "loss": 0.9558,
      "step": 18480
    },
    {
      "epoch": 0.9297063555913113,
      "grad_norm": 51.25,
      "learning_rate": 1.814058728881738e-05,
      "loss": 1.1789,
      "step": 18490
    },
    {
      "epoch": 0.9302091713596138,
      "grad_norm": 22.75,
      "learning_rate": 1.8139581657280775e-05,
      "loss": 0.9987,
      "step": 18500
    },
    {
      "epoch": 0.9302091713596138,
      "eval_accuracy": 0.5117997025285077,
      "eval_loss": 1.0290436744689941,
      "eval_runtime": 464.0292,
      "eval_samples_per_second": 86.934,
      "eval_steps_per_second": 86.934,
      "step": 18500
    },
    {
      "epoch": 0.9307119871279164,
      "grad_norm": 8.1875,
      "learning_rate": 1.8138576025744168e-05,
      "loss": 1.0059,
      "step": 18510
    },
    {
      "epoch": 0.9312148028962188,
      "grad_norm": 26.875,
      "learning_rate": 1.8137570394207563e-05,
      "loss": 1.0094,
      "step": 18520
    },
    {
      "epoch": 0.9317176186645213,
      "grad_norm": 69.0,
      "learning_rate": 1.813656476267096e-05,
      "loss": 1.2449,
      "step": 18530
    },
    {
      "epoch": 0.9322204344328238,
      "grad_norm": 44.5,
      "learning_rate": 1.8135559131134355e-05,
      "loss": 1.097,
      "step": 18540
    },
    {
      "epoch": 0.9327232502011263,
      "grad_norm": 29.875,
      "learning_rate": 1.8134553499597748e-05,
      "loss": 1.1332,
      "step": 18550
    },
    {
      "epoch": 0.9332260659694288,
      "grad_norm": 17.625,
      "learning_rate": 1.8133547868061144e-05,
      "loss": 1.2941,
      "step": 18560
    },
    {
      "epoch": 0.9337288817377313,
      "grad_norm": 17.125,
      "learning_rate": 1.813254223652454e-05,
      "loss": 0.959,
      "step": 18570
    },
    {
      "epoch": 0.9342316975060337,
      "grad_norm": 82.0,
      "learning_rate": 1.8131536604987935e-05,
      "loss": 1.1269,
      "step": 18580
    },
    {
      "epoch": 0.9347345132743363,
      "grad_norm": 21.375,
      "learning_rate": 1.8130530973451328e-05,
      "loss": 1.1298,
      "step": 18590
    },
    {
      "epoch": 0.9352373290426388,
      "grad_norm": 32.75,
      "learning_rate": 1.8129525341914724e-05,
      "loss": 0.8219,
      "step": 18600
    },
    {
      "epoch": 0.9357401448109413,
      "grad_norm": 19.5,
      "learning_rate": 1.812851971037812e-05,
      "loss": 1.1532,
      "step": 18610
    },
    {
      "epoch": 0.9362429605792437,
      "grad_norm": 13.8125,
      "learning_rate": 1.8127514078841515e-05,
      "loss": 0.7704,
      "step": 18620
    },
    {
      "epoch": 0.9367457763475463,
      "grad_norm": 12.75,
      "learning_rate": 1.8126508447304908e-05,
      "loss": 1.1767,
      "step": 18630
    },
    {
      "epoch": 0.9372485921158488,
      "grad_norm": 39.5,
      "learning_rate": 1.8125502815768304e-05,
      "loss": 1.1902,
      "step": 18640
    },
    {
      "epoch": 0.9377514078841512,
      "grad_norm": 61.5,
      "learning_rate": 1.81244971842317e-05,
      "loss": 1.3738,
      "step": 18650
    },
    {
      "epoch": 0.9382542236524537,
      "grad_norm": 15.25,
      "learning_rate": 1.8123491552695092e-05,
      "loss": 1.0483,
      "step": 18660
    },
    {
      "epoch": 0.9387570394207563,
      "grad_norm": 14.5625,
      "learning_rate": 1.8122485921158488e-05,
      "loss": 0.7977,
      "step": 18670
    },
    {
      "epoch": 0.9392598551890587,
      "grad_norm": 9.25,
      "learning_rate": 1.8121480289621884e-05,
      "loss": 1.0632,
      "step": 18680
    },
    {
      "epoch": 0.9397626709573612,
      "grad_norm": 26.125,
      "learning_rate": 1.812047465808528e-05,
      "loss": 0.8323,
      "step": 18690
    },
    {
      "epoch": 0.9402654867256637,
      "grad_norm": 18.25,
      "learning_rate": 1.8119469026548676e-05,
      "loss": 0.9594,
      "step": 18700
    },
    {
      "epoch": 0.9407683024939663,
      "grad_norm": 47.25,
      "learning_rate": 1.8118463395012068e-05,
      "loss": 1.0462,
      "step": 18710
    },
    {
      "epoch": 0.9412711182622687,
      "grad_norm": 18.75,
      "learning_rate": 1.8117457763475464e-05,
      "loss": 1.0559,
      "step": 18720
    },
    {
      "epoch": 0.9417739340305712,
      "grad_norm": 18.875,
      "learning_rate": 1.811645213193886e-05,
      "loss": 0.9539,
      "step": 18730
    },
    {
      "epoch": 0.9422767497988737,
      "grad_norm": 9.1875,
      "learning_rate": 1.8115446500402252e-05,
      "loss": 0.9342,
      "step": 18740
    },
    {
      "epoch": 0.9427795655671762,
      "grad_norm": 17.375,
      "learning_rate": 1.811444086886565e-05,
      "loss": 1.3474,
      "step": 18750
    },
    {
      "epoch": 0.9432823813354787,
      "grad_norm": 16.0,
      "learning_rate": 1.8113435237329044e-05,
      "loss": 1.074,
      "step": 18760
    },
    {
      "epoch": 0.9437851971037812,
      "grad_norm": 8.6875,
      "learning_rate": 1.811242960579244e-05,
      "loss": 1.2219,
      "step": 18770
    },
    {
      "epoch": 0.9442880128720836,
      "grad_norm": 19.125,
      "learning_rate": 1.8111423974255836e-05,
      "loss": 1.0072,
      "step": 18780
    },
    {
      "epoch": 0.9447908286403862,
      "grad_norm": 51.75,
      "learning_rate": 1.811041834271923e-05,
      "loss": 1.13,
      "step": 18790
    },
    {
      "epoch": 0.9452936444086887,
      "grad_norm": 23.0,
      "learning_rate": 1.8109412711182624e-05,
      "loss": 0.6863,
      "step": 18800
    },
    {
      "epoch": 0.9457964601769911,
      "grad_norm": 35.25,
      "learning_rate": 1.810840707964602e-05,
      "loss": 1.0483,
      "step": 18810
    },
    {
      "epoch": 0.9462992759452936,
      "grad_norm": 41.75,
      "learning_rate": 1.8107401448109413e-05,
      "loss": 0.9284,
      "step": 18820
    },
    {
      "epoch": 0.9468020917135961,
      "grad_norm": 51.0,
      "learning_rate": 1.810639581657281e-05,
      "loss": 1.1328,
      "step": 18830
    },
    {
      "epoch": 0.9473049074818987,
      "grad_norm": 40.0,
      "learning_rate": 1.8105390185036204e-05,
      "loss": 1.1017,
      "step": 18840
    },
    {
      "epoch": 0.9478077232502011,
      "grad_norm": 5.8125,
      "learning_rate": 1.81043845534996e-05,
      "loss": 0.956,
      "step": 18850
    },
    {
      "epoch": 0.9483105390185036,
      "grad_norm": 16.5,
      "learning_rate": 1.8103378921962996e-05,
      "loss": 0.7215,
      "step": 18860
    },
    {
      "epoch": 0.9488133547868061,
      "grad_norm": 9.25,
      "learning_rate": 1.810237329042639e-05,
      "loss": 0.9559,
      "step": 18870
    },
    {
      "epoch": 0.9493161705551086,
      "grad_norm": 21.625,
      "learning_rate": 1.8101367658889785e-05,
      "loss": 1.0588,
      "step": 18880
    },
    {
      "epoch": 0.9498189863234111,
      "grad_norm": 6.0,
      "learning_rate": 1.810036202735318e-05,
      "loss": 1.5373,
      "step": 18890
    },
    {
      "epoch": 0.9503218020917136,
      "grad_norm": 10.4375,
      "learning_rate": 1.8099356395816573e-05,
      "loss": 0.9312,
      "step": 18900
    },
    {
      "epoch": 0.950824617860016,
      "grad_norm": 20.125,
      "learning_rate": 1.809835076427997e-05,
      "loss": 0.9668,
      "step": 18910
    },
    {
      "epoch": 0.9513274336283186,
      "grad_norm": 15.375,
      "learning_rate": 1.8097345132743365e-05,
      "loss": 1.019,
      "step": 18920
    },
    {
      "epoch": 0.9518302493966211,
      "grad_norm": 44.0,
      "learning_rate": 1.8096339501206757e-05,
      "loss": 1.0835,
      "step": 18930
    },
    {
      "epoch": 0.9523330651649236,
      "grad_norm": 10.0,
      "learning_rate": 1.8095333869670157e-05,
      "loss": 0.7352,
      "step": 18940
    },
    {
      "epoch": 0.952835880933226,
      "grad_norm": 42.25,
      "learning_rate": 1.809432823813355e-05,
      "loss": 1.0419,
      "step": 18950
    },
    {
      "epoch": 0.9533386967015286,
      "grad_norm": 9.5625,
      "learning_rate": 1.8093322606596945e-05,
      "loss": 0.9516,
      "step": 18960
    },
    {
      "epoch": 0.9538415124698311,
      "grad_norm": 56.75,
      "learning_rate": 1.809231697506034e-05,
      "loss": 1.3043,
      "step": 18970
    },
    {
      "epoch": 0.9543443282381335,
      "grad_norm": 8.3125,
      "learning_rate": 1.8091311343523733e-05,
      "loss": 1.2066,
      "step": 18980
    },
    {
      "epoch": 0.954847144006436,
      "grad_norm": 13.75,
      "learning_rate": 1.809030571198713e-05,
      "loss": 1.1852,
      "step": 18990
    },
    {
      "epoch": 0.9553499597747386,
      "grad_norm": 5.75,
      "learning_rate": 1.8089300080450525e-05,
      "loss": 0.6543,
      "step": 19000
    },
    {
      "epoch": 0.9553499597747386,
      "eval_accuracy": 0.5119980168567179,
      "eval_loss": 1.0287611484527588,
      "eval_runtime": 463.8661,
      "eval_samples_per_second": 86.965,
      "eval_steps_per_second": 86.965,
      "step": 19000
    },
    {
      "epoch": 0.955852775543041,
      "grad_norm": 7.15625,
      "learning_rate": 1.8088294448913918e-05,
      "loss": 0.7083,
      "step": 19010
    },
    {
      "epoch": 0.9563555913113435,
      "grad_norm": 6.40625,
      "learning_rate": 1.8087288817377317e-05,
      "loss": 0.9441,
      "step": 19020
    },
    {
      "epoch": 0.956858407079646,
      "grad_norm": 72.5,
      "learning_rate": 1.808628318584071e-05,
      "loss": 1.0899,
      "step": 19030
    },
    {
      "epoch": 0.9573612228479486,
      "grad_norm": 8.375,
      "learning_rate": 1.8085277554304105e-05,
      "loss": 0.9151,
      "step": 19040
    },
    {
      "epoch": 0.957864038616251,
      "grad_norm": 11.9375,
      "learning_rate": 1.80842719227675e-05,
      "loss": 1.2156,
      "step": 19050
    },
    {
      "epoch": 0.9583668543845535,
      "grad_norm": 24.75,
      "learning_rate": 1.8083266291230894e-05,
      "loss": 0.8549,
      "step": 19060
    },
    {
      "epoch": 0.958869670152856,
      "grad_norm": 24.5,
      "learning_rate": 1.808226065969429e-05,
      "loss": 0.8033,
      "step": 19070
    },
    {
      "epoch": 0.9593724859211585,
      "grad_norm": 28.375,
      "learning_rate": 1.8081255028157685e-05,
      "loss": 0.9534,
      "step": 19080
    },
    {
      "epoch": 0.959875301689461,
      "grad_norm": 37.25,
      "learning_rate": 1.8080249396621078e-05,
      "loss": 0.9813,
      "step": 19090
    },
    {
      "epoch": 0.9603781174577635,
      "grad_norm": 17.625,
      "learning_rate": 1.8079243765084477e-05,
      "loss": 1.0775,
      "step": 19100
    },
    {
      "epoch": 0.9608809332260659,
      "grad_norm": 36.0,
      "learning_rate": 1.807823813354787e-05,
      "loss": 0.8017,
      "step": 19110
    },
    {
      "epoch": 0.9613837489943685,
      "grad_norm": 32.5,
      "learning_rate": 1.8077232502011265e-05,
      "loss": 1.029,
      "step": 19120
    },
    {
      "epoch": 0.961886564762671,
      "grad_norm": 13.125,
      "learning_rate": 1.807622687047466e-05,
      "loss": 1.0136,
      "step": 19130
    },
    {
      "epoch": 0.9623893805309734,
      "grad_norm": 22.875,
      "learning_rate": 1.8075221238938054e-05,
      "loss": 1.0102,
      "step": 19140
    },
    {
      "epoch": 0.9628921962992759,
      "grad_norm": 15.3125,
      "learning_rate": 1.807421560740145e-05,
      "loss": 0.9161,
      "step": 19150
    },
    {
      "epoch": 0.9633950120675785,
      "grad_norm": 6.625,
      "learning_rate": 1.8073209975864846e-05,
      "loss": 0.9422,
      "step": 19160
    },
    {
      "epoch": 0.963897827835881,
      "grad_norm": 39.0,
      "learning_rate": 1.8072204344328238e-05,
      "loss": 0.936,
      "step": 19170
    },
    {
      "epoch": 0.9644006436041834,
      "grad_norm": 17.125,
      "learning_rate": 1.8071198712791634e-05,
      "loss": 0.9629,
      "step": 19180
    },
    {
      "epoch": 0.9649034593724859,
      "grad_norm": 24.875,
      "learning_rate": 1.807019308125503e-05,
      "loss": 1.1532,
      "step": 19190
    },
    {
      "epoch": 0.9654062751407884,
      "grad_norm": 6.90625,
      "learning_rate": 1.8069187449718422e-05,
      "loss": 1.2381,
      "step": 19200
    },
    {
      "epoch": 0.9659090909090909,
      "grad_norm": 6.0625,
      "learning_rate": 1.806818181818182e-05,
      "loss": 0.878,
      "step": 19210
    },
    {
      "epoch": 0.9664119066773934,
      "grad_norm": 11.75,
      "learning_rate": 1.8067176186645214e-05,
      "loss": 0.9768,
      "step": 19220
    },
    {
      "epoch": 0.9669147224456959,
      "grad_norm": 39.5,
      "learning_rate": 1.806617055510861e-05,
      "loss": 1.3144,
      "step": 19230
    },
    {
      "epoch": 0.9674175382139983,
      "grad_norm": 19.125,
      "learning_rate": 1.8065164923572006e-05,
      "loss": 0.8789,
      "step": 19240
    },
    {
      "epoch": 0.9679203539823009,
      "grad_norm": 25.0,
      "learning_rate": 1.8064159292035398e-05,
      "loss": 1.0344,
      "step": 19250
    },
    {
      "epoch": 0.9684231697506034,
      "grad_norm": 25.0,
      "learning_rate": 1.8063153660498794e-05,
      "loss": 1.0752,
      "step": 19260
    },
    {
      "epoch": 0.9689259855189059,
      "grad_norm": 10.8125,
      "learning_rate": 1.806214802896219e-05,
      "loss": 1.0643,
      "step": 19270
    },
    {
      "epoch": 0.9694288012872083,
      "grad_norm": 19.125,
      "learning_rate": 1.8061142397425583e-05,
      "loss": 0.9669,
      "step": 19280
    },
    {
      "epoch": 0.9699316170555109,
      "grad_norm": 15.5625,
      "learning_rate": 1.8060136765888982e-05,
      "loss": 0.7741,
      "step": 19290
    },
    {
      "epoch": 0.9704344328238134,
      "grad_norm": 25.125,
      "learning_rate": 1.8059131134352374e-05,
      "loss": 1.0341,
      "step": 19300
    },
    {
      "epoch": 0.9709372485921158,
      "grad_norm": 28.25,
      "learning_rate": 1.805812550281577e-05,
      "loss": 1.0745,
      "step": 19310
    },
    {
      "epoch": 0.9714400643604183,
      "grad_norm": 8.25,
      "learning_rate": 1.8057119871279166e-05,
      "loss": 0.9194,
      "step": 19320
    },
    {
      "epoch": 0.9719428801287209,
      "grad_norm": 29.0,
      "learning_rate": 1.805611423974256e-05,
      "loss": 1.2579,
      "step": 19330
    },
    {
      "epoch": 0.9724456958970233,
      "grad_norm": 16.75,
      "learning_rate": 1.8055108608205954e-05,
      "loss": 1.0866,
      "step": 19340
    },
    {
      "epoch": 0.9729485116653258,
      "grad_norm": 9.5625,
      "learning_rate": 1.805410297666935e-05,
      "loss": 0.8343,
      "step": 19350
    },
    {
      "epoch": 0.9734513274336283,
      "grad_norm": 20.125,
      "learning_rate": 1.8053097345132743e-05,
      "loss": 1.0838,
      "step": 19360
    },
    {
      "epoch": 0.9739541432019309,
      "grad_norm": 46.0,
      "learning_rate": 1.8052091713596142e-05,
      "loss": 1.2646,
      "step": 19370
    },
    {
      "epoch": 0.9744569589702333,
      "grad_norm": 15.125,
      "learning_rate": 1.8051086082059535e-05,
      "loss": 0.9618,
      "step": 19380
    },
    {
      "epoch": 0.9749597747385358,
      "grad_norm": 41.5,
      "learning_rate": 1.805008045052293e-05,
      "loss": 0.9206,
      "step": 19390
    },
    {
      "epoch": 0.9754625905068383,
      "grad_norm": 22.75,
      "learning_rate": 1.8049074818986326e-05,
      "loss": 1.0321,
      "step": 19400
    },
    {
      "epoch": 0.9759654062751408,
      "grad_norm": 19.25,
      "learning_rate": 1.804806918744972e-05,
      "loss": 1.0122,
      "step": 19410
    },
    {
      "epoch": 0.9764682220434433,
      "grad_norm": 30.625,
      "learning_rate": 1.8047063555913115e-05,
      "loss": 1.0318,
      "step": 19420
    },
    {
      "epoch": 0.9769710378117458,
      "grad_norm": 38.5,
      "learning_rate": 1.804605792437651e-05,
      "loss": 1.0234,
      "step": 19430
    },
    {
      "epoch": 0.9774738535800482,
      "grad_norm": 7.1875,
      "learning_rate": 1.8045052292839903e-05,
      "loss": 0.7192,
      "step": 19440
    },
    {
      "epoch": 0.9779766693483508,
      "grad_norm": 62.5,
      "learning_rate": 1.80440466613033e-05,
      "loss": 1.355,
      "step": 19450
    },
    {
      "epoch": 0.9784794851166533,
      "grad_norm": 37.0,
      "learning_rate": 1.8043041029766695e-05,
      "loss": 1.1513,
      "step": 19460
    },
    {
      "epoch": 0.9789823008849557,
      "grad_norm": 20.375,
      "learning_rate": 1.8042035398230087e-05,
      "loss": 0.907,
      "step": 19470
    },
    {
      "epoch": 0.9794851166532582,
      "grad_norm": 40.25,
      "learning_rate": 1.8041029766693487e-05,
      "loss": 0.9807,
      "step": 19480
    },
    {
      "epoch": 0.9799879324215608,
      "grad_norm": 27.375,
      "learning_rate": 1.804002413515688e-05,
      "loss": 0.8325,
      "step": 19490
    },
    {
      "epoch": 0.9804907481898633,
      "grad_norm": 9.375,
      "learning_rate": 1.8039018503620275e-05,
      "loss": 0.9498,
      "step": 19500
    },
    {
      "epoch": 0.9804907481898633,
      "eval_accuracy": 0.5127169062964799,
      "eval_loss": 1.0279489755630493,
      "eval_runtime": 464.7726,
      "eval_samples_per_second": 86.795,
      "eval_steps_per_second": 86.795,
      "step": 19500
    },
    {
      "epoch": 0.9809935639581657,
      "grad_norm": 16.125,
      "learning_rate": 1.803801287208367e-05,
      "loss": 0.8838,
      "step": 19510
    },
    {
      "epoch": 0.9814963797264682,
      "grad_norm": 62.5,
      "learning_rate": 1.8037007240547063e-05,
      "loss": 1.3944,
      "step": 19520
    },
    {
      "epoch": 0.9819991954947708,
      "grad_norm": 16.875,
      "learning_rate": 1.803600160901046e-05,
      "loss": 0.6796,
      "step": 19530
    },
    {
      "epoch": 0.9825020112630732,
      "grad_norm": 15.0,
      "learning_rate": 1.8034995977473855e-05,
      "loss": 1.0838,
      "step": 19540
    },
    {
      "epoch": 0.9830048270313757,
      "grad_norm": 14.6875,
      "learning_rate": 1.803399034593725e-05,
      "loss": 1.2927,
      "step": 19550
    },
    {
      "epoch": 0.9835076427996782,
      "grad_norm": 19.0,
      "learning_rate": 1.8032984714400647e-05,
      "loss": 0.9657,
      "step": 19560
    },
    {
      "epoch": 0.9840104585679806,
      "grad_norm": 33.0,
      "learning_rate": 1.803197908286404e-05,
      "loss": 0.8857,
      "step": 19570
    },
    {
      "epoch": 0.9845132743362832,
      "grad_norm": 25.875,
      "learning_rate": 1.8030973451327435e-05,
      "loss": 1.0286,
      "step": 19580
    },
    {
      "epoch": 0.9850160901045857,
      "grad_norm": 26.625,
      "learning_rate": 1.802996781979083e-05,
      "loss": 1.124,
      "step": 19590
    },
    {
      "epoch": 0.9855189058728882,
      "grad_norm": 5.5625,
      "learning_rate": 1.8028962188254224e-05,
      "loss": 1.153,
      "step": 19600
    },
    {
      "epoch": 0.9860217216411906,
      "grad_norm": 21.625,
      "learning_rate": 1.802795655671762e-05,
      "loss": 0.9554,
      "step": 19610
    },
    {
      "epoch": 0.9865245374094932,
      "grad_norm": 11.75,
      "learning_rate": 1.8026950925181015e-05,
      "loss": 0.7725,
      "step": 19620
    },
    {
      "epoch": 0.9870273531777957,
      "grad_norm": 9.9375,
      "learning_rate": 1.802594529364441e-05,
      "loss": 0.9303,
      "step": 19630
    },
    {
      "epoch": 0.9875301689460981,
      "grad_norm": 26.75,
      "learning_rate": 1.8024939662107807e-05,
      "loss": 1.1122,
      "step": 19640
    },
    {
      "epoch": 0.9880329847144006,
      "grad_norm": 36.5,
      "learning_rate": 1.80239340305712e-05,
      "loss": 0.9611,
      "step": 19650
    },
    {
      "epoch": 0.9885358004827032,
      "grad_norm": 75.0,
      "learning_rate": 1.8022928399034595e-05,
      "loss": 0.9523,
      "step": 19660
    },
    {
      "epoch": 0.9890386162510056,
      "grad_norm": 12.375,
      "learning_rate": 1.802192276749799e-05,
      "loss": 0.8791,
      "step": 19670
    },
    {
      "epoch": 0.9895414320193081,
      "grad_norm": 8.3125,
      "learning_rate": 1.8020917135961384e-05,
      "loss": 0.7772,
      "step": 19680
    },
    {
      "epoch": 0.9900442477876106,
      "grad_norm": 11.0625,
      "learning_rate": 1.801991150442478e-05,
      "loss": 0.6931,
      "step": 19690
    },
    {
      "epoch": 0.9905470635559132,
      "grad_norm": 52.75,
      "learning_rate": 1.8018905872888176e-05,
      "loss": 1.2384,
      "step": 19700
    },
    {
      "epoch": 0.9910498793242156,
      "grad_norm": 29.125,
      "learning_rate": 1.801790024135157e-05,
      "loss": 1.0724,
      "step": 19710
    },
    {
      "epoch": 0.9915526950925181,
      "grad_norm": 22.625,
      "learning_rate": 1.8016894609814964e-05,
      "loss": 0.8798,
      "step": 19720
    },
    {
      "epoch": 0.9920555108608206,
      "grad_norm": 53.0,
      "learning_rate": 1.801588897827836e-05,
      "loss": 1.164,
      "step": 19730
    },
    {
      "epoch": 0.9925583266291231,
      "grad_norm": 11.0625,
      "learning_rate": 1.8014883346741756e-05,
      "loss": 0.9659,
      "step": 19740
    },
    {
      "epoch": 0.9930611423974256,
      "grad_norm": 9.9375,
      "learning_rate": 1.801387771520515e-05,
      "loss": 0.8944,
      "step": 19750
    },
    {
      "epoch": 0.9935639581657281,
      "grad_norm": 6.53125,
      "learning_rate": 1.8012872083668544e-05,
      "loss": 1.1573,
      "step": 19760
    },
    {
      "epoch": 0.9940667739340305,
      "grad_norm": 24.0,
      "learning_rate": 1.801186645213194e-05,
      "loss": 0.9801,
      "step": 19770
    },
    {
      "epoch": 0.9945695897023331,
      "grad_norm": 39.0,
      "learning_rate": 1.8010860820595336e-05,
      "loss": 0.988,
      "step": 19780
    },
    {
      "epoch": 0.9950724054706356,
      "grad_norm": 29.625,
      "learning_rate": 1.800985518905873e-05,
      "loss": 1.1283,
      "step": 19790
    },
    {
      "epoch": 0.995575221238938,
      "grad_norm": 8.4375,
      "learning_rate": 1.8008849557522124e-05,
      "loss": 0.9538,
      "step": 19800
    },
    {
      "epoch": 0.9960780370072405,
      "grad_norm": 23.875,
      "learning_rate": 1.800784392598552e-05,
      "loss": 1.1283,
      "step": 19810
    },
    {
      "epoch": 0.9965808527755431,
      "grad_norm": 27.75,
      "learning_rate": 1.8006838294448916e-05,
      "loss": 1.3719,
      "step": 19820
    },
    {
      "epoch": 0.9970836685438456,
      "grad_norm": 18.75,
      "learning_rate": 1.8005832662912312e-05,
      "loss": 1.0065,
      "step": 19830
    },
    {
      "epoch": 0.997586484312148,
      "grad_norm": 9.875,
      "learning_rate": 1.8004827031375704e-05,
      "loss": 0.8001,
      "step": 19840
    },
    {
      "epoch": 0.9980893000804505,
      "grad_norm": 23.625,
      "learning_rate": 1.80038213998391e-05,
      "loss": 0.838,
      "step": 19850
    },
    {
      "epoch": 0.9985921158487531,
      "grad_norm": 10.875,
      "learning_rate": 1.8002815768302496e-05,
      "loss": 0.9935,
      "step": 19860
    },
    {
      "epoch": 0.9990949316170555,
      "grad_norm": 21.5,
      "learning_rate": 1.8001810136765892e-05,
      "loss": 0.9495,
      "step": 19870
    },
    {
      "epoch": 0.999597747385358,
      "grad_norm": 21.0,
      "learning_rate": 1.8000804505229284e-05,
      "loss": 1.3054,
      "step": 19880
    },
    {
      "epoch": 1.0001005631536606,
      "grad_norm": 12.8125,
      "learning_rate": 1.799979887369268e-05,
      "loss": 0.9558,
      "step": 19890
    },
    {
      "epoch": 1.000603378921963,
      "grad_norm": 23.5,
      "learning_rate": 1.7998793242156076e-05,
      "loss": 0.9212,
      "step": 19900
    },
    {
      "epoch": 1.0011061946902655,
      "grad_norm": 36.5,
      "learning_rate": 1.7997787610619472e-05,
      "loss": 1.2904,
      "step": 19910
    },
    {
      "epoch": 1.001609010458568,
      "grad_norm": 17.625,
      "learning_rate": 1.7996781979082865e-05,
      "loss": 0.8919,
      "step": 19920
    },
    {
      "epoch": 1.0021118262268705,
      "grad_norm": 52.0,
      "learning_rate": 1.799577634754626e-05,
      "loss": 1.0397,
      "step": 19930
    },
    {
      "epoch": 1.002614641995173,
      "grad_norm": 12.4375,
      "learning_rate": 1.7994770716009656e-05,
      "loss": 0.9062,
      "step": 19940
    },
    {
      "epoch": 1.0031174577634754,
      "grad_norm": 20.5,
      "learning_rate": 1.7993765084473052e-05,
      "loss": 1.1182,
      "step": 19950
    },
    {
      "epoch": 1.0036202735317779,
      "grad_norm": 12.8125,
      "learning_rate": 1.7992759452936445e-05,
      "loss": 1.035,
      "step": 19960
    },
    {
      "epoch": 1.0041230893000805,
      "grad_norm": 26.5,
      "learning_rate": 1.799175382139984e-05,
      "loss": 0.8186,
      "step": 19970
    },
    {
      "epoch": 1.004625905068383,
      "grad_norm": 6.03125,
      "learning_rate": 1.7990748189863236e-05,
      "loss": 0.7744,
      "step": 19980
    },
    {
      "epoch": 1.0051287208366855,
      "grad_norm": 30.25,
      "learning_rate": 1.798974255832663e-05,
      "loss": 0.9524,
      "step": 19990
    },
    {
      "epoch": 1.005631536604988,
      "grad_norm": 41.0,
      "learning_rate": 1.7988736926790025e-05,
      "loss": 1.1021,
      "step": 20000
    },
    {
      "epoch": 1.005631536604988,
      "eval_accuracy": 0.5122706990580069,
      "eval_loss": 1.0278122425079346,
      "eval_runtime": 464.7435,
      "eval_samples_per_second": 86.801,
      "eval_steps_per_second": 86.801,
      "step": 20000
    },
    {
      "epoch": 1.0061343523732904,
      "grad_norm": 16.125,
      "learning_rate": 1.798773129525342e-05,
      "loss": 0.8576,
      "step": 20010
    },
    {
      "epoch": 1.0066371681415929,
      "grad_norm": 16.625,
      "learning_rate": 1.7986725663716817e-05,
      "loss": 0.8863,
      "step": 20020
    },
    {
      "epoch": 1.0071399839098953,
      "grad_norm": 14.9375,
      "learning_rate": 1.7985720032180212e-05,
      "loss": 1.0501,
      "step": 20030
    },
    {
      "epoch": 1.0076427996781978,
      "grad_norm": 18.375,
      "learning_rate": 1.7984714400643605e-05,
      "loss": 1.1526,
      "step": 20040
    },
    {
      "epoch": 1.0081456154465005,
      "grad_norm": 14.0,
      "learning_rate": 1.7983708769107e-05,
      "loss": 1.2451,
      "step": 20050
    },
    {
      "epoch": 1.008648431214803,
      "grad_norm": 31.375,
      "learning_rate": 1.7982703137570397e-05,
      "loss": 0.9783,
      "step": 20060
    },
    {
      "epoch": 1.0091512469831054,
      "grad_norm": 27.375,
      "learning_rate": 1.798169750603379e-05,
      "loss": 0.6951,
      "step": 20070
    },
    {
      "epoch": 1.009654062751408,
      "grad_norm": 7.03125,
      "learning_rate": 1.7980691874497185e-05,
      "loss": 1.0121,
      "step": 20080
    },
    {
      "epoch": 1.0101568785197104,
      "grad_norm": 23.125,
      "learning_rate": 1.797968624296058e-05,
      "loss": 1.0822,
      "step": 20090
    },
    {
      "epoch": 1.0106596942880128,
      "grad_norm": 14.1875,
      "learning_rate": 1.7978680611423977e-05,
      "loss": 1.1156,
      "step": 20100
    },
    {
      "epoch": 1.0111625100563153,
      "grad_norm": 56.25,
      "learning_rate": 1.7977674979887373e-05,
      "loss": 1.0195,
      "step": 20110
    },
    {
      "epoch": 1.0116653258246178,
      "grad_norm": 9.1875,
      "learning_rate": 1.7976669348350765e-05,
      "loss": 0.891,
      "step": 20120
    },
    {
      "epoch": 1.0121681415929205,
      "grad_norm": 33.5,
      "learning_rate": 1.797566371681416e-05,
      "loss": 0.9067,
      "step": 20130
    },
    {
      "epoch": 1.012670957361223,
      "grad_norm": 29.75,
      "learning_rate": 1.7974658085277557e-05,
      "loss": 1.0456,
      "step": 20140
    },
    {
      "epoch": 1.0131737731295254,
      "grad_norm": 4.28125,
      "learning_rate": 1.797365245374095e-05,
      "loss": 0.9581,
      "step": 20150
    },
    {
      "epoch": 1.0136765888978279,
      "grad_norm": 12.875,
      "learning_rate": 1.7972646822204345e-05,
      "loss": 0.9097,
      "step": 20160
    },
    {
      "epoch": 1.0141794046661303,
      "grad_norm": 32.25,
      "learning_rate": 1.797164119066774e-05,
      "loss": 1.026,
      "step": 20170
    },
    {
      "epoch": 1.0146822204344328,
      "grad_norm": 11.1875,
      "learning_rate": 1.7970635559131137e-05,
      "loss": 0.8718,
      "step": 20180
    },
    {
      "epoch": 1.0151850362027353,
      "grad_norm": 61.5,
      "learning_rate": 1.7969629927594533e-05,
      "loss": 1.1184,
      "step": 20190
    },
    {
      "epoch": 1.0156878519710377,
      "grad_norm": 3.4375,
      "learning_rate": 1.7968624296057925e-05,
      "loss": 1.0073,
      "step": 20200
    },
    {
      "epoch": 1.0161906677393404,
      "grad_norm": 14.4375,
      "learning_rate": 1.796761866452132e-05,
      "loss": 0.9546,
      "step": 20210
    },
    {
      "epoch": 1.0166934835076429,
      "grad_norm": 30.75,
      "learning_rate": 1.7966613032984717e-05,
      "loss": 0.8237,
      "step": 20220
    },
    {
      "epoch": 1.0171962992759453,
      "grad_norm": 8.375,
      "learning_rate": 1.796560740144811e-05,
      "loss": 0.9486,
      "step": 20230
    },
    {
      "epoch": 1.0176991150442478,
      "grad_norm": 52.0,
      "learning_rate": 1.7964601769911506e-05,
      "loss": 1.0439,
      "step": 20240
    },
    {
      "epoch": 1.0182019308125503,
      "grad_norm": 13.4375,
      "learning_rate": 1.79635961383749e-05,
      "loss": 1.0821,
      "step": 20250
    },
    {
      "epoch": 1.0187047465808527,
      "grad_norm": 42.5,
      "learning_rate": 1.7962590506838294e-05,
      "loss": 1.1346,
      "step": 20260
    },
    {
      "epoch": 1.0192075623491552,
      "grad_norm": 80.0,
      "learning_rate": 1.7961584875301693e-05,
      "loss": 1.2817,
      "step": 20270
    },
    {
      "epoch": 1.0197103781174577,
      "grad_norm": 73.0,
      "learning_rate": 1.7960579243765086e-05,
      "loss": 0.9632,
      "step": 20280
    },
    {
      "epoch": 1.0202131938857602,
      "grad_norm": 20.375,
      "learning_rate": 1.795957361222848e-05,
      "loss": 0.7583,
      "step": 20290
    },
    {
      "epoch": 1.0207160096540628,
      "grad_norm": 32.5,
      "learning_rate": 1.7958567980691877e-05,
      "loss": 1.0943,
      "step": 20300
    },
    {
      "epoch": 1.0212188254223653,
      "grad_norm": 19.875,
      "learning_rate": 1.795756234915527e-05,
      "loss": 1.2049,
      "step": 20310
    },
    {
      "epoch": 1.0217216411906678,
      "grad_norm": 86.0,
      "learning_rate": 1.7956556717618666e-05,
      "loss": 0.9441,
      "step": 20320
    },
    {
      "epoch": 1.0222244569589702,
      "grad_norm": 16.875,
      "learning_rate": 1.7955551086082062e-05,
      "loss": 0.9337,
      "step": 20330
    },
    {
      "epoch": 1.0227272727272727,
      "grad_norm": 16.25,
      "learning_rate": 1.7954545454545454e-05,
      "loss": 1.2926,
      "step": 20340
    },
    {
      "epoch": 1.0232300884955752,
      "grad_norm": 54.0,
      "learning_rate": 1.7953539823008853e-05,
      "loss": 1.3617,
      "step": 20350
    },
    {
      "epoch": 1.0237329042638776,
      "grad_norm": 4.125,
      "learning_rate": 1.7952534191472246e-05,
      "loss": 0.8597,
      "step": 20360
    },
    {
      "epoch": 1.02423572003218,
      "grad_norm": 6.9375,
      "learning_rate": 1.7951528559935642e-05,
      "loss": 1.1577,
      "step": 20370
    },
    {
      "epoch": 1.0247385358004828,
      "grad_norm": 28.375,
      "learning_rate": 1.7950522928399038e-05,
      "loss": 1.203,
      "step": 20380
    },
    {
      "epoch": 1.0252413515687853,
      "grad_norm": 31.625,
      "learning_rate": 1.794951729686243e-05,
      "loss": 1.1522,
      "step": 20390
    },
    {
      "epoch": 1.0257441673370877,
      "grad_norm": 88.5,
      "learning_rate": 1.7948511665325826e-05,
      "loss": 1.1883,
      "step": 20400
    },
    {
      "epoch": 1.0262469831053902,
      "grad_norm": 27.125,
      "learning_rate": 1.7947506033789222e-05,
      "loss": 0.9953,
      "step": 20410
    },
    {
      "epoch": 1.0267497988736927,
      "grad_norm": 39.75,
      "learning_rate": 1.7946500402252614e-05,
      "loss": 1.2923,
      "step": 20420
    },
    {
      "epoch": 1.0272526146419951,
      "grad_norm": 11.9375,
      "learning_rate": 1.7945494770716014e-05,
      "loss": 0.7893,
      "step": 20430
    },
    {
      "epoch": 1.0277554304102976,
      "grad_norm": 13.5,
      "learning_rate": 1.7944489139179406e-05,
      "loss": 1.0324,
      "step": 20440
    },
    {
      "epoch": 1.0282582461786,
      "grad_norm": 24.0,
      "learning_rate": 1.7943483507642802e-05,
      "loss": 1.0502,
      "step": 20450
    },
    {
      "epoch": 1.0287610619469028,
      "grad_norm": 38.75,
      "learning_rate": 1.7942477876106198e-05,
      "loss": 0.8382,
      "step": 20460
    },
    {
      "epoch": 1.0292638777152052,
      "grad_norm": 17.875,
      "learning_rate": 1.794147224456959e-05,
      "loss": 1.1179,
      "step": 20470
    },
    {
      "epoch": 1.0297666934835077,
      "grad_norm": 14.4375,
      "learning_rate": 1.7940466613032986e-05,
      "loss": 1.0102,
      "step": 20480
    },
    {
      "epoch": 1.0302695092518102,
      "grad_norm": 6.84375,
      "learning_rate": 1.7939460981496382e-05,
      "loss": 0.8806,
      "step": 20490
    },
    {
      "epoch": 1.0307723250201126,
      "grad_norm": 28.125,
      "learning_rate": 1.7938455349959775e-05,
      "loss": 0.8708,
      "step": 20500
    },
    {
      "epoch": 1.0307723250201126,
      "eval_accuracy": 0.5117253346554289,
      "eval_loss": 1.0275936126708984,
      "eval_runtime": 465.1747,
      "eval_samples_per_second": 86.72,
      "eval_steps_per_second": 86.72,
      "step": 20500
    },
    {
      "epoch": 1.031275140788415,
      "grad_norm": 19.25,
      "learning_rate": 1.793744971842317e-05,
      "loss": 0.879,
      "step": 20510
    },
    {
      "epoch": 1.0317779565567176,
      "grad_norm": 52.5,
      "learning_rate": 1.7936444086886566e-05,
      "loss": 0.9911,
      "step": 20520
    },
    {
      "epoch": 1.03228077232502,
      "grad_norm": 13.125,
      "learning_rate": 1.793543845534996e-05,
      "loss": 1.0048,
      "step": 20530
    },
    {
      "epoch": 1.0327835880933227,
      "grad_norm": 13.75,
      "learning_rate": 1.7934432823813358e-05,
      "loss": 1.1395,
      "step": 20540
    },
    {
      "epoch": 1.0332864038616252,
      "grad_norm": 26.375,
      "learning_rate": 1.793342719227675e-05,
      "loss": 1.181,
      "step": 20550
    },
    {
      "epoch": 1.0337892196299276,
      "grad_norm": 25.125,
      "learning_rate": 1.7932421560740147e-05,
      "loss": 1.117,
      "step": 20560
    },
    {
      "epoch": 1.0342920353982301,
      "grad_norm": 12.1875,
      "learning_rate": 1.7931415929203542e-05,
      "loss": 0.848,
      "step": 20570
    },
    {
      "epoch": 1.0347948511665326,
      "grad_norm": 23.0,
      "learning_rate": 1.7930410297666935e-05,
      "loss": 0.7857,
      "step": 20580
    },
    {
      "epoch": 1.035297666934835,
      "grad_norm": 8.3125,
      "learning_rate": 1.792940466613033e-05,
      "loss": 1.0403,
      "step": 20590
    },
    {
      "epoch": 1.0358004827031375,
      "grad_norm": 7.21875,
      "learning_rate": 1.7928399034593727e-05,
      "loss": 1.1314,
      "step": 20600
    },
    {
      "epoch": 1.03630329847144,
      "grad_norm": 34.25,
      "learning_rate": 1.792739340305712e-05,
      "loss": 0.8839,
      "step": 20610
    },
    {
      "epoch": 1.0368061142397424,
      "grad_norm": 17.125,
      "learning_rate": 1.792638777152052e-05,
      "loss": 1.2083,
      "step": 20620
    },
    {
      "epoch": 1.0373089300080451,
      "grad_norm": 17.0,
      "learning_rate": 1.792538213998391e-05,
      "loss": 1.0208,
      "step": 20630
    },
    {
      "epoch": 1.0378117457763476,
      "grad_norm": 8.8125,
      "learning_rate": 1.7924376508447307e-05,
      "loss": 0.9831,
      "step": 20640
    },
    {
      "epoch": 1.03831456154465,
      "grad_norm": 9.375,
      "learning_rate": 1.7923370876910703e-05,
      "loss": 1.0773,
      "step": 20650
    },
    {
      "epoch": 1.0388173773129525,
      "grad_norm": 10.9375,
      "learning_rate": 1.7922365245374095e-05,
      "loss": 0.6951,
      "step": 20660
    },
    {
      "epoch": 1.039320193081255,
      "grad_norm": 38.75,
      "learning_rate": 1.792135961383749e-05,
      "loss": 1.0535,
      "step": 20670
    },
    {
      "epoch": 1.0398230088495575,
      "grad_norm": 18.625,
      "learning_rate": 1.7920353982300887e-05,
      "loss": 0.7994,
      "step": 20680
    },
    {
      "epoch": 1.04032582461786,
      "grad_norm": 95.5,
      "learning_rate": 1.791934835076428e-05,
      "loss": 1.2618,
      "step": 20690
    },
    {
      "epoch": 1.0408286403861624,
      "grad_norm": 18.75,
      "learning_rate": 1.791834271922768e-05,
      "loss": 1.2127,
      "step": 20700
    },
    {
      "epoch": 1.041331456154465,
      "grad_norm": 9.0625,
      "learning_rate": 1.791733708769107e-05,
      "loss": 0.9537,
      "step": 20710
    },
    {
      "epoch": 1.0418342719227676,
      "grad_norm": 34.75,
      "learning_rate": 1.7916331456154467e-05,
      "loss": 1.252,
      "step": 20720
    },
    {
      "epoch": 1.04233708769107,
      "grad_norm": 17.625,
      "learning_rate": 1.7915325824617863e-05,
      "loss": 1.0437,
      "step": 20730
    },
    {
      "epoch": 1.0428399034593725,
      "grad_norm": 12.4375,
      "learning_rate": 1.7914320193081255e-05,
      "loss": 1.047,
      "step": 20740
    },
    {
      "epoch": 1.043342719227675,
      "grad_norm": 11.5625,
      "learning_rate": 1.791331456154465e-05,
      "loss": 1.0491,
      "step": 20750
    },
    {
      "epoch": 1.0438455349959774,
      "grad_norm": 27.75,
      "learning_rate": 1.7912308930008047e-05,
      "loss": 0.8504,
      "step": 20760
    },
    {
      "epoch": 1.04434835076428,
      "grad_norm": 5.71875,
      "learning_rate": 1.791130329847144e-05,
      "loss": 0.8787,
      "step": 20770
    },
    {
      "epoch": 1.0448511665325824,
      "grad_norm": 20.25,
      "learning_rate": 1.7910297666934836e-05,
      "loss": 0.9655,
      "step": 20780
    },
    {
      "epoch": 1.045353982300885,
      "grad_norm": 21.625,
      "learning_rate": 1.790929203539823e-05,
      "loss": 1.0036,
      "step": 20790
    },
    {
      "epoch": 1.0458567980691875,
      "grad_norm": 34.25,
      "learning_rate": 1.7908286403861624e-05,
      "loss": 0.8298,
      "step": 20800
    },
    {
      "epoch": 1.04635961383749,
      "grad_norm": 58.25,
      "learning_rate": 1.7907280772325023e-05,
      "loss": 0.8104,
      "step": 20810
    },
    {
      "epoch": 1.0468624296057925,
      "grad_norm": 6.1875,
      "learning_rate": 1.7906275140788416e-05,
      "loss": 0.8737,
      "step": 20820
    },
    {
      "epoch": 1.047365245374095,
      "grad_norm": 46.75,
      "learning_rate": 1.790526950925181e-05,
      "loss": 0.8487,
      "step": 20830
    },
    {
      "epoch": 1.0478680611423974,
      "grad_norm": 5.59375,
      "learning_rate": 1.7904263877715207e-05,
      "loss": 1.2296,
      "step": 20840
    },
    {
      "epoch": 1.0483708769106999,
      "grad_norm": 56.5,
      "learning_rate": 1.79032582461786e-05,
      "loss": 1.2816,
      "step": 20850
    },
    {
      "epoch": 1.0488736926790023,
      "grad_norm": 64.5,
      "learning_rate": 1.7902252614641996e-05,
      "loss": 0.915,
      "step": 20860
    },
    {
      "epoch": 1.049376508447305,
      "grad_norm": 8.375,
      "learning_rate": 1.7901246983105392e-05,
      "loss": 0.7916,
      "step": 20870
    },
    {
      "epoch": 1.0498793242156075,
      "grad_norm": 10.3125,
      "learning_rate": 1.7900241351568784e-05,
      "loss": 0.8386,
      "step": 20880
    },
    {
      "epoch": 1.05038213998391,
      "grad_norm": 20.5,
      "learning_rate": 1.7899235720032184e-05,
      "loss": 1.1192,
      "step": 20890
    },
    {
      "epoch": 1.0508849557522124,
      "grad_norm": 39.5,
      "learning_rate": 1.7898230088495576e-05,
      "loss": 1.1573,
      "step": 20900
    },
    {
      "epoch": 1.0513877715205149,
      "grad_norm": 43.75,
      "learning_rate": 1.7897224456958972e-05,
      "loss": 1.0448,
      "step": 20910
    },
    {
      "epoch": 1.0518905872888173,
      "grad_norm": 80.0,
      "learning_rate": 1.7896218825422368e-05,
      "loss": 1.1349,
      "step": 20920
    },
    {
      "epoch": 1.0523934030571198,
      "grad_norm": 13.375,
      "learning_rate": 1.789521319388576e-05,
      "loss": 0.8388,
      "step": 20930
    },
    {
      "epoch": 1.0528962188254223,
      "grad_norm": 28.875,
      "learning_rate": 1.7894207562349156e-05,
      "loss": 1.6216,
      "step": 20940
    },
    {
      "epoch": 1.053399034593725,
      "grad_norm": 18.5,
      "learning_rate": 1.7893201930812552e-05,
      "loss": 0.9621,
      "step": 20950
    },
    {
      "epoch": 1.0539018503620274,
      "grad_norm": 4.3125,
      "learning_rate": 1.7892196299275944e-05,
      "loss": 0.7724,
      "step": 20960
    },
    {
      "epoch": 1.05440466613033,
      "grad_norm": 9.3125,
      "learning_rate": 1.7891190667739344e-05,
      "loss": 1.0626,
      "step": 20970
    },
    {
      "epoch": 1.0549074818986324,
      "grad_norm": 58.75,
      "learning_rate": 1.7890185036202736e-05,
      "loss": 0.951,
      "step": 20980
    },
    {
      "epoch": 1.0554102976669348,
      "grad_norm": 45.75,
      "learning_rate": 1.7889179404666132e-05,
      "loss": 0.9314,
      "step": 20990
    },
    {
      "epoch": 1.0559131134352373,
      "grad_norm": 15.625,
      "learning_rate": 1.7888173773129528e-05,
      "loss": 1.0461,
      "step": 21000
    },
    {
      "epoch": 1.0559131134352373,
      "eval_accuracy": 0.512097174020823,
      "eval_loss": 1.0282175540924072,
      "eval_runtime": 464.6404,
      "eval_samples_per_second": 86.82,
      "eval_steps_per_second": 86.82,
      "step": 21000
    },
    {
      "epoch": 1.0564159292035398,
      "grad_norm": 5.84375,
      "learning_rate": 1.788716814159292e-05,
      "loss": 1.0763,
      "step": 21010
    },
    {
      "epoch": 1.0569187449718422,
      "grad_norm": 13.4375,
      "learning_rate": 1.7886162510056316e-05,
      "loss": 1.0952,
      "step": 21020
    },
    {
      "epoch": 1.0574215607401447,
      "grad_norm": 35.25,
      "learning_rate": 1.7885156878519712e-05,
      "loss": 1.2184,
      "step": 21030
    },
    {
      "epoch": 1.0579243765084474,
      "grad_norm": 16.125,
      "learning_rate": 1.7884151246983105e-05,
      "loss": 0.6456,
      "step": 21040
    },
    {
      "epoch": 1.0584271922767499,
      "grad_norm": 11.9375,
      "learning_rate": 1.78831456154465e-05,
      "loss": 0.7569,
      "step": 21050
    },
    {
      "epoch": 1.0589300080450523,
      "grad_norm": 5.125,
      "learning_rate": 1.7882139983909897e-05,
      "loss": 0.8019,
      "step": 21060
    },
    {
      "epoch": 1.0594328238133548,
      "grad_norm": 7.84375,
      "learning_rate": 1.7881134352373292e-05,
      "loss": 1.0129,
      "step": 21070
    },
    {
      "epoch": 1.0599356395816573,
      "grad_norm": 14.6875,
      "learning_rate": 1.7880128720836688e-05,
      "loss": 0.7816,
      "step": 21080
    },
    {
      "epoch": 1.0604384553499597,
      "grad_norm": 22.625,
      "learning_rate": 1.787912308930008e-05,
      "loss": 1.0689,
      "step": 21090
    },
    {
      "epoch": 1.0609412711182622,
      "grad_norm": 24.375,
      "learning_rate": 1.7878117457763477e-05,
      "loss": 1.1388,
      "step": 21100
    },
    {
      "epoch": 1.0614440868865647,
      "grad_norm": 52.0,
      "learning_rate": 1.7877111826226873e-05,
      "loss": 1.4824,
      "step": 21110
    },
    {
      "epoch": 1.0619469026548674,
      "grad_norm": 32.25,
      "learning_rate": 1.7876106194690265e-05,
      "loss": 1.0978,
      "step": 21120
    },
    {
      "epoch": 1.0624497184231698,
      "grad_norm": 43.25,
      "learning_rate": 1.787510056315366e-05,
      "loss": 0.9738,
      "step": 21130
    },
    {
      "epoch": 1.0629525341914723,
      "grad_norm": 31.25,
      "learning_rate": 1.7874094931617057e-05,
      "loss": 1.2175,
      "step": 21140
    },
    {
      "epoch": 1.0634553499597748,
      "grad_norm": 14.4375,
      "learning_rate": 1.7873089300080453e-05,
      "loss": 0.8389,
      "step": 21150
    },
    {
      "epoch": 1.0639581657280772,
      "grad_norm": 5.875,
      "learning_rate": 1.787208366854385e-05,
      "loss": 0.94,
      "step": 21160
    },
    {
      "epoch": 1.0644609814963797,
      "grad_norm": 32.25,
      "learning_rate": 1.787107803700724e-05,
      "loss": 1.2419,
      "step": 21170
    },
    {
      "epoch": 1.0649637972646822,
      "grad_norm": 8.4375,
      "learning_rate": 1.7870072405470637e-05,
      "loss": 0.942,
      "step": 21180
    },
    {
      "epoch": 1.0654666130329846,
      "grad_norm": 24.75,
      "learning_rate": 1.7869066773934033e-05,
      "loss": 0.6711,
      "step": 21190
    },
    {
      "epoch": 1.0659694288012873,
      "grad_norm": 33.0,
      "learning_rate": 1.7868061142397425e-05,
      "loss": 0.7403,
      "step": 21200
    },
    {
      "epoch": 1.0664722445695898,
      "grad_norm": 6.90625,
      "learning_rate": 1.786705551086082e-05,
      "loss": 0.6906,
      "step": 21210
    },
    {
      "epoch": 1.0669750603378922,
      "grad_norm": 20.25,
      "learning_rate": 1.7866049879324217e-05,
      "loss": 1.0858,
      "step": 21220
    },
    {
      "epoch": 1.0674778761061947,
      "grad_norm": 13.5625,
      "learning_rate": 1.7865044247787613e-05,
      "loss": 1.061,
      "step": 21230
    },
    {
      "epoch": 1.0679806918744972,
      "grad_norm": 26.25,
      "learning_rate": 1.786403861625101e-05,
      "loss": 0.8693,
      "step": 21240
    },
    {
      "epoch": 1.0684835076427996,
      "grad_norm": 60.0,
      "learning_rate": 1.78630329847144e-05,
      "loss": 1.0064,
      "step": 21250
    },
    {
      "epoch": 1.0689863234111021,
      "grad_norm": 5.78125,
      "learning_rate": 1.7862027353177797e-05,
      "loss": 1.2933,
      "step": 21260
    },
    {
      "epoch": 1.0694891391794046,
      "grad_norm": 24.25,
      "learning_rate": 1.7861021721641193e-05,
      "loss": 1.3688,
      "step": 21270
    },
    {
      "epoch": 1.069991954947707,
      "grad_norm": 9.8125,
      "learning_rate": 1.7860016090104586e-05,
      "loss": 0.9372,
      "step": 21280
    },
    {
      "epoch": 1.0704947707160097,
      "grad_norm": 12.9375,
      "learning_rate": 1.785901045856798e-05,
      "loss": 0.7877,
      "step": 21290
    },
    {
      "epoch": 1.0709975864843122,
      "grad_norm": 9.5,
      "learning_rate": 1.7858004827031377e-05,
      "loss": 0.9761,
      "step": 21300
    },
    {
      "epoch": 1.0715004022526147,
      "grad_norm": 12.375,
      "learning_rate": 1.7856999195494773e-05,
      "loss": 0.9644,
      "step": 21310
    },
    {
      "epoch": 1.0720032180209171,
      "grad_norm": 43.25,
      "learning_rate": 1.7855993563958166e-05,
      "loss": 1.2843,
      "step": 21320
    },
    {
      "epoch": 1.0725060337892196,
      "grad_norm": 7.75,
      "learning_rate": 1.785498793242156e-05,
      "loss": 0.9162,
      "step": 21330
    },
    {
      "epoch": 1.073008849557522,
      "grad_norm": 17.0,
      "learning_rate": 1.7853982300884957e-05,
      "loss": 1.0219,
      "step": 21340
    },
    {
      "epoch": 1.0735116653258245,
      "grad_norm": 20.25,
      "learning_rate": 1.7852976669348353e-05,
      "loss": 1.0869,
      "step": 21350
    },
    {
      "epoch": 1.0740144810941272,
      "grad_norm": 39.75,
      "learning_rate": 1.785197103781175e-05,
      "loss": 1.3475,
      "step": 21360
    },
    {
      "epoch": 1.0745172968624297,
      "grad_norm": 12.0625,
      "learning_rate": 1.785096540627514e-05,
      "loss": 0.7761,
      "step": 21370
    },
    {
      "epoch": 1.0750201126307322,
      "grad_norm": 22.375,
      "learning_rate": 1.7849959774738538e-05,
      "loss": 1.1574,
      "step": 21380
    },
    {
      "epoch": 1.0755229283990346,
      "grad_norm": 37.0,
      "learning_rate": 1.7848954143201933e-05,
      "loss": 1.2209,
      "step": 21390
    },
    {
      "epoch": 1.076025744167337,
      "grad_norm": 8.9375,
      "learning_rate": 1.7847948511665326e-05,
      "loss": 0.8283,
      "step": 21400
    },
    {
      "epoch": 1.0765285599356396,
      "grad_norm": 66.5,
      "learning_rate": 1.7846942880128722e-05,
      "loss": 1.1853,
      "step": 21410
    },
    {
      "epoch": 1.077031375703942,
      "grad_norm": 12.5,
      "learning_rate": 1.7845937248592118e-05,
      "loss": 0.9759,
      "step": 21420
    },
    {
      "epoch": 1.0775341914722445,
      "grad_norm": 32.75,
      "learning_rate": 1.7844931617055514e-05,
      "loss": 1.1772,
      "step": 21430
    },
    {
      "epoch": 1.078037007240547,
      "grad_norm": 18.375,
      "learning_rate": 1.784392598551891e-05,
      "loss": 1.0038,
      "step": 21440
    },
    {
      "epoch": 1.0785398230088497,
      "grad_norm": 7.03125,
      "learning_rate": 1.7842920353982302e-05,
      "loss": 1.0123,
      "step": 21450
    },
    {
      "epoch": 1.0790426387771521,
      "grad_norm": 8.0625,
      "learning_rate": 1.7841914722445698e-05,
      "loss": 1.1355,
      "step": 21460
    },
    {
      "epoch": 1.0795454545454546,
      "grad_norm": 47.5,
      "learning_rate": 1.7840909090909094e-05,
      "loss": 1.0454,
      "step": 21470
    },
    {
      "epoch": 1.080048270313757,
      "grad_norm": 41.0,
      "learning_rate": 1.7839903459372486e-05,
      "loss": 0.9378,
      "step": 21480
    },
    {
      "epoch": 1.0805510860820595,
      "grad_norm": 27.875,
      "learning_rate": 1.7838897827835882e-05,
      "loss": 1.154,
      "step": 21490
    },
    {
      "epoch": 1.081053901850362,
      "grad_norm": 52.75,
      "learning_rate": 1.7837892196299278e-05,
      "loss": 1.2339,
      "step": 21500
    },
    {
      "epoch": 1.081053901850362,
      "eval_accuracy": 0.5117253346554289,
      "eval_loss": 1.0274715423583984,
      "eval_runtime": 463.7635,
      "eval_samples_per_second": 86.984,
      "eval_steps_per_second": 86.984,
      "step": 21500
    },
    {
      "epoch": 1.0815567176186645,
      "grad_norm": 11.625,
      "learning_rate": 1.7836886564762674e-05,
      "loss": 0.9948,
      "step": 21510
    },
    {
      "epoch": 1.082059533386967,
      "grad_norm": 17.125,
      "learning_rate": 1.783588093322607e-05,
      "loss": 0.8493,
      "step": 21520
    },
    {
      "epoch": 1.0825623491552696,
      "grad_norm": 42.5,
      "learning_rate": 1.7834875301689462e-05,
      "loss": 1.0992,
      "step": 21530
    },
    {
      "epoch": 1.083065164923572,
      "grad_norm": 10.125,
      "learning_rate": 1.7833869670152858e-05,
      "loss": 0.8972,
      "step": 21540
    },
    {
      "epoch": 1.0835679806918745,
      "grad_norm": 12.375,
      "learning_rate": 1.7832864038616254e-05,
      "loss": 0.9422,
      "step": 21550
    },
    {
      "epoch": 1.084070796460177,
      "grad_norm": 40.5,
      "learning_rate": 1.7831858407079646e-05,
      "loss": 0.9811,
      "step": 21560
    },
    {
      "epoch": 1.0845736122284795,
      "grad_norm": 80.0,
      "learning_rate": 1.7830852775543042e-05,
      "loss": 1.3956,
      "step": 21570
    },
    {
      "epoch": 1.085076427996782,
      "grad_norm": 21.625,
      "learning_rate": 1.7829847144006438e-05,
      "loss": 1.1549,
      "step": 21580
    },
    {
      "epoch": 1.0855792437650844,
      "grad_norm": 13.125,
      "learning_rate": 1.782884151246983e-05,
      "loss": 0.7393,
      "step": 21590
    },
    {
      "epoch": 1.0860820595333869,
      "grad_norm": 40.25,
      "learning_rate": 1.782783588093323e-05,
      "loss": 0.9915,
      "step": 21600
    },
    {
      "epoch": 1.0865848753016896,
      "grad_norm": 27.0,
      "learning_rate": 1.7826830249396622e-05,
      "loss": 1.1711,
      "step": 21610
    },
    {
      "epoch": 1.087087691069992,
      "grad_norm": 7.6875,
      "learning_rate": 1.7825824617860018e-05,
      "loss": 0.7671,
      "step": 21620
    },
    {
      "epoch": 1.0875905068382945,
      "grad_norm": 19.375,
      "learning_rate": 1.7824818986323414e-05,
      "loss": 0.8765,
      "step": 21630
    },
    {
      "epoch": 1.088093322606597,
      "grad_norm": 26.0,
      "learning_rate": 1.7823813354786807e-05,
      "loss": 0.9716,
      "step": 21640
    },
    {
      "epoch": 1.0885961383748994,
      "grad_norm": 45.25,
      "learning_rate": 1.7822807723250203e-05,
      "loss": 1.2362,
      "step": 21650
    },
    {
      "epoch": 1.089098954143202,
      "grad_norm": 6.03125,
      "learning_rate": 1.78218020917136e-05,
      "loss": 1.2094,
      "step": 21660
    },
    {
      "epoch": 1.0896017699115044,
      "grad_norm": 8.75,
      "learning_rate": 1.782079646017699e-05,
      "loss": 0.9633,
      "step": 21670
    },
    {
      "epoch": 1.0901045856798068,
      "grad_norm": 18.0,
      "learning_rate": 1.781979082864039e-05,
      "loss": 0.8835,
      "step": 21680
    },
    {
      "epoch": 1.0906074014481093,
      "grad_norm": 22.75,
      "learning_rate": 1.7818785197103783e-05,
      "loss": 0.9466,
      "step": 21690
    },
    {
      "epoch": 1.091110217216412,
      "grad_norm": 17.0,
      "learning_rate": 1.781777956556718e-05,
      "loss": 1.132,
      "step": 21700
    },
    {
      "epoch": 1.0916130329847145,
      "grad_norm": 26.875,
      "learning_rate": 1.7816773934030574e-05,
      "loss": 0.9002,
      "step": 21710
    },
    {
      "epoch": 1.092115848753017,
      "grad_norm": 36.0,
      "learning_rate": 1.7815768302493967e-05,
      "loss": 1.3747,
      "step": 21720
    },
    {
      "epoch": 1.0926186645213194,
      "grad_norm": 12.125,
      "learning_rate": 1.7814762670957363e-05,
      "loss": 0.8992,
      "step": 21730
    },
    {
      "epoch": 1.0931214802896219,
      "grad_norm": 27.25,
      "learning_rate": 1.781375703942076e-05,
      "loss": 0.9019,
      "step": 21740
    },
    {
      "epoch": 1.0936242960579243,
      "grad_norm": 58.25,
      "learning_rate": 1.781275140788415e-05,
      "loss": 1.1676,
      "step": 21750
    },
    {
      "epoch": 1.0941271118262268,
      "grad_norm": 23.0,
      "learning_rate": 1.781174577634755e-05,
      "loss": 0.9264,
      "step": 21760
    },
    {
      "epoch": 1.0946299275945293,
      "grad_norm": 12.75,
      "learning_rate": 1.7810740144810943e-05,
      "loss": 0.7773,
      "step": 21770
    },
    {
      "epoch": 1.095132743362832,
      "grad_norm": 33.5,
      "learning_rate": 1.780973451327434e-05,
      "loss": 0.9479,
      "step": 21780
    },
    {
      "epoch": 1.0956355591311344,
      "grad_norm": 33.0,
      "learning_rate": 1.7808728881737735e-05,
      "loss": 1.08,
      "step": 21790
    },
    {
      "epoch": 1.0961383748994369,
      "grad_norm": 10.625,
      "learning_rate": 1.7807723250201127e-05,
      "loss": 0.9675,
      "step": 21800
    },
    {
      "epoch": 1.0966411906677394,
      "grad_norm": 5.75,
      "learning_rate": 1.7806717618664523e-05,
      "loss": 0.6619,
      "step": 21810
    },
    {
      "epoch": 1.0971440064360418,
      "grad_norm": 11.8125,
      "learning_rate": 1.780571198712792e-05,
      "loss": 1.0537,
      "step": 21820
    },
    {
      "epoch": 1.0976468222043443,
      "grad_norm": 16.375,
      "learning_rate": 1.780470635559131e-05,
      "loss": 0.8265,
      "step": 21830
    },
    {
      "epoch": 1.0981496379726468,
      "grad_norm": 24.5,
      "learning_rate": 1.7803700724054707e-05,
      "loss": 0.809,
      "step": 21840
    },
    {
      "epoch": 1.0986524537409492,
      "grad_norm": 96.5,
      "learning_rate": 1.7802695092518103e-05,
      "loss": 0.9203,
      "step": 21850
    },
    {
      "epoch": 1.099155269509252,
      "grad_norm": 4.78125,
      "learning_rate": 1.7801689460981496e-05,
      "loss": 0.7973,
      "step": 21860
    },
    {
      "epoch": 1.0996580852775544,
      "grad_norm": 37.75,
      "learning_rate": 1.7800683829444895e-05,
      "loss": 1.1721,
      "step": 21870
    },
    {
      "epoch": 1.1001609010458568,
      "grad_norm": 17.375,
      "learning_rate": 1.7799678197908287e-05,
      "loss": 1.2053,
      "step": 21880
    },
    {
      "epoch": 1.1006637168141593,
      "grad_norm": 7.65625,
      "learning_rate": 1.7798672566371683e-05,
      "loss": 0.9289,
      "step": 21890
    },
    {
      "epoch": 1.1011665325824618,
      "grad_norm": 27.0,
      "learning_rate": 1.779766693483508e-05,
      "loss": 0.9854,
      "step": 21900
    },
    {
      "epoch": 1.1016693483507642,
      "grad_norm": 48.0,
      "learning_rate": 1.779666130329847e-05,
      "loss": 1.1019,
      "step": 21910
    },
    {
      "epoch": 1.1021721641190667,
      "grad_norm": 25.75,
      "learning_rate": 1.7795655671761868e-05,
      "loss": 0.8881,
      "step": 21920
    },
    {
      "epoch": 1.1026749798873692,
      "grad_norm": 19.375,
      "learning_rate": 1.7794650040225263e-05,
      "loss": 0.9022,
      "step": 21930
    },
    {
      "epoch": 1.1031777956556716,
      "grad_norm": 89.5,
      "learning_rate": 1.7793644408688656e-05,
      "loss": 0.8931,
      "step": 21940
    },
    {
      "epoch": 1.1036806114239743,
      "grad_norm": 7.3125,
      "learning_rate": 1.7792638777152055e-05,
      "loss": 1.136,
      "step": 21950
    },
    {
      "epoch": 1.1041834271922768,
      "grad_norm": 100.0,
      "learning_rate": 1.7791633145615448e-05,
      "loss": 1.3572,
      "step": 21960
    },
    {
      "epoch": 1.1046862429605793,
      "grad_norm": 8.875,
      "learning_rate": 1.7790627514078844e-05,
      "loss": 1.089,
      "step": 21970
    },
    {
      "epoch": 1.1051890587288817,
      "grad_norm": 7.75,
      "learning_rate": 1.778962188254224e-05,
      "loss": 1.0411,
      "step": 21980
    },
    {
      "epoch": 1.1056918744971842,
      "grad_norm": 8.625,
      "learning_rate": 1.7788616251005632e-05,
      "loss": 0.9349,
      "step": 21990
    },
    {
      "epoch": 1.1061946902654867,
      "grad_norm": 33.5,
      "learning_rate": 1.7787610619469028e-05,
      "loss": 1.0148,
      "step": 22000
    },
    {
      "epoch": 1.1061946902654867,
      "eval_accuracy": 0.5120475954387704,
      "eval_loss": 1.0274765491485596,
      "eval_runtime": 465.1719,
      "eval_samples_per_second": 86.721,
      "eval_steps_per_second": 86.721,
      "step": 22000
    },
    {
      "epoch": 1.1066975060337891,
      "grad_norm": 29.25,
      "learning_rate": 1.7786604987932424e-05,
      "loss": 0.9445,
      "step": 22010
    },
    {
      "epoch": 1.1072003218020918,
      "grad_norm": 31.25,
      "learning_rate": 1.7785599356395816e-05,
      "loss": 0.9474,
      "step": 22020
    },
    {
      "epoch": 1.1077031375703943,
      "grad_norm": 21.5,
      "learning_rate": 1.7784593724859215e-05,
      "loss": 1.0811,
      "step": 22030
    },
    {
      "epoch": 1.1082059533386968,
      "grad_norm": 42.5,
      "learning_rate": 1.7783588093322608e-05,
      "loss": 0.6445,
      "step": 22040
    },
    {
      "epoch": 1.1087087691069992,
      "grad_norm": 14.125,
      "learning_rate": 1.7782582461786e-05,
      "loss": 0.8715,
      "step": 22050
    },
    {
      "epoch": 1.1092115848753017,
      "grad_norm": 13.125,
      "learning_rate": 1.77815768302494e-05,
      "loss": 0.8852,
      "step": 22060
    },
    {
      "epoch": 1.1097144006436042,
      "grad_norm": 19.625,
      "learning_rate": 1.7780571198712792e-05,
      "loss": 0.7959,
      "step": 22070
    },
    {
      "epoch": 1.1102172164119066,
      "grad_norm": 21.0,
      "learning_rate": 1.7779565567176188e-05,
      "loss": 1.2125,
      "step": 22080
    },
    {
      "epoch": 1.110720032180209,
      "grad_norm": 7.21875,
      "learning_rate": 1.7778559935639584e-05,
      "loss": 1.1311,
      "step": 22090
    },
    {
      "epoch": 1.1112228479485116,
      "grad_norm": 21.125,
      "learning_rate": 1.7777554304102976e-05,
      "loss": 0.8596,
      "step": 22100
    },
    {
      "epoch": 1.1117256637168142,
      "grad_norm": 44.5,
      "learning_rate": 1.7776548672566372e-05,
      "loss": 1.1607,
      "step": 22110
    },
    {
      "epoch": 1.1122284794851167,
      "grad_norm": 6.03125,
      "learning_rate": 1.7775543041029768e-05,
      "loss": 1.1916,
      "step": 22120
    },
    {
      "epoch": 1.1127312952534192,
      "grad_norm": 11.625,
      "learning_rate": 1.777453740949316e-05,
      "loss": 1.0198,
      "step": 22130
    },
    {
      "epoch": 1.1132341110217217,
      "grad_norm": 13.4375,
      "learning_rate": 1.777353177795656e-05,
      "loss": 0.8767,
      "step": 22140
    },
    {
      "epoch": 1.1137369267900241,
      "grad_norm": 36.75,
      "learning_rate": 1.7772526146419952e-05,
      "loss": 0.7837,
      "step": 22150
    },
    {
      "epoch": 1.1142397425583266,
      "grad_norm": 25.75,
      "learning_rate": 1.777152051488335e-05,
      "loss": 0.9473,
      "step": 22160
    },
    {
      "epoch": 1.114742558326629,
      "grad_norm": 6.0,
      "learning_rate": 1.7770514883346744e-05,
      "loss": 0.9205,
      "step": 22170
    },
    {
      "epoch": 1.1152453740949315,
      "grad_norm": 94.0,
      "learning_rate": 1.7769509251810137e-05,
      "loss": 1.0047,
      "step": 22180
    },
    {
      "epoch": 1.1157481898632342,
      "grad_norm": 42.5,
      "learning_rate": 1.7768503620273533e-05,
      "loss": 1.0946,
      "step": 22190
    },
    {
      "epoch": 1.1162510056315367,
      "grad_norm": 10.8125,
      "learning_rate": 1.776749798873693e-05,
      "loss": 1.1423,
      "step": 22200
    },
    {
      "epoch": 1.1167538213998391,
      "grad_norm": 27.875,
      "learning_rate": 1.776649235720032e-05,
      "loss": 0.9372,
      "step": 22210
    },
    {
      "epoch": 1.1172566371681416,
      "grad_norm": 9.3125,
      "learning_rate": 1.776548672566372e-05,
      "loss": 0.869,
      "step": 22220
    },
    {
      "epoch": 1.117759452936444,
      "grad_norm": 20.75,
      "learning_rate": 1.7764481094127113e-05,
      "loss": 0.9639,
      "step": 22230
    },
    {
      "epoch": 1.1182622687047465,
      "grad_norm": 12.75,
      "learning_rate": 1.776347546259051e-05,
      "loss": 0.8065,
      "step": 22240
    },
    {
      "epoch": 1.118765084473049,
      "grad_norm": 62.25,
      "learning_rate": 1.7762469831053904e-05,
      "loss": 1.1562,
      "step": 22250
    },
    {
      "epoch": 1.1192679002413515,
      "grad_norm": 10.375,
      "learning_rate": 1.7761464199517297e-05,
      "loss": 0.9164,
      "step": 22260
    },
    {
      "epoch": 1.1197707160096542,
      "grad_norm": 10.0625,
      "learning_rate": 1.7760458567980693e-05,
      "loss": 0.8158,
      "step": 22270
    },
    {
      "epoch": 1.1202735317779566,
      "grad_norm": 7.96875,
      "learning_rate": 1.775945293644409e-05,
      "loss": 0.9131,
      "step": 22280
    },
    {
      "epoch": 1.120776347546259,
      "grad_norm": 33.0,
      "learning_rate": 1.775844730490748e-05,
      "loss": 1.0323,
      "step": 22290
    },
    {
      "epoch": 1.1212791633145616,
      "grad_norm": 30.875,
      "learning_rate": 1.7757441673370877e-05,
      "loss": 0.8337,
      "step": 22300
    },
    {
      "epoch": 1.121781979082864,
      "grad_norm": 81.0,
      "learning_rate": 1.7756436041834273e-05,
      "loss": 1.3995,
      "step": 22310
    },
    {
      "epoch": 1.1222847948511665,
      "grad_norm": 31.25,
      "learning_rate": 1.775543041029767e-05,
      "loss": 0.9287,
      "step": 22320
    },
    {
      "epoch": 1.122787610619469,
      "grad_norm": 12.5,
      "learning_rate": 1.7754424778761065e-05,
      "loss": 0.9329,
      "step": 22330
    },
    {
      "epoch": 1.1232904263877714,
      "grad_norm": 16.0,
      "learning_rate": 1.7753419147224457e-05,
      "loss": 1.1779,
      "step": 22340
    },
    {
      "epoch": 1.123793242156074,
      "grad_norm": 33.25,
      "learning_rate": 1.7752413515687853e-05,
      "loss": 1.0966,
      "step": 22350
    },
    {
      "epoch": 1.1242960579243766,
      "grad_norm": 45.5,
      "learning_rate": 1.775140788415125e-05,
      "loss": 1.0195,
      "step": 22360
    },
    {
      "epoch": 1.124798873692679,
      "grad_norm": 9.0,
      "learning_rate": 1.775040225261464e-05,
      "loss": 0.9775,
      "step": 22370
    },
    {
      "epoch": 1.1253016894609815,
      "grad_norm": 20.875,
      "learning_rate": 1.7749396621078037e-05,
      "loss": 1.2571,
      "step": 22380
    },
    {
      "epoch": 1.125804505229284,
      "grad_norm": 36.25,
      "learning_rate": 1.7748390989541433e-05,
      "loss": 1.0198,
      "step": 22390
    },
    {
      "epoch": 1.1263073209975865,
      "grad_norm": 24.25,
      "learning_rate": 1.774738535800483e-05,
      "loss": 1.4281,
      "step": 22400
    },
    {
      "epoch": 1.126810136765889,
      "grad_norm": 26.75,
      "learning_rate": 1.7746379726468225e-05,
      "loss": 0.9632,
      "step": 22410
    },
    {
      "epoch": 1.1273129525341914,
      "grad_norm": 10.875,
      "learning_rate": 1.7745374094931617e-05,
      "loss": 0.9468,
      "step": 22420
    },
    {
      "epoch": 1.127815768302494,
      "grad_norm": 13.25,
      "learning_rate": 1.7744368463395013e-05,
      "loss": 0.7698,
      "step": 22430
    },
    {
      "epoch": 1.1283185840707965,
      "grad_norm": 10.25,
      "learning_rate": 1.774336283185841e-05,
      "loss": 0.8508,
      "step": 22440
    },
    {
      "epoch": 1.128821399839099,
      "grad_norm": 27.25,
      "learning_rate": 1.7742357200321802e-05,
      "loss": 1.3173,
      "step": 22450
    },
    {
      "epoch": 1.1293242156074015,
      "grad_norm": 26.0,
      "learning_rate": 1.7741351568785198e-05,
      "loss": 0.9969,
      "step": 22460
    },
    {
      "epoch": 1.129827031375704,
      "grad_norm": 17.5,
      "learning_rate": 1.7740345937248593e-05,
      "loss": 0.7503,
      "step": 22470
    },
    {
      "epoch": 1.1303298471440064,
      "grad_norm": 64.0,
      "learning_rate": 1.773934030571199e-05,
      "loss": 0.961,
      "step": 22480
    },
    {
      "epoch": 1.1308326629123089,
      "grad_norm": 9.4375,
      "learning_rate": 1.7738334674175385e-05,
      "loss": 0.7246,
      "step": 22490
    },
    {
      "epoch": 1.1313354786806114,
      "grad_norm": 31.875,
      "learning_rate": 1.7737329042638778e-05,
      "loss": 1.2052,
      "step": 22500
    },
    {
      "epoch": 1.1313354786806114,
      "eval_accuracy": 0.5117501239464551,
      "eval_loss": 1.0277217626571655,
      "eval_runtime": 463.3068,
      "eval_samples_per_second": 87.07,
      "eval_steps_per_second": 87.07,
      "step": 22500
    },
    {
      "epoch": 1.1318382944489138,
      "grad_norm": 13.5,
      "learning_rate": 1.7736323411102174e-05,
      "loss": 0.8448,
      "step": 22510
    },
    {
      "epoch": 1.1323411102172165,
      "grad_norm": 47.5,
      "learning_rate": 1.773531777956557e-05,
      "loss": 1.137,
      "step": 22520
    },
    {
      "epoch": 1.132843925985519,
      "grad_norm": 120.0,
      "learning_rate": 1.7734312148028962e-05,
      "loss": 1.1703,
      "step": 22530
    },
    {
      "epoch": 1.1333467417538214,
      "grad_norm": 15.4375,
      "learning_rate": 1.7733306516492358e-05,
      "loss": 0.9042,
      "step": 22540
    },
    {
      "epoch": 1.133849557522124,
      "grad_norm": 20.625,
      "learning_rate": 1.7732300884955754e-05,
      "loss": 0.8207,
      "step": 22550
    },
    {
      "epoch": 1.1343523732904264,
      "grad_norm": 21.875,
      "learning_rate": 1.773129525341915e-05,
      "loss": 1.0629,
      "step": 22560
    },
    {
      "epoch": 1.1348551890587288,
      "grad_norm": 12.0,
      "learning_rate": 1.7730289621882542e-05,
      "loss": 1.1815,
      "step": 22570
    },
    {
      "epoch": 1.1353580048270313,
      "grad_norm": 13.625,
      "learning_rate": 1.7729283990345938e-05,
      "loss": 1.2511,
      "step": 22580
    },
    {
      "epoch": 1.1358608205953338,
      "grad_norm": 59.25,
      "learning_rate": 1.7728278358809334e-05,
      "loss": 1.1412,
      "step": 22590
    },
    {
      "epoch": 1.1363636363636362,
      "grad_norm": 75.5,
      "learning_rate": 1.772727272727273e-05,
      "loss": 1.2536,
      "step": 22600
    },
    {
      "epoch": 1.136866452131939,
      "grad_norm": 22.125,
      "learning_rate": 1.7726267095736122e-05,
      "loss": 1.0635,
      "step": 22610
    },
    {
      "epoch": 1.1373692679002414,
      "grad_norm": 42.5,
      "learning_rate": 1.7725261464199518e-05,
      "loss": 1.1801,
      "step": 22620
    },
    {
      "epoch": 1.1378720836685439,
      "grad_norm": 58.75,
      "learning_rate": 1.7724255832662914e-05,
      "loss": 1.4422,
      "step": 22630
    },
    {
      "epoch": 1.1383748994368463,
      "grad_norm": 26.5,
      "learning_rate": 1.772325020112631e-05,
      "loss": 1.1444,
      "step": 22640
    },
    {
      "epoch": 1.1388777152051488,
      "grad_norm": 11.1875,
      "learning_rate": 1.7722244569589702e-05,
      "loss": 0.7427,
      "step": 22650
    },
    {
      "epoch": 1.1393805309734513,
      "grad_norm": 22.0,
      "learning_rate": 1.7721238938053098e-05,
      "loss": 1.0044,
      "step": 22660
    },
    {
      "epoch": 1.1398833467417537,
      "grad_norm": 45.0,
      "learning_rate": 1.7720233306516494e-05,
      "loss": 1.3701,
      "step": 22670
    },
    {
      "epoch": 1.1403861625100564,
      "grad_norm": 23.125,
      "learning_rate": 1.771922767497989e-05,
      "loss": 0.7522,
      "step": 22680
    },
    {
      "epoch": 1.140888978278359,
      "grad_norm": 31.875,
      "learning_rate": 1.7718222043443282e-05,
      "loss": 1.1871,
      "step": 22690
    },
    {
      "epoch": 1.1413917940466614,
      "grad_norm": 31.875,
      "learning_rate": 1.771721641190668e-05,
      "loss": 1.2968,
      "step": 22700
    },
    {
      "epoch": 1.1418946098149638,
      "grad_norm": 11.75,
      "learning_rate": 1.7716210780370074e-05,
      "loss": 0.9268,
      "step": 22710
    },
    {
      "epoch": 1.1423974255832663,
      "grad_norm": 15.0,
      "learning_rate": 1.771520514883347e-05,
      "loss": 0.7753,
      "step": 22720
    },
    {
      "epoch": 1.1429002413515688,
      "grad_norm": 11.5,
      "learning_rate": 1.7714199517296863e-05,
      "loss": 1.2526,
      "step": 22730
    },
    {
      "epoch": 1.1434030571198712,
      "grad_norm": 24.375,
      "learning_rate": 1.771319388576026e-05,
      "loss": 1.0725,
      "step": 22740
    },
    {
      "epoch": 1.1439058728881737,
      "grad_norm": 14.8125,
      "learning_rate": 1.7712188254223654e-05,
      "loss": 1.0416,
      "step": 22750
    },
    {
      "epoch": 1.1444086886564762,
      "grad_norm": 33.75,
      "learning_rate": 1.771118262268705e-05,
      "loss": 1.1174,
      "step": 22760
    },
    {
      "epoch": 1.1449115044247788,
      "grad_norm": 12.5625,
      "learning_rate": 1.7710176991150443e-05,
      "loss": 1.0502,
      "step": 22770
    },
    {
      "epoch": 1.1454143201930813,
      "grad_norm": 25.75,
      "learning_rate": 1.770917135961384e-05,
      "loss": 1.374,
      "step": 22780
    },
    {
      "epoch": 1.1459171359613838,
      "grad_norm": 21.25,
      "learning_rate": 1.7708165728077234e-05,
      "loss": 1.1696,
      "step": 22790
    },
    {
      "epoch": 1.1464199517296862,
      "grad_norm": 22.625,
      "learning_rate": 1.770716009654063e-05,
      "loss": 0.878,
      "step": 22800
    },
    {
      "epoch": 1.1469227674979887,
      "grad_norm": 8.625,
      "learning_rate": 1.7706154465004023e-05,
      "loss": 1.477,
      "step": 22810
    },
    {
      "epoch": 1.1474255832662912,
      "grad_norm": 13.375,
      "learning_rate": 1.770514883346742e-05,
      "loss": 0.7693,
      "step": 22820
    },
    {
      "epoch": 1.1479283990345936,
      "grad_norm": 58.0,
      "learning_rate": 1.7704143201930815e-05,
      "loss": 1.2071,
      "step": 22830
    },
    {
      "epoch": 1.1484312148028963,
      "grad_norm": 58.75,
      "learning_rate": 1.7703137570394207e-05,
      "loss": 1.0951,
      "step": 22840
    },
    {
      "epoch": 1.1489340305711988,
      "grad_norm": 20.375,
      "learning_rate": 1.7702131938857603e-05,
      "loss": 1.0095,
      "step": 22850
    },
    {
      "epoch": 1.1494368463395013,
      "grad_norm": 15.0625,
      "learning_rate": 1.7701126307321e-05,
      "loss": 1.0402,
      "step": 22860
    },
    {
      "epoch": 1.1499396621078037,
      "grad_norm": 5.6875,
      "learning_rate": 1.7700120675784395e-05,
      "loss": 0.9428,
      "step": 22870
    },
    {
      "epoch": 1.1504424778761062,
      "grad_norm": 7.25,
      "learning_rate": 1.769911504424779e-05,
      "loss": 0.7458,
      "step": 22880
    },
    {
      "epoch": 1.1509452936444087,
      "grad_norm": 22.75,
      "learning_rate": 1.7698109412711183e-05,
      "loss": 0.9448,
      "step": 22890
    },
    {
      "epoch": 1.1514481094127111,
      "grad_norm": 9.3125,
      "learning_rate": 1.769710378117458e-05,
      "loss": 0.6868,
      "step": 22900
    },
    {
      "epoch": 1.1519509251810136,
      "grad_norm": 9.25,
      "learning_rate": 1.7696098149637975e-05,
      "loss": 0.9815,
      "step": 22910
    },
    {
      "epoch": 1.152453740949316,
      "grad_norm": 15.0,
      "learning_rate": 1.7695092518101367e-05,
      "loss": 0.9312,
      "step": 22920
    },
    {
      "epoch": 1.1529565567176188,
      "grad_norm": 7.34375,
      "learning_rate": 1.7694086886564763e-05,
      "loss": 0.6993,
      "step": 22930
    },
    {
      "epoch": 1.1534593724859212,
      "grad_norm": 25.75,
      "learning_rate": 1.769308125502816e-05,
      "loss": 1.1152,
      "step": 22940
    },
    {
      "epoch": 1.1539621882542237,
      "grad_norm": 17.75,
      "learning_rate": 1.7692075623491555e-05,
      "loss": 1.2753,
      "step": 22950
    },
    {
      "epoch": 1.1544650040225262,
      "grad_norm": 26.0,
      "learning_rate": 1.769106999195495e-05,
      "loss": 1.0267,
      "step": 22960
    },
    {
      "epoch": 1.1549678197908286,
      "grad_norm": 13.6875,
      "learning_rate": 1.7690064360418343e-05,
      "loss": 0.9757,
      "step": 22970
    },
    {
      "epoch": 1.155470635559131,
      "grad_norm": 10.125,
      "learning_rate": 1.768905872888174e-05,
      "loss": 1.2714,
      "step": 22980
    },
    {
      "epoch": 1.1559734513274336,
      "grad_norm": 29.875,
      "learning_rate": 1.7688053097345135e-05,
      "loss": 0.9669,
      "step": 22990
    },
    {
      "epoch": 1.156476267095736,
      "grad_norm": 3.84375,
      "learning_rate": 1.7687047465808528e-05,
      "loss": 0.9753,
      "step": 23000
    },
    {
      "epoch": 1.156476267095736,
      "eval_accuracy": 0.5119236489836391,
      "eval_loss": 1.0275810956954956,
      "eval_runtime": 465.0943,
      "eval_samples_per_second": 86.735,
      "eval_steps_per_second": 86.735,
      "step": 23000
    },
    {
      "epoch": 1.1569790828640385,
      "grad_norm": 37.75,
      "learning_rate": 1.7686041834271923e-05,
      "loss": 1.0032,
      "step": 23010
    },
    {
      "epoch": 1.1574818986323412,
      "grad_norm": 16.125,
      "learning_rate": 1.768503620273532e-05,
      "loss": 0.6949,
      "step": 23020
    },
    {
      "epoch": 1.1579847144006437,
      "grad_norm": 23.25,
      "learning_rate": 1.7684030571198715e-05,
      "loss": 1.0075,
      "step": 23030
    },
    {
      "epoch": 1.1584875301689461,
      "grad_norm": 14.375,
      "learning_rate": 1.768302493966211e-05,
      "loss": 1.3225,
      "step": 23040
    },
    {
      "epoch": 1.1589903459372486,
      "grad_norm": 17.25,
      "learning_rate": 1.7682019308125504e-05,
      "loss": 1.026,
      "step": 23050
    },
    {
      "epoch": 1.159493161705551,
      "grad_norm": 47.0,
      "learning_rate": 1.76810136765889e-05,
      "loss": 1.1041,
      "step": 23060
    },
    {
      "epoch": 1.1599959774738535,
      "grad_norm": 14.5625,
      "learning_rate": 1.7680008045052295e-05,
      "loss": 1.0156,
      "step": 23070
    },
    {
      "epoch": 1.160498793242156,
      "grad_norm": 36.5,
      "learning_rate": 1.7679002413515688e-05,
      "loss": 1.3041,
      "step": 23080
    },
    {
      "epoch": 1.1610016090104587,
      "grad_norm": 12.8125,
      "learning_rate": 1.7677996781979084e-05,
      "loss": 0.6669,
      "step": 23090
    },
    {
      "epoch": 1.1615044247787611,
      "grad_norm": 36.25,
      "learning_rate": 1.767699115044248e-05,
      "loss": 1.2957,
      "step": 23100
    },
    {
      "epoch": 1.1620072405470636,
      "grad_norm": 12.9375,
      "learning_rate": 1.7675985518905872e-05,
      "loss": 0.9539,
      "step": 23110
    },
    {
      "epoch": 1.162510056315366,
      "grad_norm": 15.75,
      "learning_rate": 1.767497988736927e-05,
      "loss": 1.3809,
      "step": 23120
    },
    {
      "epoch": 1.1630128720836685,
      "grad_norm": 18.75,
      "learning_rate": 1.7673974255832664e-05,
      "loss": 0.9716,
      "step": 23130
    },
    {
      "epoch": 1.163515687851971,
      "grad_norm": 4.59375,
      "learning_rate": 1.767296862429606e-05,
      "loss": 0.8701,
      "step": 23140
    },
    {
      "epoch": 1.1640185036202735,
      "grad_norm": 22.625,
      "learning_rate": 1.7671962992759456e-05,
      "loss": 1.2827,
      "step": 23150
    },
    {
      "epoch": 1.164521319388576,
      "grad_norm": 32.0,
      "learning_rate": 1.7670957361222848e-05,
      "loss": 1.144,
      "step": 23160
    },
    {
      "epoch": 1.1650241351568784,
      "grad_norm": 26.625,
      "learning_rate": 1.7669951729686244e-05,
      "loss": 1.0199,
      "step": 23170
    },
    {
      "epoch": 1.165526950925181,
      "grad_norm": 7.0625,
      "learning_rate": 1.766894609814964e-05,
      "loss": 1.1393,
      "step": 23180
    },
    {
      "epoch": 1.1660297666934836,
      "grad_norm": 3.734375,
      "learning_rate": 1.7667940466613032e-05,
      "loss": 0.9583,
      "step": 23190
    },
    {
      "epoch": 1.166532582461786,
      "grad_norm": 9.625,
      "learning_rate": 1.766693483507643e-05,
      "loss": 0.725,
      "step": 23200
    },
    {
      "epoch": 1.1670353982300885,
      "grad_norm": 5.21875,
      "learning_rate": 1.7665929203539824e-05,
      "loss": 1.0345,
      "step": 23210
    },
    {
      "epoch": 1.167538213998391,
      "grad_norm": 25.375,
      "learning_rate": 1.766492357200322e-05,
      "loss": 0.7926,
      "step": 23220
    },
    {
      "epoch": 1.1680410297666934,
      "grad_norm": 16.75,
      "learning_rate": 1.7663917940466616e-05,
      "loss": 0.9483,
      "step": 23230
    },
    {
      "epoch": 1.168543845534996,
      "grad_norm": 66.0,
      "learning_rate": 1.766291230893001e-05,
      "loss": 0.9957,
      "step": 23240
    },
    {
      "epoch": 1.1690466613032986,
      "grad_norm": 13.75,
      "learning_rate": 1.7661906677393404e-05,
      "loss": 0.9644,
      "step": 23250
    },
    {
      "epoch": 1.169549477071601,
      "grad_norm": 15.0625,
      "learning_rate": 1.76609010458568e-05,
      "loss": 0.8691,
      "step": 23260
    },
    {
      "epoch": 1.1700522928399035,
      "grad_norm": 32.0,
      "learning_rate": 1.7659895414320193e-05,
      "loss": 0.9546,
      "step": 23270
    },
    {
      "epoch": 1.170555108608206,
      "grad_norm": 15.875,
      "learning_rate": 1.7658889782783592e-05,
      "loss": 0.8745,
      "step": 23280
    },
    {
      "epoch": 1.1710579243765085,
      "grad_norm": 12.25,
      "learning_rate": 1.7657884151246984e-05,
      "loss": 0.863,
      "step": 23290
    },
    {
      "epoch": 1.171560740144811,
      "grad_norm": 44.5,
      "learning_rate": 1.765687851971038e-05,
      "loss": 0.968,
      "step": 23300
    },
    {
      "epoch": 1.1720635559131134,
      "grad_norm": 23.75,
      "learning_rate": 1.7655872888173776e-05,
      "loss": 1.3652,
      "step": 23310
    },
    {
      "epoch": 1.1725663716814159,
      "grad_norm": 44.25,
      "learning_rate": 1.765486725663717e-05,
      "loss": 0.7615,
      "step": 23320
    },
    {
      "epoch": 1.1730691874497183,
      "grad_norm": 42.25,
      "learning_rate": 1.7653861625100565e-05,
      "loss": 1.1396,
      "step": 23330
    },
    {
      "epoch": 1.173572003218021,
      "grad_norm": 32.25,
      "learning_rate": 1.765285599356396e-05,
      "loss": 1.2715,
      "step": 23340
    },
    {
      "epoch": 1.1740748189863235,
      "grad_norm": 17.875,
      "learning_rate": 1.7651850362027353e-05,
      "loss": 0.9486,
      "step": 23350
    },
    {
      "epoch": 1.174577634754626,
      "grad_norm": 12.3125,
      "learning_rate": 1.765084473049075e-05,
      "loss": 0.6887,
      "step": 23360
    },
    {
      "epoch": 1.1750804505229284,
      "grad_norm": 5.71875,
      "learning_rate": 1.7649839098954145e-05,
      "loss": 0.8763,
      "step": 23370
    },
    {
      "epoch": 1.1755832662912309,
      "grad_norm": 18.5,
      "learning_rate": 1.7648833467417537e-05,
      "loss": 0.8562,
      "step": 23380
    },
    {
      "epoch": 1.1760860820595334,
      "grad_norm": 19.375,
      "learning_rate": 1.7647827835880936e-05,
      "loss": 1.08,
      "step": 23390
    },
    {
      "epoch": 1.1765888978278358,
      "grad_norm": 5.6875,
      "learning_rate": 1.764682220434433e-05,
      "loss": 1.1091,
      "step": 23400
    },
    {
      "epoch": 1.1770917135961383,
      "grad_norm": 48.0,
      "learning_rate": 1.7645816572807725e-05,
      "loss": 0.5942,
      "step": 23410
    },
    {
      "epoch": 1.1775945293644408,
      "grad_norm": 10.625,
      "learning_rate": 1.764481094127112e-05,
      "loss": 0.7779,
      "step": 23420
    },
    {
      "epoch": 1.1780973451327434,
      "grad_norm": 6.78125,
      "learning_rate": 1.7643805309734513e-05,
      "loss": 1.1771,
      "step": 23430
    },
    {
      "epoch": 1.178600160901046,
      "grad_norm": 7.46875,
      "learning_rate": 1.764279967819791e-05,
      "loss": 0.921,
      "step": 23440
    },
    {
      "epoch": 1.1791029766693484,
      "grad_norm": 7.3125,
      "learning_rate": 1.7641794046661305e-05,
      "loss": 1.1787,
      "step": 23450
    },
    {
      "epoch": 1.1796057924376508,
      "grad_norm": 10.375,
      "learning_rate": 1.7640788415124697e-05,
      "loss": 1.1628,
      "step": 23460
    },
    {
      "epoch": 1.1801086082059533,
      "grad_norm": 11.4375,
      "learning_rate": 1.7639782783588097e-05,
      "loss": 1.032,
      "step": 23470
    },
    {
      "epoch": 1.1806114239742558,
      "grad_norm": 81.0,
      "learning_rate": 1.763877715205149e-05,
      "loss": 1.0787,
      "step": 23480
    },
    {
      "epoch": 1.1811142397425582,
      "grad_norm": 4.625,
      "learning_rate": 1.7637771520514885e-05,
      "loss": 1.0088,
      "step": 23490
    },
    {
      "epoch": 1.181617055510861,
      "grad_norm": 16.0,
      "learning_rate": 1.763676588897828e-05,
      "loss": 0.9408,
      "step": 23500
    },
    {
      "epoch": 1.181617055510861,
      "eval_accuracy": 0.5123450669310857,
      "eval_loss": 1.027835488319397,
      "eval_runtime": 465.4206,
      "eval_samples_per_second": 86.674,
      "eval_steps_per_second": 86.674,
      "step": 23500
    },
    {
      "epoch": 1.1821198712791634,
      "grad_norm": 32.75,
      "learning_rate": 1.7635760257441673e-05,
      "loss": 1.0849,
      "step": 23510
    },
    {
      "epoch": 1.1826226870474659,
      "grad_norm": 13.8125,
      "learning_rate": 1.763475462590507e-05,
      "loss": 0.8151,
      "step": 23520
    },
    {
      "epoch": 1.1831255028157683,
      "grad_norm": 16.625,
      "learning_rate": 1.7633748994368465e-05,
      "loss": 1.1236,
      "step": 23530
    },
    {
      "epoch": 1.1836283185840708,
      "grad_norm": 8.3125,
      "learning_rate": 1.7632743362831858e-05,
      "loss": 0.8569,
      "step": 23540
    },
    {
      "epoch": 1.1841311343523733,
      "grad_norm": 48.0,
      "learning_rate": 1.7631737731295257e-05,
      "loss": 1.125,
      "step": 23550
    },
    {
      "epoch": 1.1846339501206757,
      "grad_norm": 56.0,
      "learning_rate": 1.763073209975865e-05,
      "loss": 1.2212,
      "step": 23560
    },
    {
      "epoch": 1.1851367658889782,
      "grad_norm": 4.40625,
      "learning_rate": 1.7629726468222045e-05,
      "loss": 0.9803,
      "step": 23570
    },
    {
      "epoch": 1.1856395816572807,
      "grad_norm": 4.625,
      "learning_rate": 1.762872083668544e-05,
      "loss": 1.09,
      "step": 23580
    },
    {
      "epoch": 1.1861423974255834,
      "grad_norm": 21.875,
      "learning_rate": 1.7627715205148834e-05,
      "loss": 0.8297,
      "step": 23590
    },
    {
      "epoch": 1.1866452131938858,
      "grad_norm": 90.0,
      "learning_rate": 1.762670957361223e-05,
      "loss": 1.0218,
      "step": 23600
    },
    {
      "epoch": 1.1871480289621883,
      "grad_norm": 7.6875,
      "learning_rate": 1.7625703942075625e-05,
      "loss": 0.9486,
      "step": 23610
    },
    {
      "epoch": 1.1876508447304908,
      "grad_norm": 5.15625,
      "learning_rate": 1.7624698310539018e-05,
      "loss": 0.7103,
      "step": 23620
    },
    {
      "epoch": 1.1881536604987932,
      "grad_norm": 13.5625,
      "learning_rate": 1.7623692679002414e-05,
      "loss": 1.1761,
      "step": 23630
    },
    {
      "epoch": 1.1886564762670957,
      "grad_norm": 48.25,
      "learning_rate": 1.762268704746581e-05,
      "loss": 1.2124,
      "step": 23640
    },
    {
      "epoch": 1.1891592920353982,
      "grad_norm": 21.375,
      "learning_rate": 1.7621681415929206e-05,
      "loss": 0.882,
      "step": 23650
    },
    {
      "epoch": 1.1896621078037006,
      "grad_norm": 27.5,
      "learning_rate": 1.76206757843926e-05,
      "loss": 1.2112,
      "step": 23660
    },
    {
      "epoch": 1.190164923572003,
      "grad_norm": 16.125,
      "learning_rate": 1.7619670152855994e-05,
      "loss": 1.114,
      "step": 23670
    },
    {
      "epoch": 1.1906677393403058,
      "grad_norm": 15.4375,
      "learning_rate": 1.761866452131939e-05,
      "loss": 0.6975,
      "step": 23680
    },
    {
      "epoch": 1.1911705551086083,
      "grad_norm": 26.875,
      "learning_rate": 1.7617658889782786e-05,
      "loss": 0.8997,
      "step": 23690
    },
    {
      "epoch": 1.1916733708769107,
      "grad_norm": 20.25,
      "learning_rate": 1.7616653258246178e-05,
      "loss": 0.9427,
      "step": 23700
    },
    {
      "epoch": 1.1921761866452132,
      "grad_norm": 27.625,
      "learning_rate": 1.7615647626709574e-05,
      "loss": 1.257,
      "step": 23710
    },
    {
      "epoch": 1.1926790024135157,
      "grad_norm": 49.75,
      "learning_rate": 1.761464199517297e-05,
      "loss": 0.6566,
      "step": 23720
    },
    {
      "epoch": 1.1931818181818181,
      "grad_norm": 87.5,
      "learning_rate": 1.7613636363636366e-05,
      "loss": 1.435,
      "step": 23730
    },
    {
      "epoch": 1.1936846339501206,
      "grad_norm": 14.4375,
      "learning_rate": 1.761263073209976e-05,
      "loss": 0.8452,
      "step": 23740
    },
    {
      "epoch": 1.1941874497184233,
      "grad_norm": 24.375,
      "learning_rate": 1.7611625100563154e-05,
      "loss": 1.0094,
      "step": 23750
    },
    {
      "epoch": 1.1946902654867257,
      "grad_norm": 78.5,
      "learning_rate": 1.761061946902655e-05,
      "loss": 1.4776,
      "step": 23760
    },
    {
      "epoch": 1.1951930812550282,
      "grad_norm": 13.0625,
      "learning_rate": 1.7609613837489946e-05,
      "loss": 1.0456,
      "step": 23770
    },
    {
      "epoch": 1.1956958970233307,
      "grad_norm": 39.5,
      "learning_rate": 1.760860820595334e-05,
      "loss": 1.1381,
      "step": 23780
    },
    {
      "epoch": 1.1961987127916331,
      "grad_norm": 61.75,
      "learning_rate": 1.7607602574416734e-05,
      "loss": 1.2111,
      "step": 23790
    },
    {
      "epoch": 1.1967015285599356,
      "grad_norm": 34.5,
      "learning_rate": 1.760659694288013e-05,
      "loss": 0.9401,
      "step": 23800
    },
    {
      "epoch": 1.197204344328238,
      "grad_norm": 50.5,
      "learning_rate": 1.7605591311343526e-05,
      "loss": 1.1804,
      "step": 23810
    },
    {
      "epoch": 1.1977071600965405,
      "grad_norm": 20.375,
      "learning_rate": 1.7604585679806922e-05,
      "loss": 0.8857,
      "step": 23820
    },
    {
      "epoch": 1.198209975864843,
      "grad_norm": 27.75,
      "learning_rate": 1.7603580048270314e-05,
      "loss": 1.0728,
      "step": 23830
    },
    {
      "epoch": 1.1987127916331457,
      "grad_norm": 14.875,
      "learning_rate": 1.760257441673371e-05,
      "loss": 0.7973,
      "step": 23840
    },
    {
      "epoch": 1.1992156074014482,
      "grad_norm": 27.75,
      "learning_rate": 1.7601568785197106e-05,
      "loss": 0.8246,
      "step": 23850
    },
    {
      "epoch": 1.1997184231697506,
      "grad_norm": 11.75,
      "learning_rate": 1.76005631536605e-05,
      "loss": 1.0531,
      "step": 23860
    },
    {
      "epoch": 1.200221238938053,
      "grad_norm": 34.0,
      "learning_rate": 1.7599557522123895e-05,
      "loss": 1.1293,
      "step": 23870
    },
    {
      "epoch": 1.2007240547063556,
      "grad_norm": 40.75,
      "learning_rate": 1.759855189058729e-05,
      "loss": 0.8612,
      "step": 23880
    },
    {
      "epoch": 1.201226870474658,
      "grad_norm": 12.875,
      "learning_rate": 1.7597546259050686e-05,
      "loss": 1.0903,
      "step": 23890
    },
    {
      "epoch": 1.2017296862429605,
      "grad_norm": 23.375,
      "learning_rate": 1.759654062751408e-05,
      "loss": 1.0791,
      "step": 23900
    },
    {
      "epoch": 1.2022325020112632,
      "grad_norm": 15.0625,
      "learning_rate": 1.7595534995977475e-05,
      "loss": 0.8966,
      "step": 23910
    },
    {
      "epoch": 1.2027353177795657,
      "grad_norm": 43.5,
      "learning_rate": 1.759452936444087e-05,
      "loss": 1.1129,
      "step": 23920
    },
    {
      "epoch": 1.2032381335478681,
      "grad_norm": 4.78125,
      "learning_rate": 1.7593523732904266e-05,
      "loss": 1.0212,
      "step": 23930
    },
    {
      "epoch": 1.2037409493161706,
      "grad_norm": 10.375,
      "learning_rate": 1.759251810136766e-05,
      "loss": 0.8902,
      "step": 23940
    },
    {
      "epoch": 1.204243765084473,
      "grad_norm": 42.0,
      "learning_rate": 1.7591512469831055e-05,
      "loss": 1.0692,
      "step": 23950
    },
    {
      "epoch": 1.2047465808527755,
      "grad_norm": 3.109375,
      "learning_rate": 1.759050683829445e-05,
      "loss": 0.8836,
      "step": 23960
    },
    {
      "epoch": 1.205249396621078,
      "grad_norm": 14.0625,
      "learning_rate": 1.7589501206757847e-05,
      "loss": 0.873,
      "step": 23970
    },
    {
      "epoch": 1.2057522123893805,
      "grad_norm": 18.625,
      "learning_rate": 1.758849557522124e-05,
      "loss": 0.8087,
      "step": 23980
    },
    {
      "epoch": 1.206255028157683,
      "grad_norm": 8.375,
      "learning_rate": 1.7587489943684635e-05,
      "loss": 1.0753,
      "step": 23990
    },
    {
      "epoch": 1.2067578439259856,
      "grad_norm": 21.375,
      "learning_rate": 1.758648431214803e-05,
      "loss": 0.8194,
      "step": 24000
    },
    {
      "epoch": 1.2067578439259856,
      "eval_accuracy": 0.5111303916707982,
      "eval_loss": 1.026992678642273,
      "eval_runtime": 464.5281,
      "eval_samples_per_second": 86.841,
      "eval_steps_per_second": 86.841,
      "step": 24000
    },
    {
      "epoch": 1.207260659694288,
      "grad_norm": 17.5,
      "learning_rate": 1.7585478680611427e-05,
      "loss": 1.0922,
      "step": 24010
    },
    {
      "epoch": 1.2077634754625906,
      "grad_norm": 55.75,
      "learning_rate": 1.758447304907482e-05,
      "loss": 0.9839,
      "step": 24020
    },
    {
      "epoch": 1.208266291230893,
      "grad_norm": 35.25,
      "learning_rate": 1.7583467417538215e-05,
      "loss": 1.1738,
      "step": 24030
    },
    {
      "epoch": 1.2087691069991955,
      "grad_norm": 15.6875,
      "learning_rate": 1.758246178600161e-05,
      "loss": 1.0115,
      "step": 24040
    },
    {
      "epoch": 1.209271922767498,
      "grad_norm": 15.3125,
      "learning_rate": 1.7581456154465007e-05,
      "loss": 0.8306,
      "step": 24050
    },
    {
      "epoch": 1.2097747385358004,
      "grad_norm": 6.625,
      "learning_rate": 1.75804505229284e-05,
      "loss": 0.9243,
      "step": 24060
    },
    {
      "epoch": 1.2102775543041029,
      "grad_norm": 29.75,
      "learning_rate": 1.7579444891391795e-05,
      "loss": 1.0863,
      "step": 24070
    },
    {
      "epoch": 1.2107803700724054,
      "grad_norm": 14.875,
      "learning_rate": 1.757843925985519e-05,
      "loss": 1.0515,
      "step": 24080
    },
    {
      "epoch": 1.211283185840708,
      "grad_norm": 15.0625,
      "learning_rate": 1.7577433628318587e-05,
      "loss": 1.0031,
      "step": 24090
    },
    {
      "epoch": 1.2117860016090105,
      "grad_norm": 6.9375,
      "learning_rate": 1.757642799678198e-05,
      "loss": 0.9882,
      "step": 24100
    },
    {
      "epoch": 1.212288817377313,
      "grad_norm": 15.5,
      "learning_rate": 1.7575422365245375e-05,
      "loss": 0.9528,
      "step": 24110
    },
    {
      "epoch": 1.2127916331456154,
      "grad_norm": 15.125,
      "learning_rate": 1.757441673370877e-05,
      "loss": 0.947,
      "step": 24120
    },
    {
      "epoch": 1.213294448913918,
      "grad_norm": 12.1875,
      "learning_rate": 1.7573411102172167e-05,
      "loss": 0.8629,
      "step": 24130
    },
    {
      "epoch": 1.2137972646822204,
      "grad_norm": 39.5,
      "learning_rate": 1.757240547063556e-05,
      "loss": 1.1496,
      "step": 24140
    },
    {
      "epoch": 1.2143000804505228,
      "grad_norm": 5.125,
      "learning_rate": 1.7571399839098955e-05,
      "loss": 1.0487,
      "step": 24150
    },
    {
      "epoch": 1.2148028962188255,
      "grad_norm": 8.125,
      "learning_rate": 1.757039420756235e-05,
      "loss": 0.8648,
      "step": 24160
    },
    {
      "epoch": 1.215305711987128,
      "grad_norm": 10.875,
      "learning_rate": 1.7569388576025744e-05,
      "loss": 0.9609,
      "step": 24170
    },
    {
      "epoch": 1.2158085277554305,
      "grad_norm": 30.125,
      "learning_rate": 1.756838294448914e-05,
      "loss": 1.2957,
      "step": 24180
    },
    {
      "epoch": 1.216311343523733,
      "grad_norm": 22.125,
      "learning_rate": 1.7567377312952536e-05,
      "loss": 0.6935,
      "step": 24190
    },
    {
      "epoch": 1.2168141592920354,
      "grad_norm": 17.5,
      "learning_rate": 1.756637168141593e-05,
      "loss": 0.8464,
      "step": 24200
    },
    {
      "epoch": 1.2173169750603379,
      "grad_norm": 9.4375,
      "learning_rate": 1.7565366049879327e-05,
      "loss": 0.852,
      "step": 24210
    },
    {
      "epoch": 1.2178197908286403,
      "grad_norm": 28.75,
      "learning_rate": 1.756436041834272e-05,
      "loss": 1.0455,
      "step": 24220
    },
    {
      "epoch": 1.2183226065969428,
      "grad_norm": 9.25,
      "learning_rate": 1.7563354786806116e-05,
      "loss": 0.8503,
      "step": 24230
    },
    {
      "epoch": 1.2188254223652453,
      "grad_norm": 10.8125,
      "learning_rate": 1.756234915526951e-05,
      "loss": 1.0461,
      "step": 24240
    },
    {
      "epoch": 1.219328238133548,
      "grad_norm": 25.125,
      "learning_rate": 1.7561343523732904e-05,
      "loss": 1.0022,
      "step": 24250
    },
    {
      "epoch": 1.2198310539018504,
      "grad_norm": 14.4375,
      "learning_rate": 1.75603378921963e-05,
      "loss": 1.2905,
      "step": 24260
    },
    {
      "epoch": 1.220333869670153,
      "grad_norm": 29.625,
      "learning_rate": 1.7559332260659696e-05,
      "loss": 0.8372,
      "step": 24270
    },
    {
      "epoch": 1.2208366854384554,
      "grad_norm": 40.75,
      "learning_rate": 1.7558326629123092e-05,
      "loss": 1.0122,
      "step": 24280
    },
    {
      "epoch": 1.2213395012067578,
      "grad_norm": 70.0,
      "learning_rate": 1.7557320997586488e-05,
      "loss": 1.0579,
      "step": 24290
    },
    {
      "epoch": 1.2218423169750603,
      "grad_norm": 28.5,
      "learning_rate": 1.755631536604988e-05,
      "loss": 1.0476,
      "step": 24300
    },
    {
      "epoch": 1.2223451327433628,
      "grad_norm": 31.875,
      "learning_rate": 1.7555309734513276e-05,
      "loss": 1.16,
      "step": 24310
    },
    {
      "epoch": 1.2228479485116655,
      "grad_norm": 15.75,
      "learning_rate": 1.7554304102976672e-05,
      "loss": 0.8882,
      "step": 24320
    },
    {
      "epoch": 1.2233507642799677,
      "grad_norm": 13.5625,
      "learning_rate": 1.7553298471440064e-05,
      "loss": 1.0403,
      "step": 24330
    },
    {
      "epoch": 1.2238535800482704,
      "grad_norm": 47.0,
      "learning_rate": 1.755229283990346e-05,
      "loss": 1.2663,
      "step": 24340
    },
    {
      "epoch": 1.2243563958165729,
      "grad_norm": 63.25,
      "learning_rate": 1.7551287208366856e-05,
      "loss": 1.3382,
      "step": 24350
    },
    {
      "epoch": 1.2248592115848753,
      "grad_norm": 18.125,
      "learning_rate": 1.7550281576830252e-05,
      "loss": 0.8724,
      "step": 24360
    },
    {
      "epoch": 1.2253620273531778,
      "grad_norm": 11.1875,
      "learning_rate": 1.7549275945293648e-05,
      "loss": 0.8227,
      "step": 24370
    },
    {
      "epoch": 1.2258648431214803,
      "grad_norm": 16.375,
      "learning_rate": 1.754827031375704e-05,
      "loss": 0.8842,
      "step": 24380
    },
    {
      "epoch": 1.2263676588897827,
      "grad_norm": 20.25,
      "learning_rate": 1.7547264682220436e-05,
      "loss": 1.1674,
      "step": 24390
    },
    {
      "epoch": 1.2268704746580852,
      "grad_norm": 46.0,
      "learning_rate": 1.7546259050683832e-05,
      "loss": 1.475,
      "step": 24400
    },
    {
      "epoch": 1.2273732904263879,
      "grad_norm": 43.0,
      "learning_rate": 1.7545253419147225e-05,
      "loss": 1.0684,
      "step": 24410
    },
    {
      "epoch": 1.2278761061946903,
      "grad_norm": 17.875,
      "learning_rate": 1.754424778761062e-05,
      "loss": 0.6315,
      "step": 24420
    },
    {
      "epoch": 1.2283789219629928,
      "grad_norm": 23.25,
      "learning_rate": 1.7543242156074016e-05,
      "loss": 0.9833,
      "step": 24430
    },
    {
      "epoch": 1.2288817377312953,
      "grad_norm": 24.25,
      "learning_rate": 1.754223652453741e-05,
      "loss": 0.911,
      "step": 24440
    },
    {
      "epoch": 1.2293845534995977,
      "grad_norm": 20.625,
      "learning_rate": 1.7541230893000808e-05,
      "loss": 0.9227,
      "step": 24450
    },
    {
      "epoch": 1.2298873692679002,
      "grad_norm": 22.25,
      "learning_rate": 1.75402252614642e-05,
      "loss": 0.8765,
      "step": 24460
    },
    {
      "epoch": 1.2303901850362027,
      "grad_norm": 19.875,
      "learning_rate": 1.7539219629927596e-05,
      "loss": 1.1736,
      "step": 24470
    },
    {
      "epoch": 1.2308930008045051,
      "grad_norm": 19.625,
      "learning_rate": 1.7538213998390992e-05,
      "loss": 0.9268,
      "step": 24480
    },
    {
      "epoch": 1.2313958165728076,
      "grad_norm": 27.375,
      "learning_rate": 1.7537208366854385e-05,
      "loss": 0.7001,
      "step": 24490
    },
    {
      "epoch": 1.2318986323411103,
      "grad_norm": 37.5,
      "learning_rate": 1.753620273531778e-05,
      "loss": 0.95,
      "step": 24500
    },
    {
      "epoch": 1.2318986323411103,
      "eval_accuracy": 0.5124938026772434,
      "eval_loss": 1.0269086360931396,
      "eval_runtime": 464.5162,
      "eval_samples_per_second": 86.843,
      "eval_steps_per_second": 86.843,
      "step": 24500
    },
    {
      "epoch": 1.2324014481094128,
      "grad_norm": 8.625,
      "learning_rate": 1.7535197103781177e-05,
      "loss": 0.76,
      "step": 24510
    },
    {
      "epoch": 1.2329042638777152,
      "grad_norm": 62.0,
      "learning_rate": 1.753419147224457e-05,
      "loss": 0.8921,
      "step": 24520
    },
    {
      "epoch": 1.2334070796460177,
      "grad_norm": 25.125,
      "learning_rate": 1.753318584070797e-05,
      "loss": 0.9695,
      "step": 24530
    },
    {
      "epoch": 1.2339098954143202,
      "grad_norm": 10.5625,
      "learning_rate": 1.753218020917136e-05,
      "loss": 0.918,
      "step": 24540
    },
    {
      "epoch": 1.2344127111826226,
      "grad_norm": 4.375,
      "learning_rate": 1.7531174577634757e-05,
      "loss": 0.9681,
      "step": 24550
    },
    {
      "epoch": 1.234915526950925,
      "grad_norm": 46.5,
      "learning_rate": 1.7530168946098153e-05,
      "loss": 1.3049,
      "step": 24560
    },
    {
      "epoch": 1.2354183427192278,
      "grad_norm": 12.4375,
      "learning_rate": 1.7529163314561545e-05,
      "loss": 0.7567,
      "step": 24570
    },
    {
      "epoch": 1.2359211584875303,
      "grad_norm": 21.625,
      "learning_rate": 1.752815768302494e-05,
      "loss": 1.2744,
      "step": 24580
    },
    {
      "epoch": 1.2364239742558327,
      "grad_norm": 10.5625,
      "learning_rate": 1.7527152051488337e-05,
      "loss": 0.7922,
      "step": 24590
    },
    {
      "epoch": 1.2369267900241352,
      "grad_norm": 17.375,
      "learning_rate": 1.752614641995173e-05,
      "loss": 0.9628,
      "step": 24600
    },
    {
      "epoch": 1.2374296057924377,
      "grad_norm": 37.25,
      "learning_rate": 1.752514078841513e-05,
      "loss": 1.1672,
      "step": 24610
    },
    {
      "epoch": 1.2379324215607401,
      "grad_norm": 29.25,
      "learning_rate": 1.752413515687852e-05,
      "loss": 1.0309,
      "step": 24620
    },
    {
      "epoch": 1.2384352373290426,
      "grad_norm": 12.5625,
      "learning_rate": 1.7523129525341917e-05,
      "loss": 1.0564,
      "step": 24630
    },
    {
      "epoch": 1.238938053097345,
      "grad_norm": 33.5,
      "learning_rate": 1.7522123893805313e-05,
      "loss": 0.7376,
      "step": 24640
    },
    {
      "epoch": 1.2394408688656475,
      "grad_norm": 15.3125,
      "learning_rate": 1.7521118262268705e-05,
      "loss": 1.1451,
      "step": 24650
    },
    {
      "epoch": 1.2399436846339502,
      "grad_norm": 7.78125,
      "learning_rate": 1.75201126307321e-05,
      "loss": 0.9741,
      "step": 24660
    },
    {
      "epoch": 1.2404465004022527,
      "grad_norm": 11.6875,
      "learning_rate": 1.7519106999195497e-05,
      "loss": 0.847,
      "step": 24670
    },
    {
      "epoch": 1.2409493161705552,
      "grad_norm": 12.9375,
      "learning_rate": 1.751810136765889e-05,
      "loss": 1.0629,
      "step": 24680
    },
    {
      "epoch": 1.2414521319388576,
      "grad_norm": 52.75,
      "learning_rate": 1.7517095736122285e-05,
      "loss": 1.2336,
      "step": 24690
    },
    {
      "epoch": 1.24195494770716,
      "grad_norm": 9.0,
      "learning_rate": 1.751609010458568e-05,
      "loss": 0.8113,
      "step": 24700
    },
    {
      "epoch": 1.2424577634754626,
      "grad_norm": 60.5,
      "learning_rate": 1.7515084473049074e-05,
      "loss": 1.0329,
      "step": 24710
    },
    {
      "epoch": 1.242960579243765,
      "grad_norm": 12.0625,
      "learning_rate": 1.7514078841512473e-05,
      "loss": 0.9797,
      "step": 24720
    },
    {
      "epoch": 1.2434633950120675,
      "grad_norm": 17.0,
      "learning_rate": 1.7513073209975866e-05,
      "loss": 0.922,
      "step": 24730
    },
    {
      "epoch": 1.24396621078037,
      "grad_norm": 10.6875,
      "learning_rate": 1.751206757843926e-05,
      "loss": 1.0215,
      "step": 24740
    },
    {
      "epoch": 1.2444690265486726,
      "grad_norm": 52.0,
      "learning_rate": 1.7511061946902657e-05,
      "loss": 0.9568,
      "step": 24750
    },
    {
      "epoch": 1.244971842316975,
      "grad_norm": 15.5625,
      "learning_rate": 1.751005631536605e-05,
      "loss": 0.8775,
      "step": 24760
    },
    {
      "epoch": 1.2454746580852776,
      "grad_norm": 37.5,
      "learning_rate": 1.7509050683829446e-05,
      "loss": 1.0183,
      "step": 24770
    },
    {
      "epoch": 1.24597747385358,
      "grad_norm": 8.5625,
      "learning_rate": 1.750804505229284e-05,
      "loss": 0.9632,
      "step": 24780
    },
    {
      "epoch": 1.2464802896218825,
      "grad_norm": 29.0,
      "learning_rate": 1.7507039420756234e-05,
      "loss": 1.1757,
      "step": 24790
    },
    {
      "epoch": 1.246983105390185,
      "grad_norm": 5.53125,
      "learning_rate": 1.7506033789219633e-05,
      "loss": 0.7569,
      "step": 24800
    },
    {
      "epoch": 1.2474859211584874,
      "grad_norm": 22.125,
      "learning_rate": 1.7505028157683026e-05,
      "loss": 1.535,
      "step": 24810
    },
    {
      "epoch": 1.2479887369267901,
      "grad_norm": 19.0,
      "learning_rate": 1.7504022526146422e-05,
      "loss": 0.8728,
      "step": 24820
    },
    {
      "epoch": 1.2484915526950926,
      "grad_norm": 28.625,
      "learning_rate": 1.7503016894609818e-05,
      "loss": 1.1287,
      "step": 24830
    },
    {
      "epoch": 1.248994368463395,
      "grad_norm": 38.75,
      "learning_rate": 1.750201126307321e-05,
      "loss": 0.9899,
      "step": 24840
    },
    {
      "epoch": 1.2494971842316975,
      "grad_norm": 42.75,
      "learning_rate": 1.7501005631536606e-05,
      "loss": 1.0035,
      "step": 24850
    },
    {
      "epoch": 1.25,
      "grad_norm": 34.25,
      "learning_rate": 1.7500000000000002e-05,
      "loss": 1.0257,
      "step": 24860
    },
    {
      "epoch": 1.2505028157683025,
      "grad_norm": 28.375,
      "learning_rate": 1.7498994368463394e-05,
      "loss": 0.9122,
      "step": 24870
    },
    {
      "epoch": 1.251005631536605,
      "grad_norm": 14.625,
      "learning_rate": 1.7497988736926794e-05,
      "loss": 0.8518,
      "step": 24880
    },
    {
      "epoch": 1.2515084473049074,
      "grad_norm": 33.25,
      "learning_rate": 1.7496983105390186e-05,
      "loss": 0.8781,
      "step": 24890
    },
    {
      "epoch": 1.2520112630732099,
      "grad_norm": 12.0625,
      "learning_rate": 1.7495977473853582e-05,
      "loss": 0.9869,
      "step": 24900
    },
    {
      "epoch": 1.2525140788415126,
      "grad_norm": 12.75,
      "learning_rate": 1.7494971842316978e-05,
      "loss": 0.7633,
      "step": 24910
    },
    {
      "epoch": 1.253016894609815,
      "grad_norm": 19.375,
      "learning_rate": 1.749396621078037e-05,
      "loss": 0.9641,
      "step": 24920
    },
    {
      "epoch": 1.2535197103781175,
      "grad_norm": 18.375,
      "learning_rate": 1.7492960579243766e-05,
      "loss": 0.9392,
      "step": 24930
    },
    {
      "epoch": 1.25402252614642,
      "grad_norm": 30.0,
      "learning_rate": 1.7491954947707162e-05,
      "loss": 1.3341,
      "step": 24940
    },
    {
      "epoch": 1.2545253419147224,
      "grad_norm": 5.09375,
      "learning_rate": 1.7490949316170555e-05,
      "loss": 1.1869,
      "step": 24950
    },
    {
      "epoch": 1.255028157683025,
      "grad_norm": 20.875,
      "learning_rate": 1.748994368463395e-05,
      "loss": 1.211,
      "step": 24960
    },
    {
      "epoch": 1.2555309734513274,
      "grad_norm": 15.125,
      "learning_rate": 1.7488938053097346e-05,
      "loss": 0.8436,
      "step": 24970
    },
    {
      "epoch": 1.25603378921963,
      "grad_norm": 11.8125,
      "learning_rate": 1.7487932421560742e-05,
      "loss": 0.8189,
      "step": 24980
    },
    {
      "epoch": 1.2565366049879323,
      "grad_norm": 32.5,
      "learning_rate": 1.7486926790024138e-05,
      "loss": 0.9909,
      "step": 24990
    },
    {
      "epoch": 1.257039420756235,
      "grad_norm": 21.375,
      "learning_rate": 1.748592115848753e-05,
      "loss": 0.999,
      "step": 25000
    },
    {
      "epoch": 1.257039420756235,
      "eval_accuracy": 0.5123698562221121,
      "eval_loss": 1.0257246494293213,
      "eval_runtime": 463.7658,
      "eval_samples_per_second": 86.984,
      "eval_steps_per_second": 86.984,
      "step": 25000
    },
    {
      "epoch": 1.2575422365245374,
      "grad_norm": 54.0,
      "learning_rate": 1.7484915526950926e-05,
      "loss": 0.9596,
      "step": 25010
    },
    {
      "epoch": 1.25804505229284,
      "grad_norm": 28.625,
      "learning_rate": 1.7483909895414322e-05,
      "loss": 1.0657,
      "step": 25020
    },
    {
      "epoch": 1.2585478680611424,
      "grad_norm": 33.5,
      "learning_rate": 1.7482904263877715e-05,
      "loss": 1.0772,
      "step": 25030
    },
    {
      "epoch": 1.2590506838294448,
      "grad_norm": 16.0,
      "learning_rate": 1.748189863234111e-05,
      "loss": 0.9709,
      "step": 25040
    },
    {
      "epoch": 1.2595534995977473,
      "grad_norm": 21.25,
      "learning_rate": 1.7480893000804507e-05,
      "loss": 0.783,
      "step": 25050
    },
    {
      "epoch": 1.2600563153660498,
      "grad_norm": 18.875,
      "learning_rate": 1.7479887369267902e-05,
      "loss": 1.0462,
      "step": 25060
    },
    {
      "epoch": 1.2605591311343525,
      "grad_norm": 12.6875,
      "learning_rate": 1.74788817377313e-05,
      "loss": 0.7678,
      "step": 25070
    },
    {
      "epoch": 1.261061946902655,
      "grad_norm": 40.0,
      "learning_rate": 1.747787610619469e-05,
      "loss": 1.1915,
      "step": 25080
    },
    {
      "epoch": 1.2615647626709574,
      "grad_norm": 16.375,
      "learning_rate": 1.7476870474658087e-05,
      "loss": 0.8261,
      "step": 25090
    },
    {
      "epoch": 1.2620675784392599,
      "grad_norm": 11.5625,
      "learning_rate": 1.7475864843121483e-05,
      "loss": 1.0281,
      "step": 25100
    },
    {
      "epoch": 1.2625703942075623,
      "grad_norm": 32.75,
      "learning_rate": 1.7474859211584875e-05,
      "loss": 0.9481,
      "step": 25110
    },
    {
      "epoch": 1.2630732099758648,
      "grad_norm": 17.625,
      "learning_rate": 1.747385358004827e-05,
      "loss": 0.9939,
      "step": 25120
    },
    {
      "epoch": 1.2635760257441673,
      "grad_norm": 10.4375,
      "learning_rate": 1.7472847948511667e-05,
      "loss": 1.0496,
      "step": 25130
    },
    {
      "epoch": 1.26407884151247,
      "grad_norm": 57.5,
      "learning_rate": 1.7471842316975063e-05,
      "loss": 1.1696,
      "step": 25140
    },
    {
      "epoch": 1.2645816572807722,
      "grad_norm": 20.125,
      "learning_rate": 1.747083668543846e-05,
      "loss": 0.9824,
      "step": 25150
    },
    {
      "epoch": 1.265084473049075,
      "grad_norm": 72.0,
      "learning_rate": 1.746983105390185e-05,
      "loss": 1.0177,
      "step": 25160
    },
    {
      "epoch": 1.2655872888173774,
      "grad_norm": 20.375,
      "learning_rate": 1.7468825422365247e-05,
      "loss": 0.9834,
      "step": 25170
    },
    {
      "epoch": 1.2660901045856798,
      "grad_norm": 40.75,
      "learning_rate": 1.7467819790828643e-05,
      "loss": 0.8383,
      "step": 25180
    },
    {
      "epoch": 1.2665929203539823,
      "grad_norm": 9.9375,
      "learning_rate": 1.7466814159292035e-05,
      "loss": 0.995,
      "step": 25190
    },
    {
      "epoch": 1.2670957361222848,
      "grad_norm": 16.875,
      "learning_rate": 1.746580852775543e-05,
      "loss": 0.8431,
      "step": 25200
    },
    {
      "epoch": 1.2675985518905872,
      "grad_norm": 42.25,
      "learning_rate": 1.7464802896218827e-05,
      "loss": 0.8571,
      "step": 25210
    },
    {
      "epoch": 1.2681013676588897,
      "grad_norm": 16.0,
      "learning_rate": 1.7463797264682223e-05,
      "loss": 1.0199,
      "step": 25220
    },
    {
      "epoch": 1.2686041834271924,
      "grad_norm": 20.125,
      "learning_rate": 1.7462791633145615e-05,
      "loss": 1.0213,
      "step": 25230
    },
    {
      "epoch": 1.2691069991954946,
      "grad_norm": 11.875,
      "learning_rate": 1.746178600160901e-05,
      "loss": 0.9566,
      "step": 25240
    },
    {
      "epoch": 1.2696098149637973,
      "grad_norm": 25.0,
      "learning_rate": 1.7460780370072407e-05,
      "loss": 1.109,
      "step": 25250
    },
    {
      "epoch": 1.2701126307320998,
      "grad_norm": 15.375,
      "learning_rate": 1.7459774738535803e-05,
      "loss": 1.007,
      "step": 25260
    },
    {
      "epoch": 1.2706154465004023,
      "grad_norm": 72.5,
      "learning_rate": 1.7458769106999196e-05,
      "loss": 0.8705,
      "step": 25270
    },
    {
      "epoch": 1.2711182622687047,
      "grad_norm": 10.375,
      "learning_rate": 1.745776347546259e-05,
      "loss": 0.7095,
      "step": 25280
    },
    {
      "epoch": 1.2716210780370072,
      "grad_norm": 21.875,
      "learning_rate": 1.7456757843925987e-05,
      "loss": 0.7567,
      "step": 25290
    },
    {
      "epoch": 1.2721238938053097,
      "grad_norm": 17.25,
      "learning_rate": 1.7455752212389383e-05,
      "loss": 1.0975,
      "step": 25300
    },
    {
      "epoch": 1.2726267095736121,
      "grad_norm": 38.75,
      "learning_rate": 1.7454746580852776e-05,
      "loss": 1.1898,
      "step": 25310
    },
    {
      "epoch": 1.2731295253419148,
      "grad_norm": 9.0,
      "learning_rate": 1.745374094931617e-05,
      "loss": 1.037,
      "step": 25320
    },
    {
      "epoch": 1.2736323411102173,
      "grad_norm": 26.5,
      "learning_rate": 1.7452735317779568e-05,
      "loss": 1.185,
      "step": 25330
    },
    {
      "epoch": 1.2741351568785197,
      "grad_norm": 56.25,
      "learning_rate": 1.7451729686242963e-05,
      "loss": 1.0803,
      "step": 25340
    },
    {
      "epoch": 1.2746379726468222,
      "grad_norm": 36.0,
      "learning_rate": 1.7450724054706356e-05,
      "loss": 0.9887,
      "step": 25350
    },
    {
      "epoch": 1.2751407884151247,
      "grad_norm": 13.6875,
      "learning_rate": 1.7449718423169752e-05,
      "loss": 1.0677,
      "step": 25360
    },
    {
      "epoch": 1.2756436041834271,
      "grad_norm": 11.25,
      "learning_rate": 1.7448712791633148e-05,
      "loss": 0.8358,
      "step": 25370
    },
    {
      "epoch": 1.2761464199517296,
      "grad_norm": 49.75,
      "learning_rate": 1.7447707160096544e-05,
      "loss": 1.0471,
      "step": 25380
    },
    {
      "epoch": 1.2766492357200323,
      "grad_norm": 5.90625,
      "learning_rate": 1.7446701528559936e-05,
      "loss": 1.0326,
      "step": 25390
    },
    {
      "epoch": 1.2771520514883345,
      "grad_norm": 10.875,
      "learning_rate": 1.7445695897023332e-05,
      "loss": 1.0702,
      "step": 25400
    },
    {
      "epoch": 1.2776548672566372,
      "grad_norm": 24.125,
      "learning_rate": 1.7444690265486728e-05,
      "loss": 1.2285,
      "step": 25410
    },
    {
      "epoch": 1.2781576830249397,
      "grad_norm": 9.5,
      "learning_rate": 1.7443684633950124e-05,
      "loss": 0.7725,
      "step": 25420
    },
    {
      "epoch": 1.2786604987932422,
      "grad_norm": 4.90625,
      "learning_rate": 1.7442679002413516e-05,
      "loss": 0.7407,
      "step": 25430
    },
    {
      "epoch": 1.2791633145615446,
      "grad_norm": 37.5,
      "learning_rate": 1.7441673370876912e-05,
      "loss": 0.9036,
      "step": 25440
    },
    {
      "epoch": 1.279666130329847,
      "grad_norm": 29.125,
      "learning_rate": 1.7440667739340308e-05,
      "loss": 1.0671,
      "step": 25450
    },
    {
      "epoch": 1.2801689460981496,
      "grad_norm": 10.625,
      "learning_rate": 1.7439662107803704e-05,
      "loss": 1.0831,
      "step": 25460
    },
    {
      "epoch": 1.280671761866452,
      "grad_norm": 7.875,
      "learning_rate": 1.7438656476267096e-05,
      "loss": 0.936,
      "step": 25470
    },
    {
      "epoch": 1.2811745776347547,
      "grad_norm": 43.5,
      "learning_rate": 1.7437650844730492e-05,
      "loss": 0.8274,
      "step": 25480
    },
    {
      "epoch": 1.2816773934030572,
      "grad_norm": 32.25,
      "learning_rate": 1.7436645213193888e-05,
      "loss": 1.135,
      "step": 25490
    },
    {
      "epoch": 1.2821802091713597,
      "grad_norm": 10.5625,
      "learning_rate": 1.743563958165728e-05,
      "loss": 0.744,
      "step": 25500
    },
    {
      "epoch": 1.2821802091713597,
      "eval_accuracy": 0.5118988596926128,
      "eval_loss": 1.0260111093521118,
      "eval_runtime": 463.295,
      "eval_samples_per_second": 87.072,
      "eval_steps_per_second": 87.072,
      "step": 25500
    },
    {
      "epoch": 1.2826830249396621,
      "grad_norm": 22.125,
      "learning_rate": 1.7434633950120676e-05,
      "loss": 1.1419,
      "step": 25510
    },
    {
      "epoch": 1.2831858407079646,
      "grad_norm": 15.6875,
      "learning_rate": 1.7433628318584072e-05,
      "loss": 1.0762,
      "step": 25520
    },
    {
      "epoch": 1.283688656476267,
      "grad_norm": 9.625,
      "learning_rate": 1.7432622687047468e-05,
      "loss": 0.8612,
      "step": 25530
    },
    {
      "epoch": 1.2841914722445695,
      "grad_norm": 16.875,
      "learning_rate": 1.7431617055510864e-05,
      "loss": 1.2161,
      "step": 25540
    },
    {
      "epoch": 1.2846942880128722,
      "grad_norm": 40.5,
      "learning_rate": 1.7430611423974257e-05,
      "loss": 0.7893,
      "step": 25550
    },
    {
      "epoch": 1.2851971037811745,
      "grad_norm": 61.0,
      "learning_rate": 1.7429605792437652e-05,
      "loss": 1.0139,
      "step": 25560
    },
    {
      "epoch": 1.2856999195494772,
      "grad_norm": 19.125,
      "learning_rate": 1.7428600160901048e-05,
      "loss": 0.8611,
      "step": 25570
    },
    {
      "epoch": 1.2862027353177796,
      "grad_norm": 31.375,
      "learning_rate": 1.742759452936444e-05,
      "loss": 1.2776,
      "step": 25580
    },
    {
      "epoch": 1.286705551086082,
      "grad_norm": 31.375,
      "learning_rate": 1.7426588897827837e-05,
      "loss": 1.0667,
      "step": 25590
    },
    {
      "epoch": 1.2872083668543846,
      "grad_norm": 17.75,
      "learning_rate": 1.7425583266291233e-05,
      "loss": 1.0092,
      "step": 25600
    },
    {
      "epoch": 1.287711182622687,
      "grad_norm": 72.5,
      "learning_rate": 1.742457763475463e-05,
      "loss": 1.4035,
      "step": 25610
    },
    {
      "epoch": 1.2882139983909895,
      "grad_norm": 85.5,
      "learning_rate": 1.7423572003218024e-05,
      "loss": 0.9556,
      "step": 25620
    },
    {
      "epoch": 1.288716814159292,
      "grad_norm": 19.75,
      "learning_rate": 1.7422566371681417e-05,
      "loss": 0.8422,
      "step": 25630
    },
    {
      "epoch": 1.2892196299275946,
      "grad_norm": 17.75,
      "learning_rate": 1.7421560740144813e-05,
      "loss": 1.0653,
      "step": 25640
    },
    {
      "epoch": 1.289722445695897,
      "grad_norm": 8.0,
      "learning_rate": 1.742055510860821e-05,
      "loss": 1.2561,
      "step": 25650
    },
    {
      "epoch": 1.2902252614641996,
      "grad_norm": 36.75,
      "learning_rate": 1.74195494770716e-05,
      "loss": 0.8968,
      "step": 25660
    },
    {
      "epoch": 1.290728077232502,
      "grad_norm": 45.25,
      "learning_rate": 1.7418543845534997e-05,
      "loss": 0.8655,
      "step": 25670
    },
    {
      "epoch": 1.2912308930008045,
      "grad_norm": 18.5,
      "learning_rate": 1.7417538213998393e-05,
      "loss": 1.124,
      "step": 25680
    },
    {
      "epoch": 1.291733708769107,
      "grad_norm": 19.25,
      "learning_rate": 1.741653258246179e-05,
      "loss": 0.9479,
      "step": 25690
    },
    {
      "epoch": 1.2922365245374094,
      "grad_norm": 27.75,
      "learning_rate": 1.7415526950925185e-05,
      "loss": 0.8352,
      "step": 25700
    },
    {
      "epoch": 1.292739340305712,
      "grad_norm": 20.375,
      "learning_rate": 1.7414521319388577e-05,
      "loss": 0.7589,
      "step": 25710
    },
    {
      "epoch": 1.2932421560740144,
      "grad_norm": 83.0,
      "learning_rate": 1.7413515687851973e-05,
      "loss": 0.9514,
      "step": 25720
    },
    {
      "epoch": 1.293744971842317,
      "grad_norm": 16.75,
      "learning_rate": 1.741251005631537e-05,
      "loss": 0.9654,
      "step": 25730
    },
    {
      "epoch": 1.2942477876106195,
      "grad_norm": 12.0,
      "learning_rate": 1.741150442477876e-05,
      "loss": 1.0067,
      "step": 25740
    },
    {
      "epoch": 1.294750603378922,
      "grad_norm": 23.0,
      "learning_rate": 1.7410498793242157e-05,
      "loss": 0.8138,
      "step": 25750
    },
    {
      "epoch": 1.2952534191472245,
      "grad_norm": 9.25,
      "learning_rate": 1.7409493161705553e-05,
      "loss": 0.8336,
      "step": 25760
    },
    {
      "epoch": 1.295756234915527,
      "grad_norm": 8.25,
      "learning_rate": 1.7408487530168946e-05,
      "loss": 1.0741,
      "step": 25770
    },
    {
      "epoch": 1.2962590506838294,
      "grad_norm": 39.75,
      "learning_rate": 1.7407481898632345e-05,
      "loss": 1.2583,
      "step": 25780
    },
    {
      "epoch": 1.2967618664521319,
      "grad_norm": 4.84375,
      "learning_rate": 1.7406476267095737e-05,
      "loss": 0.8397,
      "step": 25790
    },
    {
      "epoch": 1.2972646822204346,
      "grad_norm": 14.9375,
      "learning_rate": 1.7405470635559133e-05,
      "loss": 0.9662,
      "step": 25800
    },
    {
      "epoch": 1.2977674979887368,
      "grad_norm": 10.9375,
      "learning_rate": 1.740446500402253e-05,
      "loss": 0.9867,
      "step": 25810
    },
    {
      "epoch": 1.2982703137570395,
      "grad_norm": 44.25,
      "learning_rate": 1.740345937248592e-05,
      "loss": 1.0857,
      "step": 25820
    },
    {
      "epoch": 1.298773129525342,
      "grad_norm": 21.625,
      "learning_rate": 1.7402453740949317e-05,
      "loss": 1.106,
      "step": 25830
    },
    {
      "epoch": 1.2992759452936444,
      "grad_norm": 27.75,
      "learning_rate": 1.7401448109412713e-05,
      "loss": 0.9848,
      "step": 25840
    },
    {
      "epoch": 1.299778761061947,
      "grad_norm": 26.875,
      "learning_rate": 1.7400442477876106e-05,
      "loss": 1.0565,
      "step": 25850
    },
    {
      "epoch": 1.3002815768302494,
      "grad_norm": 15.5,
      "learning_rate": 1.7399436846339505e-05,
      "loss": 0.9054,
      "step": 25860
    },
    {
      "epoch": 1.3007843925985518,
      "grad_norm": 8.4375,
      "learning_rate": 1.7398431214802898e-05,
      "loss": 0.8897,
      "step": 25870
    },
    {
      "epoch": 1.3012872083668543,
      "grad_norm": 32.5,
      "learning_rate": 1.7397425583266293e-05,
      "loss": 1.1717,
      "step": 25880
    },
    {
      "epoch": 1.301790024135157,
      "grad_norm": 14.875,
      "learning_rate": 1.739641995172969e-05,
      "loss": 1.2335,
      "step": 25890
    },
    {
      "epoch": 1.3022928399034595,
      "grad_norm": 18.25,
      "learning_rate": 1.7395414320193082e-05,
      "loss": 1.1616,
      "step": 25900
    },
    {
      "epoch": 1.302795655671762,
      "grad_norm": 6.34375,
      "learning_rate": 1.7394408688656478e-05,
      "loss": 0.8582,
      "step": 25910
    },
    {
      "epoch": 1.3032984714400644,
      "grad_norm": 60.75,
      "learning_rate": 1.7393403057119874e-05,
      "loss": 1.2188,
      "step": 25920
    },
    {
      "epoch": 1.3038012872083669,
      "grad_norm": 37.0,
      "learning_rate": 1.7392397425583266e-05,
      "loss": 0.8222,
      "step": 25930
    },
    {
      "epoch": 1.3043041029766693,
      "grad_norm": 23.125,
      "learning_rate": 1.7391391794046665e-05,
      "loss": 1.4291,
      "step": 25940
    },
    {
      "epoch": 1.3048069187449718,
      "grad_norm": 16.375,
      "learning_rate": 1.7390386162510058e-05,
      "loss": 1.019,
      "step": 25950
    },
    {
      "epoch": 1.3053097345132743,
      "grad_norm": 29.0,
      "learning_rate": 1.7389380530973454e-05,
      "loss": 1.0668,
      "step": 25960
    },
    {
      "epoch": 1.3058125502815767,
      "grad_norm": 12.875,
      "learning_rate": 1.738837489943685e-05,
      "loss": 0.9316,
      "step": 25970
    },
    {
      "epoch": 1.3063153660498794,
      "grad_norm": 16.75,
      "learning_rate": 1.7387369267900242e-05,
      "loss": 1.1288,
      "step": 25980
    },
    {
      "epoch": 1.3068181818181819,
      "grad_norm": 26.5,
      "learning_rate": 1.7386363636363638e-05,
      "loss": 1.1245,
      "step": 25990
    },
    {
      "epoch": 1.3073209975864843,
      "grad_norm": 16.5,
      "learning_rate": 1.7385358004827034e-05,
      "loss": 1.0353,
      "step": 26000
    },
    {
      "epoch": 1.3073209975864843,
      "eval_accuracy": 0.5121715418939018,
      "eval_loss": 1.0261173248291016,
      "eval_runtime": 464.9379,
      "eval_samples_per_second": 86.764,
      "eval_steps_per_second": 86.764,
      "step": 26000
    },
    {
      "epoch": 1.3078238133547868,
      "grad_norm": 31.875,
      "learning_rate": 1.7384352373290426e-05,
      "loss": 0.9749,
      "step": 26010
    },
    {
      "epoch": 1.3083266291230893,
      "grad_norm": 87.0,
      "learning_rate": 1.7383346741753822e-05,
      "loss": 1.0717,
      "step": 26020
    },
    {
      "epoch": 1.3088294448913917,
      "grad_norm": 37.75,
      "learning_rate": 1.7382341110217218e-05,
      "loss": 1.0442,
      "step": 26030
    },
    {
      "epoch": 1.3093322606596942,
      "grad_norm": 15.1875,
      "learning_rate": 1.738133547868061e-05,
      "loss": 0.9141,
      "step": 26040
    },
    {
      "epoch": 1.309835076427997,
      "grad_norm": 9.625,
      "learning_rate": 1.738032984714401e-05,
      "loss": 1.0812,
      "step": 26050
    },
    {
      "epoch": 1.3103378921962991,
      "grad_norm": 17.875,
      "learning_rate": 1.7379324215607402e-05,
      "loss": 0.9091,
      "step": 26060
    },
    {
      "epoch": 1.3108407079646018,
      "grad_norm": 25.125,
      "learning_rate": 1.7378318584070798e-05,
      "loss": 1.1503,
      "step": 26070
    },
    {
      "epoch": 1.3113435237329043,
      "grad_norm": 23.625,
      "learning_rate": 1.7377312952534194e-05,
      "loss": 1.0293,
      "step": 26080
    },
    {
      "epoch": 1.3118463395012068,
      "grad_norm": 19.75,
      "learning_rate": 1.7376307320997587e-05,
      "loss": 0.6935,
      "step": 26090
    },
    {
      "epoch": 1.3123491552695092,
      "grad_norm": 7.46875,
      "learning_rate": 1.7375301689460982e-05,
      "loss": 1.0843,
      "step": 26100
    },
    {
      "epoch": 1.3128519710378117,
      "grad_norm": 12.375,
      "learning_rate": 1.7374296057924378e-05,
      "loss": 1.2859,
      "step": 26110
    },
    {
      "epoch": 1.3133547868061142,
      "grad_norm": 48.75,
      "learning_rate": 1.737329042638777e-05,
      "loss": 0.7789,
      "step": 26120
    },
    {
      "epoch": 1.3138576025744166,
      "grad_norm": 46.25,
      "learning_rate": 1.737228479485117e-05,
      "loss": 1.255,
      "step": 26130
    },
    {
      "epoch": 1.3143604183427193,
      "grad_norm": 12.4375,
      "learning_rate": 1.7371279163314563e-05,
      "loss": 0.8583,
      "step": 26140
    },
    {
      "epoch": 1.3148632341110218,
      "grad_norm": 27.25,
      "learning_rate": 1.737027353177796e-05,
      "loss": 0.8739,
      "step": 26150
    },
    {
      "epoch": 1.3153660498793243,
      "grad_norm": 30.0,
      "learning_rate": 1.7369267900241354e-05,
      "loss": 0.7232,
      "step": 26160
    },
    {
      "epoch": 1.3158688656476267,
      "grad_norm": 62.75,
      "learning_rate": 1.7368262268704747e-05,
      "loss": 1.0688,
      "step": 26170
    },
    {
      "epoch": 1.3163716814159292,
      "grad_norm": 40.25,
      "learning_rate": 1.7367256637168143e-05,
      "loss": 1.1625,
      "step": 26180
    },
    {
      "epoch": 1.3168744971842317,
      "grad_norm": 14.8125,
      "learning_rate": 1.736625100563154e-05,
      "loss": 0.8336,
      "step": 26190
    },
    {
      "epoch": 1.3173773129525341,
      "grad_norm": 32.0,
      "learning_rate": 1.736524537409493e-05,
      "loss": 1.0632,
      "step": 26200
    },
    {
      "epoch": 1.3178801287208368,
      "grad_norm": 25.75,
      "learning_rate": 1.736423974255833e-05,
      "loss": 0.9841,
      "step": 26210
    },
    {
      "epoch": 1.318382944489139,
      "grad_norm": 58.5,
      "learning_rate": 1.7363234111021723e-05,
      "loss": 1.0607,
      "step": 26220
    },
    {
      "epoch": 1.3188857602574418,
      "grad_norm": 45.25,
      "learning_rate": 1.736222847948512e-05,
      "loss": 1.0597,
      "step": 26230
    },
    {
      "epoch": 1.3193885760257442,
      "grad_norm": 19.625,
      "learning_rate": 1.7361222847948515e-05,
      "loss": 0.9358,
      "step": 26240
    },
    {
      "epoch": 1.3198913917940467,
      "grad_norm": 11.8125,
      "learning_rate": 1.7360217216411907e-05,
      "loss": 1.0523,
      "step": 26250
    },
    {
      "epoch": 1.3203942075623492,
      "grad_norm": 43.25,
      "learning_rate": 1.7359211584875303e-05,
      "loss": 1.031,
      "step": 26260
    },
    {
      "epoch": 1.3208970233306516,
      "grad_norm": 11.625,
      "learning_rate": 1.73582059533387e-05,
      "loss": 0.9887,
      "step": 26270
    },
    {
      "epoch": 1.321399839098954,
      "grad_norm": 19.0,
      "learning_rate": 1.735720032180209e-05,
      "loss": 0.9251,
      "step": 26280
    },
    {
      "epoch": 1.3219026548672566,
      "grad_norm": 12.0625,
      "learning_rate": 1.7356194690265487e-05,
      "loss": 0.708,
      "step": 26290
    },
    {
      "epoch": 1.3224054706355592,
      "grad_norm": 37.5,
      "learning_rate": 1.7355189058728883e-05,
      "loss": 0.6259,
      "step": 26300
    },
    {
      "epoch": 1.3229082864038615,
      "grad_norm": 59.75,
      "learning_rate": 1.7354183427192276e-05,
      "loss": 1.3041,
      "step": 26310
    },
    {
      "epoch": 1.3234111021721642,
      "grad_norm": 40.75,
      "learning_rate": 1.7353177795655675e-05,
      "loss": 1.1883,
      "step": 26320
    },
    {
      "epoch": 1.3239139179404666,
      "grad_norm": 13.25,
      "learning_rate": 1.7352172164119067e-05,
      "loss": 1.2893,
      "step": 26330
    },
    {
      "epoch": 1.3244167337087691,
      "grad_norm": 55.0,
      "learning_rate": 1.7351166532582463e-05,
      "loss": 1.0706,
      "step": 26340
    },
    {
      "epoch": 1.3249195494770716,
      "grad_norm": 24.0,
      "learning_rate": 1.735016090104586e-05,
      "loss": 1.0427,
      "step": 26350
    },
    {
      "epoch": 1.325422365245374,
      "grad_norm": 43.25,
      "learning_rate": 1.734915526950925e-05,
      "loss": 1.06,
      "step": 26360
    },
    {
      "epoch": 1.3259251810136765,
      "grad_norm": 8.25,
      "learning_rate": 1.7348149637972647e-05,
      "loss": 0.6312,
      "step": 26370
    },
    {
      "epoch": 1.326427996781979,
      "grad_norm": 45.75,
      "learning_rate": 1.7347144006436043e-05,
      "loss": 1.0947,
      "step": 26380
    },
    {
      "epoch": 1.3269308125502817,
      "grad_norm": 6.53125,
      "learning_rate": 1.7346138374899436e-05,
      "loss": 1.2304,
      "step": 26390
    },
    {
      "epoch": 1.3274336283185841,
      "grad_norm": 35.25,
      "learning_rate": 1.7345132743362835e-05,
      "loss": 1.0967,
      "step": 26400
    },
    {
      "epoch": 1.3279364440868866,
      "grad_norm": 40.5,
      "learning_rate": 1.7344127111826228e-05,
      "loss": 1.1498,
      "step": 26410
    },
    {
      "epoch": 1.328439259855189,
      "grad_norm": 27.75,
      "learning_rate": 1.7343121480289623e-05,
      "loss": 0.7106,
      "step": 26420
    },
    {
      "epoch": 1.3289420756234915,
      "grad_norm": 14.4375,
      "learning_rate": 1.734211584875302e-05,
      "loss": 1.1905,
      "step": 26430
    },
    {
      "epoch": 1.329444891391794,
      "grad_norm": 11.375,
      "learning_rate": 1.7341110217216412e-05,
      "loss": 0.9035,
      "step": 26440
    },
    {
      "epoch": 1.3299477071600965,
      "grad_norm": 27.125,
      "learning_rate": 1.7340104585679808e-05,
      "loss": 1.0663,
      "step": 26450
    },
    {
      "epoch": 1.3304505229283992,
      "grad_norm": 8.1875,
      "learning_rate": 1.7339098954143204e-05,
      "loss": 0.8243,
      "step": 26460
    },
    {
      "epoch": 1.3309533386967014,
      "grad_norm": 9.5625,
      "learning_rate": 1.7338093322606596e-05,
      "loss": 1.0115,
      "step": 26470
    },
    {
      "epoch": 1.331456154465004,
      "grad_norm": 13.625,
      "learning_rate": 1.7337087691069995e-05,
      "loss": 0.9974,
      "step": 26480
    },
    {
      "epoch": 1.3319589702333066,
      "grad_norm": 7.71875,
      "learning_rate": 1.7336082059533388e-05,
      "loss": 1.0828,
      "step": 26490
    },
    {
      "epoch": 1.332461786001609,
      "grad_norm": 15.875,
      "learning_rate": 1.7335076427996784e-05,
      "loss": 0.9431,
      "step": 26500
    },
    {
      "epoch": 1.332461786001609,
      "eval_accuracy": 0.5122211204759544,
      "eval_loss": 1.026441216468811,
      "eval_runtime": 464.8798,
      "eval_samples_per_second": 86.775,
      "eval_steps_per_second": 86.775,
      "step": 26500
    },
    {
      "epoch": 1.3329646017699115,
      "grad_norm": 18.0,
      "learning_rate": 1.733407079646018e-05,
      "loss": 1.0313,
      "step": 26510
    },
    {
      "epoch": 1.333467417538214,
      "grad_norm": 19.125,
      "learning_rate": 1.7333065164923572e-05,
      "loss": 0.8695,
      "step": 26520
    },
    {
      "epoch": 1.3339702333065164,
      "grad_norm": 30.625,
      "learning_rate": 1.7332059533386968e-05,
      "loss": 1.108,
      "step": 26530
    },
    {
      "epoch": 1.334473049074819,
      "grad_norm": 15.25,
      "learning_rate": 1.7331053901850364e-05,
      "loss": 0.8201,
      "step": 26540
    },
    {
      "epoch": 1.3349758648431216,
      "grad_norm": 5.34375,
      "learning_rate": 1.7330048270313756e-05,
      "loss": 0.9009,
      "step": 26550
    },
    {
      "epoch": 1.335478680611424,
      "grad_norm": 10.4375,
      "learning_rate": 1.7329042638777152e-05,
      "loss": 1.1161,
      "step": 26560
    },
    {
      "epoch": 1.3359814963797265,
      "grad_norm": 17.625,
      "learning_rate": 1.7328037007240548e-05,
      "loss": 1.1883,
      "step": 26570
    },
    {
      "epoch": 1.336484312148029,
      "grad_norm": 61.25,
      "learning_rate": 1.7327031375703944e-05,
      "loss": 1.2577,
      "step": 26580
    },
    {
      "epoch": 1.3369871279163315,
      "grad_norm": 10.1875,
      "learning_rate": 1.732602574416734e-05,
      "loss": 0.8935,
      "step": 26590
    },
    {
      "epoch": 1.337489943684634,
      "grad_norm": 64.5,
      "learning_rate": 1.7325020112630732e-05,
      "loss": 0.914,
      "step": 26600
    },
    {
      "epoch": 1.3379927594529364,
      "grad_norm": 12.9375,
      "learning_rate": 1.7324014481094128e-05,
      "loss": 1.1056,
      "step": 26610
    },
    {
      "epoch": 1.338495575221239,
      "grad_norm": 10.375,
      "learning_rate": 1.7323008849557524e-05,
      "loss": 1.0829,
      "step": 26620
    },
    {
      "epoch": 1.3389983909895413,
      "grad_norm": 8.25,
      "learning_rate": 1.7322003218020917e-05,
      "loss": 0.9714,
      "step": 26630
    },
    {
      "epoch": 1.339501206757844,
      "grad_norm": 76.0,
      "learning_rate": 1.7320997586484312e-05,
      "loss": 1.1051,
      "step": 26640
    },
    {
      "epoch": 1.3400040225261465,
      "grad_norm": 12.25,
      "learning_rate": 1.731999195494771e-05,
      "loss": 0.9843,
      "step": 26650
    },
    {
      "epoch": 1.340506838294449,
      "grad_norm": 55.75,
      "learning_rate": 1.7318986323411104e-05,
      "loss": 1.1215,
      "step": 26660
    },
    {
      "epoch": 1.3410096540627514,
      "grad_norm": 25.0,
      "learning_rate": 1.73179806918745e-05,
      "loss": 0.8088,
      "step": 26670
    },
    {
      "epoch": 1.3415124698310539,
      "grad_norm": 7.0,
      "learning_rate": 1.7316975060337893e-05,
      "loss": 0.9843,
      "step": 26680
    },
    {
      "epoch": 1.3420152855993563,
      "grad_norm": 34.25,
      "learning_rate": 1.731596942880129e-05,
      "loss": 0.8509,
      "step": 26690
    },
    {
      "epoch": 1.3425181013676588,
      "grad_norm": 32.5,
      "learning_rate": 1.7314963797264684e-05,
      "loss": 0.9856,
      "step": 26700
    },
    {
      "epoch": 1.3430209171359615,
      "grad_norm": 12.875,
      "learning_rate": 1.731395816572808e-05,
      "loss": 0.9117,
      "step": 26710
    },
    {
      "epoch": 1.3435237329042637,
      "grad_norm": 34.75,
      "learning_rate": 1.7312952534191473e-05,
      "loss": 1.2488,
      "step": 26720
    },
    {
      "epoch": 1.3440265486725664,
      "grad_norm": 11.625,
      "learning_rate": 1.731194690265487e-05,
      "loss": 1.2239,
      "step": 26730
    },
    {
      "epoch": 1.344529364440869,
      "grad_norm": 46.5,
      "learning_rate": 1.7310941271118264e-05,
      "loss": 1.0861,
      "step": 26740
    },
    {
      "epoch": 1.3450321802091714,
      "grad_norm": 9.6875,
      "learning_rate": 1.730993563958166e-05,
      "loss": 1.1312,
      "step": 26750
    },
    {
      "epoch": 1.3455349959774738,
      "grad_norm": 19.125,
      "learning_rate": 1.7308930008045053e-05,
      "loss": 0.9693,
      "step": 26760
    },
    {
      "epoch": 1.3460378117457763,
      "grad_norm": 11.9375,
      "learning_rate": 1.730792437650845e-05,
      "loss": 0.8214,
      "step": 26770
    },
    {
      "epoch": 1.3465406275140788,
      "grad_norm": 58.0,
      "learning_rate": 1.7306918744971845e-05,
      "loss": 1.1774,
      "step": 26780
    },
    {
      "epoch": 1.3470434432823812,
      "grad_norm": 34.75,
      "learning_rate": 1.730591311343524e-05,
      "loss": 1.1174,
      "step": 26790
    },
    {
      "epoch": 1.347546259050684,
      "grad_norm": 22.5,
      "learning_rate": 1.7304907481898633e-05,
      "loss": 0.9337,
      "step": 26800
    },
    {
      "epoch": 1.3480490748189864,
      "grad_norm": 13.6875,
      "learning_rate": 1.730390185036203e-05,
      "loss": 0.7819,
      "step": 26810
    },
    {
      "epoch": 1.3485518905872889,
      "grad_norm": 8.9375,
      "learning_rate": 1.7302896218825425e-05,
      "loss": 0.7,
      "step": 26820
    },
    {
      "epoch": 1.3490547063555913,
      "grad_norm": 17.375,
      "learning_rate": 1.7301890587288817e-05,
      "loss": 0.8567,
      "step": 26830
    },
    {
      "epoch": 1.3495575221238938,
      "grad_norm": 21.625,
      "learning_rate": 1.7300884955752213e-05,
      "loss": 0.9918,
      "step": 26840
    },
    {
      "epoch": 1.3500603378921963,
      "grad_norm": 19.75,
      "learning_rate": 1.729987932421561e-05,
      "loss": 0.8067,
      "step": 26850
    },
    {
      "epoch": 1.3505631536604987,
      "grad_norm": 13.1875,
      "learning_rate": 1.7298873692679005e-05,
      "loss": 0.7858,
      "step": 26860
    },
    {
      "epoch": 1.3510659694288014,
      "grad_norm": 20.25,
      "learning_rate": 1.72978680611424e-05,
      "loss": 0.7993,
      "step": 26870
    },
    {
      "epoch": 1.3515687851971037,
      "grad_norm": 43.0,
      "learning_rate": 1.7296862429605793e-05,
      "loss": 1.1135,
      "step": 26880
    },
    {
      "epoch": 1.3520716009654064,
      "grad_norm": 41.5,
      "learning_rate": 1.729585679806919e-05,
      "loss": 1.2294,
      "step": 26890
    },
    {
      "epoch": 1.3525744167337088,
      "grad_norm": 70.0,
      "learning_rate": 1.7294851166532585e-05,
      "loss": 1.6041,
      "step": 26900
    },
    {
      "epoch": 1.3530772325020113,
      "grad_norm": 23.0,
      "learning_rate": 1.7293845534995977e-05,
      "loss": 1.1968,
      "step": 26910
    },
    {
      "epoch": 1.3535800482703138,
      "grad_norm": 12.6875,
      "learning_rate": 1.7292839903459373e-05,
      "loss": 1.0039,
      "step": 26920
    },
    {
      "epoch": 1.3540828640386162,
      "grad_norm": 10.3125,
      "learning_rate": 1.729183427192277e-05,
      "loss": 0.6938,
      "step": 26930
    },
    {
      "epoch": 1.3545856798069187,
      "grad_norm": 28.125,
      "learning_rate": 1.7290828640386165e-05,
      "loss": 0.8602,
      "step": 26940
    },
    {
      "epoch": 1.3550884955752212,
      "grad_norm": 27.25,
      "learning_rate": 1.728982300884956e-05,
      "loss": 0.9944,
      "step": 26950
    },
    {
      "epoch": 1.3555913113435238,
      "grad_norm": 22.125,
      "learning_rate": 1.7288817377312953e-05,
      "loss": 0.7081,
      "step": 26960
    },
    {
      "epoch": 1.3560941271118263,
      "grad_norm": 49.25,
      "learning_rate": 1.728781174577635e-05,
      "loss": 1.337,
      "step": 26970
    },
    {
      "epoch": 1.3565969428801288,
      "grad_norm": 69.0,
      "learning_rate": 1.7286806114239745e-05,
      "loss": 1.1541,
      "step": 26980
    },
    {
      "epoch": 1.3570997586484312,
      "grad_norm": 53.0,
      "learning_rate": 1.7285800482703138e-05,
      "loss": 1.0164,
      "step": 26990
    },
    {
      "epoch": 1.3576025744167337,
      "grad_norm": 13.375,
      "learning_rate": 1.7284794851166534e-05,
      "loss": 0.9455,
      "step": 27000
    },
    {
      "epoch": 1.3576025744167337,
      "eval_accuracy": 0.5118988596926128,
      "eval_loss": 1.026297688484192,
      "eval_runtime": 465.8721,
      "eval_samples_per_second": 86.59,
      "eval_steps_per_second": 86.59,
      "step": 27000
    },
    {
      "epoch": 1.3581053901850362,
      "grad_norm": 3.59375,
      "learning_rate": 1.728378921962993e-05,
      "loss": 0.7485,
      "step": 27010
    },
    {
      "epoch": 1.3586082059533386,
      "grad_norm": 6.78125,
      "learning_rate": 1.7282783588093325e-05,
      "loss": 0.6392,
      "step": 27020
    },
    {
      "epoch": 1.359111021721641,
      "grad_norm": 65.0,
      "learning_rate": 1.728177795655672e-05,
      "loss": 1.0179,
      "step": 27030
    },
    {
      "epoch": 1.3596138374899436,
      "grad_norm": 8.8125,
      "learning_rate": 1.7280772325020114e-05,
      "loss": 0.9539,
      "step": 27040
    },
    {
      "epoch": 1.3601166532582463,
      "grad_norm": 4.5,
      "learning_rate": 1.727976669348351e-05,
      "loss": 0.8082,
      "step": 27050
    },
    {
      "epoch": 1.3606194690265487,
      "grad_norm": 61.75,
      "learning_rate": 1.7278761061946905e-05,
      "loss": 1.0629,
      "step": 27060
    },
    {
      "epoch": 1.3611222847948512,
      "grad_norm": 28.125,
      "learning_rate": 1.7277755430410298e-05,
      "loss": 1.2649,
      "step": 27070
    },
    {
      "epoch": 1.3616251005631537,
      "grad_norm": 20.875,
      "learning_rate": 1.7276749798873694e-05,
      "loss": 0.8027,
      "step": 27080
    },
    {
      "epoch": 1.3621279163314561,
      "grad_norm": 12.4375,
      "learning_rate": 1.727574416733709e-05,
      "loss": 0.9583,
      "step": 27090
    },
    {
      "epoch": 1.3626307320997586,
      "grad_norm": 40.75,
      "learning_rate": 1.7274738535800482e-05,
      "loss": 0.8504,
      "step": 27100
    },
    {
      "epoch": 1.363133547868061,
      "grad_norm": 27.25,
      "learning_rate": 1.727373290426388e-05,
      "loss": 1.0699,
      "step": 27110
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 37.5,
      "learning_rate": 1.7272727272727274e-05,
      "loss": 0.9118,
      "step": 27120
    },
    {
      "epoch": 1.364139179404666,
      "grad_norm": 35.25,
      "learning_rate": 1.727172164119067e-05,
      "loss": 1.085,
      "step": 27130
    },
    {
      "epoch": 1.3646419951729687,
      "grad_norm": 11.25,
      "learning_rate": 1.7270716009654066e-05,
      "loss": 1.0849,
      "step": 27140
    },
    {
      "epoch": 1.3651448109412712,
      "grad_norm": 15.75,
      "learning_rate": 1.7269710378117458e-05,
      "loss": 1.2348,
      "step": 27150
    },
    {
      "epoch": 1.3656476267095736,
      "grad_norm": 17.0,
      "learning_rate": 1.7268704746580854e-05,
      "loss": 0.942,
      "step": 27160
    },
    {
      "epoch": 1.366150442477876,
      "grad_norm": 25.5,
      "learning_rate": 1.726769911504425e-05,
      "loss": 1.1626,
      "step": 27170
    },
    {
      "epoch": 1.3666532582461786,
      "grad_norm": 40.5,
      "learning_rate": 1.7266693483507642e-05,
      "loss": 0.8459,
      "step": 27180
    },
    {
      "epoch": 1.367156074014481,
      "grad_norm": 32.25,
      "learning_rate": 1.7265687851971042e-05,
      "loss": 0.9472,
      "step": 27190
    },
    {
      "epoch": 1.3676588897827835,
      "grad_norm": 34.25,
      "learning_rate": 1.7264682220434434e-05,
      "loss": 1.1168,
      "step": 27200
    },
    {
      "epoch": 1.3681617055510862,
      "grad_norm": 6.9375,
      "learning_rate": 1.726367658889783e-05,
      "loss": 0.8776,
      "step": 27210
    },
    {
      "epoch": 1.3686645213193886,
      "grad_norm": 5.375,
      "learning_rate": 1.7262670957361226e-05,
      "loss": 1.0531,
      "step": 27220
    },
    {
      "epoch": 1.3691673370876911,
      "grad_norm": 11.0625,
      "learning_rate": 1.726166532582462e-05,
      "loss": 0.8757,
      "step": 27230
    },
    {
      "epoch": 1.3696701528559936,
      "grad_norm": 9.3125,
      "learning_rate": 1.7260659694288014e-05,
      "loss": 0.82,
      "step": 27240
    },
    {
      "epoch": 1.370172968624296,
      "grad_norm": 24.75,
      "learning_rate": 1.725965406275141e-05,
      "loss": 1.0047,
      "step": 27250
    },
    {
      "epoch": 1.3706757843925985,
      "grad_norm": 14.125,
      "learning_rate": 1.7258648431214803e-05,
      "loss": 1.1014,
      "step": 27260
    },
    {
      "epoch": 1.371178600160901,
      "grad_norm": 42.0,
      "learning_rate": 1.7257642799678202e-05,
      "loss": 1.504,
      "step": 27270
    },
    {
      "epoch": 1.3716814159292037,
      "grad_norm": 49.25,
      "learning_rate": 1.7256637168141594e-05,
      "loss": 1.1386,
      "step": 27280
    },
    {
      "epoch": 1.372184231697506,
      "grad_norm": 22.75,
      "learning_rate": 1.725563153660499e-05,
      "loss": 1.0192,
      "step": 27290
    },
    {
      "epoch": 1.3726870474658086,
      "grad_norm": 49.0,
      "learning_rate": 1.7254625905068386e-05,
      "loss": 0.886,
      "step": 27300
    },
    {
      "epoch": 1.373189863234111,
      "grad_norm": 13.8125,
      "learning_rate": 1.725362027353178e-05,
      "loss": 1.0352,
      "step": 27310
    },
    {
      "epoch": 1.3736926790024135,
      "grad_norm": 13.8125,
      "learning_rate": 1.7252614641995175e-05,
      "loss": 0.872,
      "step": 27320
    },
    {
      "epoch": 1.374195494770716,
      "grad_norm": 9.5,
      "learning_rate": 1.725160901045857e-05,
      "loss": 0.5692,
      "step": 27330
    },
    {
      "epoch": 1.3746983105390185,
      "grad_norm": 82.0,
      "learning_rate": 1.7250603378921963e-05,
      "loss": 1.2731,
      "step": 27340
    },
    {
      "epoch": 1.375201126307321,
      "grad_norm": 32.0,
      "learning_rate": 1.724959774738536e-05,
      "loss": 1.0763,
      "step": 27350
    },
    {
      "epoch": 1.3757039420756234,
      "grad_norm": 46.0,
      "learning_rate": 1.7248592115848755e-05,
      "loss": 0.8553,
      "step": 27360
    },
    {
      "epoch": 1.376206757843926,
      "grad_norm": 28.75,
      "learning_rate": 1.7247586484312147e-05,
      "loss": 1.1472,
      "step": 27370
    },
    {
      "epoch": 1.3767095736122283,
      "grad_norm": 7.90625,
      "learning_rate": 1.7246580852775547e-05,
      "loss": 1.0359,
      "step": 27380
    },
    {
      "epoch": 1.377212389380531,
      "grad_norm": 44.75,
      "learning_rate": 1.724557522123894e-05,
      "loss": 1.1152,
      "step": 27390
    },
    {
      "epoch": 1.3777152051488335,
      "grad_norm": 38.5,
      "learning_rate": 1.7244569589702335e-05,
      "loss": 1.1884,
      "step": 27400
    },
    {
      "epoch": 1.378218020917136,
      "grad_norm": 68.0,
      "learning_rate": 1.724356395816573e-05,
      "loss": 1.2666,
      "step": 27410
    },
    {
      "epoch": 1.3787208366854384,
      "grad_norm": 9.5625,
      "learning_rate": 1.7242558326629123e-05,
      "loss": 0.8708,
      "step": 27420
    },
    {
      "epoch": 1.379223652453741,
      "grad_norm": 18.875,
      "learning_rate": 1.724155269509252e-05,
      "loss": 1.0602,
      "step": 27430
    },
    {
      "epoch": 1.3797264682220434,
      "grad_norm": 18.0,
      "learning_rate": 1.7240547063555915e-05,
      "loss": 0.9063,
      "step": 27440
    },
    {
      "epoch": 1.3802292839903458,
      "grad_norm": 10.3125,
      "learning_rate": 1.7239541432019307e-05,
      "loss": 0.9412,
      "step": 27450
    },
    {
      "epoch": 1.3807320997586485,
      "grad_norm": 26.5,
      "learning_rate": 1.7238535800482707e-05,
      "loss": 0.9341,
      "step": 27460
    },
    {
      "epoch": 1.381234915526951,
      "grad_norm": 7.375,
      "learning_rate": 1.72375301689461e-05,
      "loss": 1.0445,
      "step": 27470
    },
    {
      "epoch": 1.3817377312952535,
      "grad_norm": 15.375,
      "learning_rate": 1.7236524537409495e-05,
      "loss": 0.8217,
      "step": 27480
    },
    {
      "epoch": 1.382240547063556,
      "grad_norm": 24.375,
      "learning_rate": 1.723551890587289e-05,
      "loss": 1.0283,
      "step": 27490
    },
    {
      "epoch": 1.3827433628318584,
      "grad_norm": 43.5,
      "learning_rate": 1.7234513274336284e-05,
      "loss": 1.0196,
      "step": 27500
    },
    {
      "epoch": 1.3827433628318584,
      "eval_accuracy": 0.5117997025285077,
      "eval_loss": 1.0258923768997192,
      "eval_runtime": 466.2297,
      "eval_samples_per_second": 86.524,
      "eval_steps_per_second": 86.524,
      "step": 27500
    },
    {
      "epoch": 1.3832461786001609,
      "grad_norm": 18.75,
      "learning_rate": 1.723350764279968e-05,
      "loss": 0.9907,
      "step": 27510
    },
    {
      "epoch": 1.3837489943684633,
      "grad_norm": 12.0625,
      "learning_rate": 1.7232502011263075e-05,
      "loss": 1.1973,
      "step": 27520
    },
    {
      "epoch": 1.384251810136766,
      "grad_norm": 29.625,
      "learning_rate": 1.7231496379726468e-05,
      "loss": 1.0821,
      "step": 27530
    },
    {
      "epoch": 1.3847546259050683,
      "grad_norm": 43.5,
      "learning_rate": 1.7230490748189867e-05,
      "loss": 1.4772,
      "step": 27540
    },
    {
      "epoch": 1.385257441673371,
      "grad_norm": 5.25,
      "learning_rate": 1.722948511665326e-05,
      "loss": 1.092,
      "step": 27550
    },
    {
      "epoch": 1.3857602574416734,
      "grad_norm": 29.5,
      "learning_rate": 1.7228479485116652e-05,
      "loss": 1.2008,
      "step": 27560
    },
    {
      "epoch": 1.3862630732099759,
      "grad_norm": 13.75,
      "learning_rate": 1.722747385358005e-05,
      "loss": 1.0287,
      "step": 27570
    },
    {
      "epoch": 1.3867658889782783,
      "grad_norm": 18.125,
      "learning_rate": 1.7226468222043444e-05,
      "loss": 0.9344,
      "step": 27580
    },
    {
      "epoch": 1.3872687047465808,
      "grad_norm": 33.0,
      "learning_rate": 1.722546259050684e-05,
      "loss": 1.0168,
      "step": 27590
    },
    {
      "epoch": 1.3877715205148833,
      "grad_norm": 12.9375,
      "learning_rate": 1.7224456958970236e-05,
      "loss": 0.9115,
      "step": 27600
    },
    {
      "epoch": 1.3882743362831858,
      "grad_norm": 6.34375,
      "learning_rate": 1.7223451327433628e-05,
      "loss": 1.0939,
      "step": 27610
    },
    {
      "epoch": 1.3887771520514884,
      "grad_norm": 16.375,
      "learning_rate": 1.7222445695897024e-05,
      "loss": 0.926,
      "step": 27620
    },
    {
      "epoch": 1.389279967819791,
      "grad_norm": 37.5,
      "learning_rate": 1.722144006436042e-05,
      "loss": 0.9744,
      "step": 27630
    },
    {
      "epoch": 1.3897827835880934,
      "grad_norm": 54.75,
      "learning_rate": 1.7220434432823812e-05,
      "loss": 1.12,
      "step": 27640
    },
    {
      "epoch": 1.3902855993563958,
      "grad_norm": 35.25,
      "learning_rate": 1.721942880128721e-05,
      "loss": 0.9905,
      "step": 27650
    },
    {
      "epoch": 1.3907884151246983,
      "grad_norm": 30.375,
      "learning_rate": 1.7218423169750604e-05,
      "loss": 1.0194,
      "step": 27660
    },
    {
      "epoch": 1.3912912308930008,
      "grad_norm": 26.25,
      "learning_rate": 1.7217417538214e-05,
      "loss": 1.1254,
      "step": 27670
    },
    {
      "epoch": 1.3917940466613032,
      "grad_norm": 31.125,
      "learning_rate": 1.7216411906677396e-05,
      "loss": 1.2793,
      "step": 27680
    },
    {
      "epoch": 1.392296862429606,
      "grad_norm": 26.0,
      "learning_rate": 1.7215406275140788e-05,
      "loss": 0.9319,
      "step": 27690
    },
    {
      "epoch": 1.3927996781979082,
      "grad_norm": 7.03125,
      "learning_rate": 1.7214400643604184e-05,
      "loss": 0.7086,
      "step": 27700
    },
    {
      "epoch": 1.3933024939662109,
      "grad_norm": 82.5,
      "learning_rate": 1.721339501206758e-05,
      "loss": 1.0465,
      "step": 27710
    },
    {
      "epoch": 1.3938053097345133,
      "grad_norm": 13.6875,
      "learning_rate": 1.7212389380530973e-05,
      "loss": 1.0555,
      "step": 27720
    },
    {
      "epoch": 1.3943081255028158,
      "grad_norm": 10.125,
      "learning_rate": 1.7211383748994372e-05,
      "loss": 1.0968,
      "step": 27730
    },
    {
      "epoch": 1.3948109412711183,
      "grad_norm": 39.0,
      "learning_rate": 1.7210378117457764e-05,
      "loss": 0.9344,
      "step": 27740
    },
    {
      "epoch": 1.3953137570394207,
      "grad_norm": 47.75,
      "learning_rate": 1.720937248592116e-05,
      "loss": 0.9877,
      "step": 27750
    },
    {
      "epoch": 1.3958165728077232,
      "grad_norm": 38.0,
      "learning_rate": 1.7208366854384556e-05,
      "loss": 1.18,
      "step": 27760
    },
    {
      "epoch": 1.3963193885760257,
      "grad_norm": 9.25,
      "learning_rate": 1.720736122284795e-05,
      "loss": 0.685,
      "step": 27770
    },
    {
      "epoch": 1.3968222043443284,
      "grad_norm": 19.0,
      "learning_rate": 1.7206355591311344e-05,
      "loss": 0.9412,
      "step": 27780
    },
    {
      "epoch": 1.3973250201126306,
      "grad_norm": 38.5,
      "learning_rate": 1.720534995977474e-05,
      "loss": 1.314,
      "step": 27790
    },
    {
      "epoch": 1.3978278358809333,
      "grad_norm": 16.625,
      "learning_rate": 1.7204344328238133e-05,
      "loss": 0.8605,
      "step": 27800
    },
    {
      "epoch": 1.3983306516492358,
      "grad_norm": 29.625,
      "learning_rate": 1.720333869670153e-05,
      "loss": 1.1277,
      "step": 27810
    },
    {
      "epoch": 1.3988334674175382,
      "grad_norm": 37.75,
      "learning_rate": 1.7202333065164925e-05,
      "loss": 0.9188,
      "step": 27820
    },
    {
      "epoch": 1.3993362831858407,
      "grad_norm": 6.96875,
      "learning_rate": 1.720132743362832e-05,
      "loss": 1.0226,
      "step": 27830
    },
    {
      "epoch": 1.3998390989541432,
      "grad_norm": 32.0,
      "learning_rate": 1.7200321802091716e-05,
      "loss": 0.9113,
      "step": 27840
    },
    {
      "epoch": 1.4003419147224456,
      "grad_norm": 43.0,
      "learning_rate": 1.719931617055511e-05,
      "loss": 1.1936,
      "step": 27850
    },
    {
      "epoch": 1.400844730490748,
      "grad_norm": 34.0,
      "learning_rate": 1.7198310539018505e-05,
      "loss": 0.8738,
      "step": 27860
    },
    {
      "epoch": 1.4013475462590508,
      "grad_norm": 30.875,
      "learning_rate": 1.71973049074819e-05,
      "loss": 1.1872,
      "step": 27870
    },
    {
      "epoch": 1.4018503620273532,
      "grad_norm": 34.25,
      "learning_rate": 1.7196299275945293e-05,
      "loss": 1.0113,
      "step": 27880
    },
    {
      "epoch": 1.4023531777956557,
      "grad_norm": 29.75,
      "learning_rate": 1.719529364440869e-05,
      "loss": 1.0791,
      "step": 27890
    },
    {
      "epoch": 1.4028559935639582,
      "grad_norm": 10.6875,
      "learning_rate": 1.7194288012872085e-05,
      "loss": 0.9193,
      "step": 27900
    },
    {
      "epoch": 1.4033588093322606,
      "grad_norm": 34.75,
      "learning_rate": 1.719328238133548e-05,
      "loss": 1.1858,
      "step": 27910
    },
    {
      "epoch": 1.4038616251005631,
      "grad_norm": 19.375,
      "learning_rate": 1.7192276749798877e-05,
      "loss": 1.1969,
      "step": 27920
    },
    {
      "epoch": 1.4043644408688656,
      "grad_norm": 22.375,
      "learning_rate": 1.719127111826227e-05,
      "loss": 1.1321,
      "step": 27930
    },
    {
      "epoch": 1.4048672566371683,
      "grad_norm": 4.25,
      "learning_rate": 1.7190265486725665e-05,
      "loss": 0.8014,
      "step": 27940
    },
    {
      "epoch": 1.4053700724054705,
      "grad_norm": 22.5,
      "learning_rate": 1.718925985518906e-05,
      "loss": 1.2167,
      "step": 27950
    },
    {
      "epoch": 1.4058728881737732,
      "grad_norm": 33.75,
      "learning_rate": 1.7188254223652453e-05,
      "loss": 1.0026,
      "step": 27960
    },
    {
      "epoch": 1.4063757039420757,
      "grad_norm": 8.1875,
      "learning_rate": 1.718724859211585e-05,
      "loss": 0.9284,
      "step": 27970
    },
    {
      "epoch": 1.4068785197103781,
      "grad_norm": 35.25,
      "learning_rate": 1.7186242960579245e-05,
      "loss": 1.1172,
      "step": 27980
    },
    {
      "epoch": 1.4073813354786806,
      "grad_norm": 42.75,
      "learning_rate": 1.718523732904264e-05,
      "loss": 0.9987,
      "step": 27990
    },
    {
      "epoch": 1.407884151246983,
      "grad_norm": 11.875,
      "learning_rate": 1.7184231697506037e-05,
      "loss": 0.8251,
      "step": 28000
    },
    {
      "epoch": 1.407884151246983,
      "eval_accuracy": 0.5123202776400595,
      "eval_loss": 1.025997281074524,
      "eval_runtime": 466.3757,
      "eval_samples_per_second": 86.497,
      "eval_steps_per_second": 86.497,
      "step": 28000
    },
    {
      "epoch": 1.4083869670152855,
      "grad_norm": 32.5,
      "learning_rate": 1.718322606596943e-05,
      "loss": 0.9243,
      "step": 28010
    },
    {
      "epoch": 1.408889782783588,
      "grad_norm": 33.25,
      "learning_rate": 1.7182220434432825e-05,
      "loss": 1.3075,
      "step": 28020
    },
    {
      "epoch": 1.4093925985518907,
      "grad_norm": 55.0,
      "learning_rate": 1.718121480289622e-05,
      "loss": 1.0607,
      "step": 28030
    },
    {
      "epoch": 1.4098954143201932,
      "grad_norm": 8.6875,
      "learning_rate": 1.7180209171359614e-05,
      "loss": 1.1302,
      "step": 28040
    },
    {
      "epoch": 1.4103982300884956,
      "grad_norm": 39.0,
      "learning_rate": 1.717920353982301e-05,
      "loss": 1.1553,
      "step": 28050
    },
    {
      "epoch": 1.410901045856798,
      "grad_norm": 35.25,
      "learning_rate": 1.7178197908286405e-05,
      "loss": 1.1181,
      "step": 28060
    },
    {
      "epoch": 1.4114038616251006,
      "grad_norm": 34.75,
      "learning_rate": 1.71771922767498e-05,
      "loss": 0.9917,
      "step": 28070
    },
    {
      "epoch": 1.411906677393403,
      "grad_norm": 8.625,
      "learning_rate": 1.7176186645213194e-05,
      "loss": 0.8746,
      "step": 28080
    },
    {
      "epoch": 1.4124094931617055,
      "grad_norm": 59.75,
      "learning_rate": 1.717518101367659e-05,
      "loss": 1.3225,
      "step": 28090
    },
    {
      "epoch": 1.412912308930008,
      "grad_norm": 37.0,
      "learning_rate": 1.7174175382139985e-05,
      "loss": 1.0551,
      "step": 28100
    },
    {
      "epoch": 1.4134151246983104,
      "grad_norm": 50.5,
      "learning_rate": 1.717316975060338e-05,
      "loss": 0.9063,
      "step": 28110
    },
    {
      "epoch": 1.4139179404666131,
      "grad_norm": 27.25,
      "learning_rate": 1.7172164119066774e-05,
      "loss": 0.7597,
      "step": 28120
    },
    {
      "epoch": 1.4144207562349156,
      "grad_norm": 5.25,
      "learning_rate": 1.717115848753017e-05,
      "loss": 1.0874,
      "step": 28130
    },
    {
      "epoch": 1.414923572003218,
      "grad_norm": 10.8125,
      "learning_rate": 1.7170152855993566e-05,
      "loss": 1.0998,
      "step": 28140
    },
    {
      "epoch": 1.4154263877715205,
      "grad_norm": 17.875,
      "learning_rate": 1.716914722445696e-05,
      "loss": 1.1355,
      "step": 28150
    },
    {
      "epoch": 1.415929203539823,
      "grad_norm": 16.75,
      "learning_rate": 1.7168141592920354e-05,
      "loss": 1.184,
      "step": 28160
    },
    {
      "epoch": 1.4164320193081255,
      "grad_norm": 14.5625,
      "learning_rate": 1.716713596138375e-05,
      "loss": 0.9419,
      "step": 28170
    },
    {
      "epoch": 1.416934835076428,
      "grad_norm": 25.375,
      "learning_rate": 1.7166130329847146e-05,
      "loss": 1.0888,
      "step": 28180
    },
    {
      "epoch": 1.4174376508447306,
      "grad_norm": 17.0,
      "learning_rate": 1.716512469831054e-05,
      "loss": 1.0192,
      "step": 28190
    },
    {
      "epoch": 1.4179404666130329,
      "grad_norm": 20.75,
      "learning_rate": 1.7164119066773934e-05,
      "loss": 1.3486,
      "step": 28200
    },
    {
      "epoch": 1.4184432823813355,
      "grad_norm": 17.75,
      "learning_rate": 1.716311343523733e-05,
      "loss": 0.8512,
      "step": 28210
    },
    {
      "epoch": 1.418946098149638,
      "grad_norm": 10.9375,
      "learning_rate": 1.7162107803700726e-05,
      "loss": 0.9389,
      "step": 28220
    },
    {
      "epoch": 1.4194489139179405,
      "grad_norm": 27.25,
      "learning_rate": 1.716110217216412e-05,
      "loss": 0.9232,
      "step": 28230
    },
    {
      "epoch": 1.419951729686243,
      "grad_norm": 11.9375,
      "learning_rate": 1.7160096540627514e-05,
      "loss": 0.9162,
      "step": 28240
    },
    {
      "epoch": 1.4204545454545454,
      "grad_norm": 14.4375,
      "learning_rate": 1.715909090909091e-05,
      "loss": 1.0054,
      "step": 28250
    },
    {
      "epoch": 1.4209573612228479,
      "grad_norm": 25.25,
      "learning_rate": 1.7158085277554306e-05,
      "loss": 1.6413,
      "step": 28260
    },
    {
      "epoch": 1.4214601769911503,
      "grad_norm": 18.375,
      "learning_rate": 1.7157079646017702e-05,
      "loss": 1.0161,
      "step": 28270
    },
    {
      "epoch": 1.421962992759453,
      "grad_norm": 42.0,
      "learning_rate": 1.7156074014481094e-05,
      "loss": 1.1054,
      "step": 28280
    },
    {
      "epoch": 1.4224658085277555,
      "grad_norm": 20.625,
      "learning_rate": 1.715506838294449e-05,
      "loss": 0.8211,
      "step": 28290
    },
    {
      "epoch": 1.422968624296058,
      "grad_norm": 19.875,
      "learning_rate": 1.7154062751407886e-05,
      "loss": 0.8651,
      "step": 28300
    },
    {
      "epoch": 1.4234714400643604,
      "grad_norm": 29.75,
      "learning_rate": 1.7153057119871282e-05,
      "loss": 0.9148,
      "step": 28310
    },
    {
      "epoch": 1.423974255832663,
      "grad_norm": 9.875,
      "learning_rate": 1.7152051488334674e-05,
      "loss": 0.9183,
      "step": 28320
    },
    {
      "epoch": 1.4244770716009654,
      "grad_norm": 17.75,
      "learning_rate": 1.715104585679807e-05,
      "loss": 1.0802,
      "step": 28330
    },
    {
      "epoch": 1.4249798873692678,
      "grad_norm": 7.125,
      "learning_rate": 1.7150040225261466e-05,
      "loss": 0.9689,
      "step": 28340
    },
    {
      "epoch": 1.4254827031375705,
      "grad_norm": 10.25,
      "learning_rate": 1.714903459372486e-05,
      "loss": 1.243,
      "step": 28350
    },
    {
      "epoch": 1.4259855189058728,
      "grad_norm": 12.125,
      "learning_rate": 1.7148028962188255e-05,
      "loss": 0.9937,
      "step": 28360
    },
    {
      "epoch": 1.4264883346741755,
      "grad_norm": 32.5,
      "learning_rate": 1.714702333065165e-05,
      "loss": 1.0427,
      "step": 28370
    },
    {
      "epoch": 1.426991150442478,
      "grad_norm": 8.3125,
      "learning_rate": 1.7146017699115046e-05,
      "loss": 0.9547,
      "step": 28380
    },
    {
      "epoch": 1.4274939662107804,
      "grad_norm": 31.5,
      "learning_rate": 1.7145012067578442e-05,
      "loss": 0.8419,
      "step": 28390
    },
    {
      "epoch": 1.4279967819790829,
      "grad_norm": 20.625,
      "learning_rate": 1.7144006436041835e-05,
      "loss": 1.1215,
      "step": 28400
    },
    {
      "epoch": 1.4284995977473853,
      "grad_norm": 21.625,
      "learning_rate": 1.714300080450523e-05,
      "loss": 1.0699,
      "step": 28410
    },
    {
      "epoch": 1.4290024135156878,
      "grad_norm": 12.8125,
      "learning_rate": 1.7141995172968626e-05,
      "loss": 1.1227,
      "step": 28420
    },
    {
      "epoch": 1.4295052292839903,
      "grad_norm": 10.5625,
      "learning_rate": 1.714098954143202e-05,
      "loss": 0.88,
      "step": 28430
    },
    {
      "epoch": 1.430008045052293,
      "grad_norm": 47.75,
      "learning_rate": 1.7139983909895418e-05,
      "loss": 0.8922,
      "step": 28440
    },
    {
      "epoch": 1.4305108608205952,
      "grad_norm": 32.0,
      "learning_rate": 1.713897827835881e-05,
      "loss": 0.7652,
      "step": 28450
    },
    {
      "epoch": 1.4310136765888979,
      "grad_norm": 37.75,
      "learning_rate": 1.7137972646822207e-05,
      "loss": 1.0093,
      "step": 28460
    },
    {
      "epoch": 1.4315164923572004,
      "grad_norm": 9.5,
      "learning_rate": 1.7136967015285602e-05,
      "loss": 1.2445,
      "step": 28470
    },
    {
      "epoch": 1.4320193081255028,
      "grad_norm": 18.0,
      "learning_rate": 1.7135961383748995e-05,
      "loss": 1.0816,
      "step": 28480
    },
    {
      "epoch": 1.4325221238938053,
      "grad_norm": 21.75,
      "learning_rate": 1.713495575221239e-05,
      "loss": 0.9411,
      "step": 28490
    },
    {
      "epoch": 1.4330249396621078,
      "grad_norm": 24.125,
      "learning_rate": 1.7133950120675787e-05,
      "loss": 0.9905,
      "step": 28500
    },
    {
      "epoch": 1.4330249396621078,
      "eval_accuracy": 0.5125185919682697,
      "eval_loss": 1.0262833833694458,
      "eval_runtime": 464.4205,
      "eval_samples_per_second": 86.861,
      "eval_steps_per_second": 86.861,
      "step": 28500
    },
    {
      "epoch": 1.4335277554304102,
      "grad_norm": 23.75,
      "learning_rate": 1.713294448913918e-05,
      "loss": 0.9344,
      "step": 28510
    },
    {
      "epoch": 1.4340305711987127,
      "grad_norm": 7.6875,
      "learning_rate": 1.713193885760258e-05,
      "loss": 0.6545,
      "step": 28520
    },
    {
      "epoch": 1.4345333869670154,
      "grad_norm": 66.0,
      "learning_rate": 1.713093322606597e-05,
      "loss": 1.1092,
      "step": 28530
    },
    {
      "epoch": 1.4350362027353178,
      "grad_norm": 5.5,
      "learning_rate": 1.7129927594529367e-05,
      "loss": 0.9843,
      "step": 28540
    },
    {
      "epoch": 1.4355390185036203,
      "grad_norm": 3.40625,
      "learning_rate": 1.7128921962992763e-05,
      "loss": 0.7533,
      "step": 28550
    },
    {
      "epoch": 1.4360418342719228,
      "grad_norm": 3.984375,
      "learning_rate": 1.7127916331456155e-05,
      "loss": 0.6773,
      "step": 28560
    },
    {
      "epoch": 1.4365446500402252,
      "grad_norm": 11.5,
      "learning_rate": 1.712691069991955e-05,
      "loss": 1.0304,
      "step": 28570
    },
    {
      "epoch": 1.4370474658085277,
      "grad_norm": 14.1875,
      "learning_rate": 1.7125905068382947e-05,
      "loss": 1.0752,
      "step": 28580
    },
    {
      "epoch": 1.4375502815768302,
      "grad_norm": 27.625,
      "learning_rate": 1.712489943684634e-05,
      "loss": 0.812,
      "step": 28590
    },
    {
      "epoch": 1.4380530973451329,
      "grad_norm": 23.5,
      "learning_rate": 1.7123893805309735e-05,
      "loss": 1.0724,
      "step": 28600
    },
    {
      "epoch": 1.4385559131134351,
      "grad_norm": 32.0,
      "learning_rate": 1.712288817377313e-05,
      "loss": 0.8439,
      "step": 28610
    },
    {
      "epoch": 1.4390587288817378,
      "grad_norm": 23.625,
      "learning_rate": 1.7121882542236524e-05,
      "loss": 1.0016,
      "step": 28620
    },
    {
      "epoch": 1.4395615446500403,
      "grad_norm": 9.625,
      "learning_rate": 1.7120876910699923e-05,
      "loss": 0.9684,
      "step": 28630
    },
    {
      "epoch": 1.4400643604183427,
      "grad_norm": 8.75,
      "learning_rate": 1.7119871279163315e-05,
      "loss": 0.9828,
      "step": 28640
    },
    {
      "epoch": 1.4405671761866452,
      "grad_norm": 11.875,
      "learning_rate": 1.711886564762671e-05,
      "loss": 1.0972,
      "step": 28650
    },
    {
      "epoch": 1.4410699919549477,
      "grad_norm": 31.0,
      "learning_rate": 1.7117860016090107e-05,
      "loss": 1.0175,
      "step": 28660
    },
    {
      "epoch": 1.4415728077232501,
      "grad_norm": 41.0,
      "learning_rate": 1.71168543845535e-05,
      "loss": 1.0386,
      "step": 28670
    },
    {
      "epoch": 1.4420756234915526,
      "grad_norm": 7.8125,
      "learning_rate": 1.7115848753016896e-05,
      "loss": 1.2269,
      "step": 28680
    },
    {
      "epoch": 1.4425784392598553,
      "grad_norm": 51.0,
      "learning_rate": 1.711484312148029e-05,
      "loss": 1.2672,
      "step": 28690
    },
    {
      "epoch": 1.4430812550281578,
      "grad_norm": 99.5,
      "learning_rate": 1.7113837489943684e-05,
      "loss": 1.0975,
      "step": 28700
    },
    {
      "epoch": 1.4435840707964602,
      "grad_norm": 20.25,
      "learning_rate": 1.7112831858407083e-05,
      "loss": 0.7884,
      "step": 28710
    },
    {
      "epoch": 1.4440868865647627,
      "grad_norm": 8.6875,
      "learning_rate": 1.7111826226870476e-05,
      "loss": 1.0283,
      "step": 28720
    },
    {
      "epoch": 1.4445897023330652,
      "grad_norm": 7.375,
      "learning_rate": 1.711082059533387e-05,
      "loss": 0.9395,
      "step": 28730
    },
    {
      "epoch": 1.4450925181013676,
      "grad_norm": 47.75,
      "learning_rate": 1.7109814963797267e-05,
      "loss": 1.1423,
      "step": 28740
    },
    {
      "epoch": 1.44559533386967,
      "grad_norm": 50.25,
      "learning_rate": 1.710880933226066e-05,
      "loss": 0.9444,
      "step": 28750
    },
    {
      "epoch": 1.4460981496379728,
      "grad_norm": 33.75,
      "learning_rate": 1.7107803700724056e-05,
      "loss": 1.2448,
      "step": 28760
    },
    {
      "epoch": 1.446600965406275,
      "grad_norm": 11.0625,
      "learning_rate": 1.7106798069187452e-05,
      "loss": 0.9538,
      "step": 28770
    },
    {
      "epoch": 1.4471037811745777,
      "grad_norm": 9.8125,
      "learning_rate": 1.7105792437650844e-05,
      "loss": 1.0312,
      "step": 28780
    },
    {
      "epoch": 1.4476065969428802,
      "grad_norm": 19.625,
      "learning_rate": 1.7104786806114243e-05,
      "loss": 1.1202,
      "step": 28790
    },
    {
      "epoch": 1.4481094127111827,
      "grad_norm": 12.125,
      "learning_rate": 1.7103781174577636e-05,
      "loss": 0.8244,
      "step": 28800
    },
    {
      "epoch": 1.4486122284794851,
      "grad_norm": 19.625,
      "learning_rate": 1.7102775543041032e-05,
      "loss": 0.9969,
      "step": 28810
    },
    {
      "epoch": 1.4491150442477876,
      "grad_norm": 23.125,
      "learning_rate": 1.7101769911504428e-05,
      "loss": 1.3132,
      "step": 28820
    },
    {
      "epoch": 1.44961786001609,
      "grad_norm": 31.125,
      "learning_rate": 1.710076427996782e-05,
      "loss": 0.9018,
      "step": 28830
    },
    {
      "epoch": 1.4501206757843925,
      "grad_norm": 27.875,
      "learning_rate": 1.7099758648431216e-05,
      "loss": 0.9388,
      "step": 28840
    },
    {
      "epoch": 1.4506234915526952,
      "grad_norm": 13.875,
      "learning_rate": 1.7098753016894612e-05,
      "loss": 1.0168,
      "step": 28850
    },
    {
      "epoch": 1.4511263073209975,
      "grad_norm": 14.375,
      "learning_rate": 1.7097747385358004e-05,
      "loss": 0.9363,
      "step": 28860
    },
    {
      "epoch": 1.4516291230893001,
      "grad_norm": 36.75,
      "learning_rate": 1.70967417538214e-05,
      "loss": 1.0581,
      "step": 28870
    },
    {
      "epoch": 1.4521319388576026,
      "grad_norm": 15.875,
      "learning_rate": 1.7095736122284796e-05,
      "loss": 1.2267,
      "step": 28880
    },
    {
      "epoch": 1.452634754625905,
      "grad_norm": 27.125,
      "learning_rate": 1.709473049074819e-05,
      "loss": 1.1292,
      "step": 28890
    },
    {
      "epoch": 1.4531375703942075,
      "grad_norm": 51.25,
      "learning_rate": 1.7093724859211588e-05,
      "loss": 0.9255,
      "step": 28900
    },
    {
      "epoch": 1.45364038616251,
      "grad_norm": 26.875,
      "learning_rate": 1.709271922767498e-05,
      "loss": 0.8085,
      "step": 28910
    },
    {
      "epoch": 1.4541432019308125,
      "grad_norm": 33.75,
      "learning_rate": 1.7091713596138376e-05,
      "loss": 0.9051,
      "step": 28920
    },
    {
      "epoch": 1.454646017699115,
      "grad_norm": 5.5,
      "learning_rate": 1.7090707964601772e-05,
      "loss": 0.6851,
      "step": 28930
    },
    {
      "epoch": 1.4551488334674176,
      "grad_norm": 6.90625,
      "learning_rate": 1.7089702333065165e-05,
      "loss": 0.8506,
      "step": 28940
    },
    {
      "epoch": 1.45565164923572,
      "grad_norm": 15.8125,
      "learning_rate": 1.708869670152856e-05,
      "loss": 1.1968,
      "step": 28950
    },
    {
      "epoch": 1.4561544650040226,
      "grad_norm": 49.5,
      "learning_rate": 1.7087691069991956e-05,
      "loss": 1.1321,
      "step": 28960
    },
    {
      "epoch": 1.456657280772325,
      "grad_norm": 13.9375,
      "learning_rate": 1.708668543845535e-05,
      "loss": 0.9458,
      "step": 28970
    },
    {
      "epoch": 1.4571600965406275,
      "grad_norm": 10.1875,
      "learning_rate": 1.7085679806918748e-05,
      "loss": 0.9521,
      "step": 28980
    },
    {
      "epoch": 1.45766291230893,
      "grad_norm": 15.125,
      "learning_rate": 1.708467417538214e-05,
      "loss": 0.8516,
      "step": 28990
    },
    {
      "epoch": 1.4581657280772324,
      "grad_norm": 27.5,
      "learning_rate": 1.7083668543845537e-05,
      "loss": 0.8892,
      "step": 29000
    },
    {
      "epoch": 1.4581657280772324,
      "eval_accuracy": 0.5116261774913238,
      "eval_loss": 1.0260343551635742,
      "eval_runtime": 464.1127,
      "eval_samples_per_second": 86.919,
      "eval_steps_per_second": 86.919,
      "step": 29000
    },
    {
      "epoch": 1.4586685438455351,
      "grad_norm": 31.375,
      "learning_rate": 1.7082662912308932e-05,
      "loss": 1.3804,
      "step": 29010
    },
    {
      "epoch": 1.4591713596138374,
      "grad_norm": 12.0625,
      "learning_rate": 1.7081657280772325e-05,
      "loss": 0.916,
      "step": 29020
    },
    {
      "epoch": 1.45967417538214,
      "grad_norm": 14.125,
      "learning_rate": 1.708065164923572e-05,
      "loss": 0.7717,
      "step": 29030
    },
    {
      "epoch": 1.4601769911504425,
      "grad_norm": 67.0,
      "learning_rate": 1.7079646017699117e-05,
      "loss": 0.9456,
      "step": 29040
    },
    {
      "epoch": 1.460679806918745,
      "grad_norm": 53.75,
      "learning_rate": 1.707864038616251e-05,
      "loss": 1.0219,
      "step": 29050
    },
    {
      "epoch": 1.4611826226870475,
      "grad_norm": 27.0,
      "learning_rate": 1.707763475462591e-05,
      "loss": 0.8159,
      "step": 29060
    },
    {
      "epoch": 1.46168543845535,
      "grad_norm": 24.5,
      "learning_rate": 1.70766291230893e-05,
      "loss": 0.8995,
      "step": 29070
    },
    {
      "epoch": 1.4621882542236524,
      "grad_norm": 6.71875,
      "learning_rate": 1.7075623491552697e-05,
      "loss": 1.0249,
      "step": 29080
    },
    {
      "epoch": 1.4626910699919549,
      "grad_norm": 35.5,
      "learning_rate": 1.7074617860016093e-05,
      "loss": 1.0504,
      "step": 29090
    },
    {
      "epoch": 1.4631938857602576,
      "grad_norm": 3.734375,
      "learning_rate": 1.7073612228479485e-05,
      "loss": 0.9149,
      "step": 29100
    },
    {
      "epoch": 1.46369670152856,
      "grad_norm": 24.875,
      "learning_rate": 1.707260659694288e-05,
      "loss": 0.9293,
      "step": 29110
    },
    {
      "epoch": 1.4641995172968625,
      "grad_norm": 12.3125,
      "learning_rate": 1.7071600965406277e-05,
      "loss": 0.874,
      "step": 29120
    },
    {
      "epoch": 1.464702333065165,
      "grad_norm": 23.5,
      "learning_rate": 1.707059533386967e-05,
      "loss": 0.9675,
      "step": 29130
    },
    {
      "epoch": 1.4652051488334674,
      "grad_norm": 4.90625,
      "learning_rate": 1.7069589702333065e-05,
      "loss": 1.0157,
      "step": 29140
    },
    {
      "epoch": 1.4657079646017699,
      "grad_norm": 28.125,
      "learning_rate": 1.706858407079646e-05,
      "loss": 1.0449,
      "step": 29150
    },
    {
      "epoch": 1.4662107803700724,
      "grad_norm": 7.3125,
      "learning_rate": 1.7067578439259857e-05,
      "loss": 1.1161,
      "step": 29160
    },
    {
      "epoch": 1.4667135961383748,
      "grad_norm": 37.0,
      "learning_rate": 1.7066572807723253e-05,
      "loss": 0.9999,
      "step": 29170
    },
    {
      "epoch": 1.4672164119066773,
      "grad_norm": 12.25,
      "learning_rate": 1.7065567176186645e-05,
      "loss": 1.0585,
      "step": 29180
    },
    {
      "epoch": 1.46771922767498,
      "grad_norm": 23.25,
      "learning_rate": 1.706456154465004e-05,
      "loss": 0.7934,
      "step": 29190
    },
    {
      "epoch": 1.4682220434432824,
      "grad_norm": 61.5,
      "learning_rate": 1.7063555913113437e-05,
      "loss": 0.7809,
      "step": 29200
    },
    {
      "epoch": 1.468724859211585,
      "grad_norm": 4.96875,
      "learning_rate": 1.706255028157683e-05,
      "loss": 0.9493,
      "step": 29210
    },
    {
      "epoch": 1.4692276749798874,
      "grad_norm": 14.0625,
      "learning_rate": 1.7061544650040226e-05,
      "loss": 0.8893,
      "step": 29220
    },
    {
      "epoch": 1.4697304907481898,
      "grad_norm": 23.375,
      "learning_rate": 1.706053901850362e-05,
      "loss": 1.1228,
      "step": 29230
    },
    {
      "epoch": 1.4702333065164923,
      "grad_norm": 6.1875,
      "learning_rate": 1.7059533386967017e-05,
      "loss": 1.1259,
      "step": 29240
    },
    {
      "epoch": 1.4707361222847948,
      "grad_norm": 57.75,
      "learning_rate": 1.7058527755430413e-05,
      "loss": 1.2578,
      "step": 29250
    },
    {
      "epoch": 1.4712389380530975,
      "grad_norm": 4.53125,
      "learning_rate": 1.7057522123893806e-05,
      "loss": 1.0258,
      "step": 29260
    },
    {
      "epoch": 1.4717417538213997,
      "grad_norm": 38.0,
      "learning_rate": 1.70565164923572e-05,
      "loss": 1.0887,
      "step": 29270
    },
    {
      "epoch": 1.4722445695897024,
      "grad_norm": 18.75,
      "learning_rate": 1.7055510860820597e-05,
      "loss": 1.1824,
      "step": 29280
    },
    {
      "epoch": 1.4727473853580049,
      "grad_norm": 25.0,
      "learning_rate": 1.705450522928399e-05,
      "loss": 0.9805,
      "step": 29290
    },
    {
      "epoch": 1.4732502011263073,
      "grad_norm": 31.875,
      "learning_rate": 1.7053499597747386e-05,
      "loss": 1.0021,
      "step": 29300
    },
    {
      "epoch": 1.4737530168946098,
      "grad_norm": 9.6875,
      "learning_rate": 1.7052493966210782e-05,
      "loss": 1.0356,
      "step": 29310
    },
    {
      "epoch": 1.4742558326629123,
      "grad_norm": 5.1875,
      "learning_rate": 1.7051488334674178e-05,
      "loss": 1.0194,
      "step": 29320
    },
    {
      "epoch": 1.4747586484312147,
      "grad_norm": 27.125,
      "learning_rate": 1.7050482703137573e-05,
      "loss": 0.6739,
      "step": 29330
    },
    {
      "epoch": 1.4752614641995172,
      "grad_norm": 22.25,
      "learning_rate": 1.7049477071600966e-05,
      "loss": 0.8764,
      "step": 29340
    },
    {
      "epoch": 1.47576427996782,
      "grad_norm": 14.4375,
      "learning_rate": 1.7048471440064362e-05,
      "loss": 0.7786,
      "step": 29350
    },
    {
      "epoch": 1.4762670957361224,
      "grad_norm": 13.75,
      "learning_rate": 1.7047465808527758e-05,
      "loss": 0.8607,
      "step": 29360
    },
    {
      "epoch": 1.4767699115044248,
      "grad_norm": 62.25,
      "learning_rate": 1.704646017699115e-05,
      "loss": 1.2947,
      "step": 29370
    },
    {
      "epoch": 1.4772727272727273,
      "grad_norm": 19.375,
      "learning_rate": 1.7045454545454546e-05,
      "loss": 0.9218,
      "step": 29380
    },
    {
      "epoch": 1.4777755430410298,
      "grad_norm": 8.3125,
      "learning_rate": 1.7044448913917942e-05,
      "loss": 0.8853,
      "step": 29390
    },
    {
      "epoch": 1.4782783588093322,
      "grad_norm": 9.0625,
      "learning_rate": 1.7043443282381338e-05,
      "loss": 0.5959,
      "step": 29400
    },
    {
      "epoch": 1.4787811745776347,
      "grad_norm": 35.0,
      "learning_rate": 1.704243765084473e-05,
      "loss": 0.6335,
      "step": 29410
    },
    {
      "epoch": 1.4792839903459374,
      "grad_norm": 16.875,
      "learning_rate": 1.7041432019308126e-05,
      "loss": 1.0752,
      "step": 29420
    },
    {
      "epoch": 1.4797868061142396,
      "grad_norm": 11.5625,
      "learning_rate": 1.7040426387771522e-05,
      "loss": 0.7244,
      "step": 29430
    },
    {
      "epoch": 1.4802896218825423,
      "grad_norm": 51.75,
      "learning_rate": 1.7039420756234918e-05,
      "loss": 1.212,
      "step": 29440
    },
    {
      "epoch": 1.4807924376508448,
      "grad_norm": 37.0,
      "learning_rate": 1.703841512469831e-05,
      "loss": 0.7977,
      "step": 29450
    },
    {
      "epoch": 1.4812952534191473,
      "grad_norm": 27.375,
      "learning_rate": 1.7037409493161706e-05,
      "loss": 0.7693,
      "step": 29460
    },
    {
      "epoch": 1.4817980691874497,
      "grad_norm": 35.25,
      "learning_rate": 1.7036403861625102e-05,
      "loss": 1.1282,
      "step": 29470
    },
    {
      "epoch": 1.4823008849557522,
      "grad_norm": 15.25,
      "learning_rate": 1.7035398230088498e-05,
      "loss": 1.1189,
      "step": 29480
    },
    {
      "epoch": 1.4828037007240547,
      "grad_norm": 6.09375,
      "learning_rate": 1.703439259855189e-05,
      "loss": 1.0673,
      "step": 29490
    },
    {
      "epoch": 1.4833065164923571,
      "grad_norm": 41.5,
      "learning_rate": 1.7033386967015287e-05,
      "loss": 0.9391,
      "step": 29500
    },
    {
      "epoch": 1.4833065164923571,
      "eval_accuracy": 0.5118740704015865,
      "eval_loss": 1.0255563259124756,
      "eval_runtime": 465.0012,
      "eval_samples_per_second": 86.752,
      "eval_steps_per_second": 86.752,
      "step": 29500
    },
    {
      "epoch": 1.4838093322606598,
      "grad_norm": 13.625,
      "learning_rate": 1.7032381335478682e-05,
      "loss": 0.8374,
      "step": 29510
    },
    {
      "epoch": 1.484312148028962,
      "grad_norm": 15.25,
      "learning_rate": 1.7031375703942078e-05,
      "loss": 0.7911,
      "step": 29520
    },
    {
      "epoch": 1.4848149637972647,
      "grad_norm": 17.75,
      "learning_rate": 1.703037007240547e-05,
      "loss": 1.417,
      "step": 29530
    },
    {
      "epoch": 1.4853177795655672,
      "grad_norm": 15.4375,
      "learning_rate": 1.7029364440868867e-05,
      "loss": 0.8044,
      "step": 29540
    },
    {
      "epoch": 1.4858205953338697,
      "grad_norm": 9.8125,
      "learning_rate": 1.7028358809332263e-05,
      "loss": 1.1349,
      "step": 29550
    },
    {
      "epoch": 1.4863234111021721,
      "grad_norm": 21.125,
      "learning_rate": 1.702735317779566e-05,
      "loss": 1.0881,
      "step": 29560
    },
    {
      "epoch": 1.4868262268704746,
      "grad_norm": 15.0625,
      "learning_rate": 1.702634754625905e-05,
      "loss": 1.3209,
      "step": 29570
    },
    {
      "epoch": 1.487329042638777,
      "grad_norm": 14.625,
      "learning_rate": 1.7025341914722447e-05,
      "loss": 1.0593,
      "step": 29580
    },
    {
      "epoch": 1.4878318584070795,
      "grad_norm": 15.9375,
      "learning_rate": 1.7024336283185843e-05,
      "loss": 0.7587,
      "step": 29590
    },
    {
      "epoch": 1.4883346741753822,
      "grad_norm": 7.9375,
      "learning_rate": 1.702333065164924e-05,
      "loss": 0.9411,
      "step": 29600
    },
    {
      "epoch": 1.4888374899436847,
      "grad_norm": 70.5,
      "learning_rate": 1.702232502011263e-05,
      "loss": 0.9838,
      "step": 29610
    },
    {
      "epoch": 1.4893403057119872,
      "grad_norm": 18.0,
      "learning_rate": 1.7021319388576027e-05,
      "loss": 1.2983,
      "step": 29620
    },
    {
      "epoch": 1.4898431214802896,
      "grad_norm": 27.125,
      "learning_rate": 1.7020313757039423e-05,
      "loss": 0.6913,
      "step": 29630
    },
    {
      "epoch": 1.490345937248592,
      "grad_norm": 27.5,
      "learning_rate": 1.701930812550282e-05,
      "loss": 1.0428,
      "step": 29640
    },
    {
      "epoch": 1.4908487530168946,
      "grad_norm": 20.125,
      "learning_rate": 1.701830249396621e-05,
      "loss": 0.8005,
      "step": 29650
    },
    {
      "epoch": 1.491351568785197,
      "grad_norm": 9.875,
      "learning_rate": 1.7017296862429607e-05,
      "loss": 0.8466,
      "step": 29660
    },
    {
      "epoch": 1.4918543845534997,
      "grad_norm": 24.5,
      "learning_rate": 1.7016291230893003e-05,
      "loss": 0.8831,
      "step": 29670
    },
    {
      "epoch": 1.492357200321802,
      "grad_norm": 31.25,
      "learning_rate": 1.7015285599356395e-05,
      "loss": 1.1605,
      "step": 29680
    },
    {
      "epoch": 1.4928600160901047,
      "grad_norm": 33.25,
      "learning_rate": 1.701427996781979e-05,
      "loss": 0.8531,
      "step": 29690
    },
    {
      "epoch": 1.4933628318584071,
      "grad_norm": 25.25,
      "learning_rate": 1.7013274336283187e-05,
      "loss": 1.3632,
      "step": 29700
    },
    {
      "epoch": 1.4938656476267096,
      "grad_norm": 39.25,
      "learning_rate": 1.7012268704746583e-05,
      "loss": 0.8736,
      "step": 29710
    },
    {
      "epoch": 1.494368463395012,
      "grad_norm": 20.5,
      "learning_rate": 1.701126307320998e-05,
      "loss": 1.1445,
      "step": 29720
    },
    {
      "epoch": 1.4948712791633145,
      "grad_norm": 13.1875,
      "learning_rate": 1.701025744167337e-05,
      "loss": 0.9888,
      "step": 29730
    },
    {
      "epoch": 1.495374094931617,
      "grad_norm": 51.5,
      "learning_rate": 1.7009251810136767e-05,
      "loss": 0.9938,
      "step": 29740
    },
    {
      "epoch": 1.4958769106999195,
      "grad_norm": 9.5,
      "learning_rate": 1.7008246178600163e-05,
      "loss": 0.9833,
      "step": 29750
    },
    {
      "epoch": 1.4963797264682221,
      "grad_norm": 30.375,
      "learning_rate": 1.7007240547063556e-05,
      "loss": 1.0086,
      "step": 29760
    },
    {
      "epoch": 1.4968825422365246,
      "grad_norm": 28.75,
      "learning_rate": 1.700623491552695e-05,
      "loss": 1.0211,
      "step": 29770
    },
    {
      "epoch": 1.497385358004827,
      "grad_norm": 71.0,
      "learning_rate": 1.7005229283990347e-05,
      "loss": 1.0638,
      "step": 29780
    },
    {
      "epoch": 1.4978881737731295,
      "grad_norm": 12.9375,
      "learning_rate": 1.7004223652453743e-05,
      "loss": 1.0213,
      "step": 29790
    },
    {
      "epoch": 1.498390989541432,
      "grad_norm": 38.0,
      "learning_rate": 1.700321802091714e-05,
      "loss": 1.001,
      "step": 29800
    },
    {
      "epoch": 1.4988938053097345,
      "grad_norm": 18.625,
      "learning_rate": 1.700221238938053e-05,
      "loss": 1.0158,
      "step": 29810
    },
    {
      "epoch": 1.499396621078037,
      "grad_norm": 25.375,
      "learning_rate": 1.7001206757843928e-05,
      "loss": 1.2687,
      "step": 29820
    },
    {
      "epoch": 1.4998994368463396,
      "grad_norm": 10.25,
      "learning_rate": 1.7000201126307323e-05,
      "loss": 0.9247,
      "step": 29830
    },
    {
      "epoch": 1.5004022526146419,
      "grad_norm": 34.0,
      "learning_rate": 1.6999195494770716e-05,
      "loss": 0.8658,
      "step": 29840
    },
    {
      "epoch": 1.5009050683829446,
      "grad_norm": 49.75,
      "learning_rate": 1.6998189863234112e-05,
      "loss": 1.0803,
      "step": 29850
    },
    {
      "epoch": 1.5014078841512468,
      "grad_norm": 16.625,
      "learning_rate": 1.6997184231697508e-05,
      "loss": 1.6505,
      "step": 29860
    },
    {
      "epoch": 1.5019106999195495,
      "grad_norm": 40.0,
      "learning_rate": 1.6996178600160904e-05,
      "loss": 1.1287,
      "step": 29870
    },
    {
      "epoch": 1.502413515687852,
      "grad_norm": 53.0,
      "learning_rate": 1.69951729686243e-05,
      "loss": 0.8106,
      "step": 29880
    },
    {
      "epoch": 1.5029163314561544,
      "grad_norm": 7.8125,
      "learning_rate": 1.6994167337087692e-05,
      "loss": 1.1726,
      "step": 29890
    },
    {
      "epoch": 1.503419147224457,
      "grad_norm": 14.3125,
      "learning_rate": 1.6993161705551088e-05,
      "loss": 0.927,
      "step": 29900
    },
    {
      "epoch": 1.5039219629927594,
      "grad_norm": 30.125,
      "learning_rate": 1.6992156074014484e-05,
      "loss": 1.1414,
      "step": 29910
    },
    {
      "epoch": 1.504424778761062,
      "grad_norm": 12.25,
      "learning_rate": 1.6991150442477876e-05,
      "loss": 0.9328,
      "step": 29920
    },
    {
      "epoch": 1.5049275945293643,
      "grad_norm": 7.09375,
      "learning_rate": 1.6990144810941272e-05,
      "loss": 1.0699,
      "step": 29930
    },
    {
      "epoch": 1.505430410297667,
      "grad_norm": 36.75,
      "learning_rate": 1.6989139179404668e-05,
      "loss": 0.9452,
      "step": 29940
    },
    {
      "epoch": 1.5059332260659695,
      "grad_norm": 49.5,
      "learning_rate": 1.698813354786806e-05,
      "loss": 1.0764,
      "step": 29950
    },
    {
      "epoch": 1.506436041834272,
      "grad_norm": 7.0625,
      "learning_rate": 1.698712791633146e-05,
      "loss": 0.8973,
      "step": 29960
    },
    {
      "epoch": 1.5069388576025744,
      "grad_norm": 23.5,
      "learning_rate": 1.6986122284794852e-05,
      "loss": 1.2942,
      "step": 29970
    },
    {
      "epoch": 1.5074416733708769,
      "grad_norm": 57.0,
      "learning_rate": 1.6985116653258248e-05,
      "loss": 0.8401,
      "step": 29980
    },
    {
      "epoch": 1.5079444891391796,
      "grad_norm": 11.4375,
      "learning_rate": 1.6984111021721644e-05,
      "loss": 1.2233,
      "step": 29990
    },
    {
      "epoch": 1.5084473049074818,
      "grad_norm": 8.1875,
      "learning_rate": 1.6983105390185036e-05,
      "loss": 0.8563,
      "step": 30000
    },
    {
      "epoch": 1.5084473049074818,
      "eval_accuracy": 0.5118988596926128,
      "eval_loss": 1.0256524085998535,
      "eval_runtime": 465.5225,
      "eval_samples_per_second": 86.655,
      "eval_steps_per_second": 86.655,
      "step": 30000
    },
    {
      "epoch": 1.5089501206757845,
      "grad_norm": 29.875,
      "learning_rate": 1.6982099758648432e-05,
      "loss": 0.8501,
      "step": 30010
    },
    {
      "epoch": 1.5094529364440867,
      "grad_norm": 34.0,
      "learning_rate": 1.6981094127111828e-05,
      "loss": 1.0097,
      "step": 30020
    },
    {
      "epoch": 1.5099557522123894,
      "grad_norm": 25.875,
      "learning_rate": 1.698008849557522e-05,
      "loss": 1.0073,
      "step": 30030
    },
    {
      "epoch": 1.510458567980692,
      "grad_norm": 15.625,
      "learning_rate": 1.697908286403862e-05,
      "loss": 1.0661,
      "step": 30040
    },
    {
      "epoch": 1.5109613837489944,
      "grad_norm": 39.25,
      "learning_rate": 1.6978077232502012e-05,
      "loss": 1.0316,
      "step": 30050
    },
    {
      "epoch": 1.5114641995172968,
      "grad_norm": 12.75,
      "learning_rate": 1.6977071600965408e-05,
      "loss": 1.0434,
      "step": 30060
    },
    {
      "epoch": 1.5119670152855993,
      "grad_norm": 16.25,
      "learning_rate": 1.6976065969428804e-05,
      "loss": 1.2525,
      "step": 30070
    },
    {
      "epoch": 1.512469831053902,
      "grad_norm": 49.0,
      "learning_rate": 1.6975060337892197e-05,
      "loss": 1.1104,
      "step": 30080
    },
    {
      "epoch": 1.5129726468222042,
      "grad_norm": 10.1875,
      "learning_rate": 1.6974054706355593e-05,
      "loss": 1.1234,
      "step": 30090
    },
    {
      "epoch": 1.513475462590507,
      "grad_norm": 40.25,
      "learning_rate": 1.697304907481899e-05,
      "loss": 1.0934,
      "step": 30100
    },
    {
      "epoch": 1.5139782783588094,
      "grad_norm": 54.0,
      "learning_rate": 1.697204344328238e-05,
      "loss": 1.2148,
      "step": 30110
    },
    {
      "epoch": 1.5144810941271118,
      "grad_norm": 20.75,
      "learning_rate": 1.697103781174578e-05,
      "loss": 0.992,
      "step": 30120
    },
    {
      "epoch": 1.5149839098954143,
      "grad_norm": 43.5,
      "learning_rate": 1.6970032180209173e-05,
      "loss": 0.9969,
      "step": 30130
    },
    {
      "epoch": 1.5154867256637168,
      "grad_norm": 19.75,
      "learning_rate": 1.696902654867257e-05,
      "loss": 1.0707,
      "step": 30140
    },
    {
      "epoch": 1.5159895414320195,
      "grad_norm": 47.75,
      "learning_rate": 1.6968020917135964e-05,
      "loss": 1.4583,
      "step": 30150
    },
    {
      "epoch": 1.5164923572003217,
      "grad_norm": 8.5625,
      "learning_rate": 1.6967015285599357e-05,
      "loss": 0.7571,
      "step": 30160
    },
    {
      "epoch": 1.5169951729686244,
      "grad_norm": 9.625,
      "learning_rate": 1.6966009654062753e-05,
      "loss": 0.756,
      "step": 30170
    },
    {
      "epoch": 1.5174979887369267,
      "grad_norm": 15.5625,
      "learning_rate": 1.696500402252615e-05,
      "loss": 0.6748,
      "step": 30180
    },
    {
      "epoch": 1.5180008045052293,
      "grad_norm": 79.0,
      "learning_rate": 1.696399839098954e-05,
      "loss": 1.1211,
      "step": 30190
    },
    {
      "epoch": 1.5185036202735318,
      "grad_norm": 30.75,
      "learning_rate": 1.6962992759452937e-05,
      "loss": 0.8689,
      "step": 30200
    },
    {
      "epoch": 1.5190064360418343,
      "grad_norm": 21.375,
      "learning_rate": 1.6961987127916333e-05,
      "loss": 1.1325,
      "step": 30210
    },
    {
      "epoch": 1.5195092518101367,
      "grad_norm": 43.25,
      "learning_rate": 1.6960981496379725e-05,
      "loss": 1.0335,
      "step": 30220
    },
    {
      "epoch": 1.5200120675784392,
      "grad_norm": 11.5,
      "learning_rate": 1.6959975864843125e-05,
      "loss": 1.0509,
      "step": 30230
    },
    {
      "epoch": 1.520514883346742,
      "grad_norm": 24.5,
      "learning_rate": 1.6958970233306517e-05,
      "loss": 0.9006,
      "step": 30240
    },
    {
      "epoch": 1.5210176991150441,
      "grad_norm": 24.625,
      "learning_rate": 1.6957964601769913e-05,
      "loss": 0.9462,
      "step": 30250
    },
    {
      "epoch": 1.5215205148833468,
      "grad_norm": 80.0,
      "learning_rate": 1.695695897023331e-05,
      "loss": 1.2025,
      "step": 30260
    },
    {
      "epoch": 1.522023330651649,
      "grad_norm": 13.375,
      "learning_rate": 1.69559533386967e-05,
      "loss": 1.0258,
      "step": 30270
    },
    {
      "epoch": 1.5225261464199518,
      "grad_norm": 12.125,
      "learning_rate": 1.6954947707160097e-05,
      "loss": 0.7776,
      "step": 30280
    },
    {
      "epoch": 1.5230289621882542,
      "grad_norm": 31.25,
      "learning_rate": 1.6953942075623493e-05,
      "loss": 1.0813,
      "step": 30290
    },
    {
      "epoch": 1.5235317779565567,
      "grad_norm": 13.5,
      "learning_rate": 1.6952936444086886e-05,
      "loss": 1.0934,
      "step": 30300
    },
    {
      "epoch": 1.5240345937248592,
      "grad_norm": 19.5,
      "learning_rate": 1.6951930812550285e-05,
      "loss": 1.2092,
      "step": 30310
    },
    {
      "epoch": 1.5245374094931616,
      "grad_norm": 22.0,
      "learning_rate": 1.6950925181013677e-05,
      "loss": 0.8415,
      "step": 30320
    },
    {
      "epoch": 1.5250402252614643,
      "grad_norm": 21.0,
      "learning_rate": 1.6949919549477073e-05,
      "loss": 1.3902,
      "step": 30330
    },
    {
      "epoch": 1.5255430410297666,
      "grad_norm": 21.375,
      "learning_rate": 1.694891391794047e-05,
      "loss": 1.1695,
      "step": 30340
    },
    {
      "epoch": 1.5260458567980693,
      "grad_norm": 40.0,
      "learning_rate": 1.694790828640386e-05,
      "loss": 1.0308,
      "step": 30350
    },
    {
      "epoch": 1.5265486725663717,
      "grad_norm": 23.25,
      "learning_rate": 1.6946902654867258e-05,
      "loss": 0.8293,
      "step": 30360
    },
    {
      "epoch": 1.5270514883346742,
      "grad_norm": 9.8125,
      "learning_rate": 1.6945897023330653e-05,
      "loss": 1.1144,
      "step": 30370
    },
    {
      "epoch": 1.5275543041029767,
      "grad_norm": 9.0625,
      "learning_rate": 1.6944891391794046e-05,
      "loss": 0.9023,
      "step": 30380
    },
    {
      "epoch": 1.5280571198712791,
      "grad_norm": 53.5,
      "learning_rate": 1.6943885760257445e-05,
      "loss": 0.9717,
      "step": 30390
    },
    {
      "epoch": 1.5285599356395818,
      "grad_norm": 18.75,
      "learning_rate": 1.6942880128720838e-05,
      "loss": 1.4354,
      "step": 30400
    },
    {
      "epoch": 1.529062751407884,
      "grad_norm": 79.0,
      "learning_rate": 1.6941874497184234e-05,
      "loss": 1.0729,
      "step": 30410
    },
    {
      "epoch": 1.5295655671761867,
      "grad_norm": 27.0,
      "learning_rate": 1.694086886564763e-05,
      "loss": 1.1544,
      "step": 30420
    },
    {
      "epoch": 1.530068382944489,
      "grad_norm": 12.8125,
      "learning_rate": 1.6939863234111022e-05,
      "loss": 0.9018,
      "step": 30430
    },
    {
      "epoch": 1.5305711987127917,
      "grad_norm": 18.875,
      "learning_rate": 1.6938857602574418e-05,
      "loss": 1.0562,
      "step": 30440
    },
    {
      "epoch": 1.5310740144810941,
      "grad_norm": 66.5,
      "learning_rate": 1.6937851971037814e-05,
      "loss": 0.9617,
      "step": 30450
    },
    {
      "epoch": 1.5315768302493966,
      "grad_norm": 12.1875,
      "learning_rate": 1.6936846339501206e-05,
      "loss": 0.6207,
      "step": 30460
    },
    {
      "epoch": 1.532079646017699,
      "grad_norm": 7.375,
      "learning_rate": 1.6935840707964602e-05,
      "loss": 0.7796,
      "step": 30470
    },
    {
      "epoch": 1.5325824617860015,
      "grad_norm": 45.5,
      "learning_rate": 1.6934835076427998e-05,
      "loss": 0.9403,
      "step": 30480
    },
    {
      "epoch": 1.5330852775543042,
      "grad_norm": 7.53125,
      "learning_rate": 1.6933829444891394e-05,
      "loss": 1.0493,
      "step": 30490
    },
    {
      "epoch": 1.5335880933226065,
      "grad_norm": 119.5,
      "learning_rate": 1.693282381335479e-05,
      "loss": 1.0694,
      "step": 30500
    },
    {
      "epoch": 1.5335880933226065,
      "eval_accuracy": 0.5113287059990085,
      "eval_loss": 1.0252476930618286,
      "eval_runtime": 466.124,
      "eval_samples_per_second": 86.543,
      "eval_steps_per_second": 86.543,
      "step": 30500
    },
    {
      "epoch": 1.5340909090909092,
      "grad_norm": 56.0,
      "learning_rate": 1.6931818181818182e-05,
      "loss": 0.9994,
      "step": 30510
    },
    {
      "epoch": 1.5345937248592116,
      "grad_norm": 23.375,
      "learning_rate": 1.6930812550281578e-05,
      "loss": 1.0398,
      "step": 30520
    },
    {
      "epoch": 1.535096540627514,
      "grad_norm": 38.5,
      "learning_rate": 1.6929806918744974e-05,
      "loss": 0.9128,
      "step": 30530
    },
    {
      "epoch": 1.5355993563958166,
      "grad_norm": 19.875,
      "learning_rate": 1.6928801287208366e-05,
      "loss": 0.8477,
      "step": 30540
    },
    {
      "epoch": 1.536102172164119,
      "grad_norm": 30.25,
      "learning_rate": 1.6927795655671762e-05,
      "loss": 0.8159,
      "step": 30550
    },
    {
      "epoch": 1.5366049879324215,
      "grad_norm": 30.625,
      "learning_rate": 1.6926790024135158e-05,
      "loss": 1.205,
      "step": 30560
    },
    {
      "epoch": 1.537107803700724,
      "grad_norm": 37.5,
      "learning_rate": 1.6925784392598554e-05,
      "loss": 1.0607,
      "step": 30570
    },
    {
      "epoch": 1.5376106194690267,
      "grad_norm": 15.75,
      "learning_rate": 1.692477876106195e-05,
      "loss": 0.8336,
      "step": 30580
    },
    {
      "epoch": 1.538113435237329,
      "grad_norm": 43.75,
      "learning_rate": 1.6923773129525342e-05,
      "loss": 0.8791,
      "step": 30590
    },
    {
      "epoch": 1.5386162510056316,
      "grad_norm": 28.375,
      "learning_rate": 1.692276749798874e-05,
      "loss": 0.8645,
      "step": 30600
    },
    {
      "epoch": 1.539119066773934,
      "grad_norm": 44.5,
      "learning_rate": 1.6921761866452134e-05,
      "loss": 0.8623,
      "step": 30610
    },
    {
      "epoch": 1.5396218825422365,
      "grad_norm": 12.75,
      "learning_rate": 1.6920756234915527e-05,
      "loss": 0.9695,
      "step": 30620
    },
    {
      "epoch": 1.540124698310539,
      "grad_norm": 17.625,
      "learning_rate": 1.6919750603378923e-05,
      "loss": 1.0245,
      "step": 30630
    },
    {
      "epoch": 1.5406275140788415,
      "grad_norm": 10.75,
      "learning_rate": 1.691874497184232e-05,
      "loss": 0.9945,
      "step": 30640
    },
    {
      "epoch": 1.5411303298471442,
      "grad_norm": 13.875,
      "learning_rate": 1.6917739340305714e-05,
      "loss": 1.0597,
      "step": 30650
    },
    {
      "epoch": 1.5416331456154464,
      "grad_norm": 9.0,
      "learning_rate": 1.691673370876911e-05,
      "loss": 1.0633,
      "step": 30660
    },
    {
      "epoch": 1.542135961383749,
      "grad_norm": 7.90625,
      "learning_rate": 1.6915728077232503e-05,
      "loss": 0.9338,
      "step": 30670
    },
    {
      "epoch": 1.5426387771520513,
      "grad_norm": 6.15625,
      "learning_rate": 1.69147224456959e-05,
      "loss": 1.082,
      "step": 30680
    },
    {
      "epoch": 1.543141592920354,
      "grad_norm": 17.125,
      "learning_rate": 1.6913716814159294e-05,
      "loss": 1.1744,
      "step": 30690
    },
    {
      "epoch": 1.5436444086886565,
      "grad_norm": 15.1875,
      "learning_rate": 1.6912711182622687e-05,
      "loss": 0.8383,
      "step": 30700
    },
    {
      "epoch": 1.544147224456959,
      "grad_norm": 19.75,
      "learning_rate": 1.6911705551086083e-05,
      "loss": 0.7889,
      "step": 30710
    },
    {
      "epoch": 1.5446500402252614,
      "grad_norm": 31.25,
      "learning_rate": 1.691069991954948e-05,
      "loss": 0.9441,
      "step": 30720
    },
    {
      "epoch": 1.545152855993564,
      "grad_norm": 25.875,
      "learning_rate": 1.6909694288012875e-05,
      "loss": 1.0763,
      "step": 30730
    },
    {
      "epoch": 1.5456556717618666,
      "grad_norm": 14.75,
      "learning_rate": 1.6908688656476267e-05,
      "loss": 0.9365,
      "step": 30740
    },
    {
      "epoch": 1.5461584875301688,
      "grad_norm": 39.25,
      "learning_rate": 1.6907683024939663e-05,
      "loss": 1.341,
      "step": 30750
    },
    {
      "epoch": 1.5466613032984715,
      "grad_norm": 18.5,
      "learning_rate": 1.690667739340306e-05,
      "loss": 0.8683,
      "step": 30760
    },
    {
      "epoch": 1.547164119066774,
      "grad_norm": 28.125,
      "learning_rate": 1.6905671761866455e-05,
      "loss": 1.2096,
      "step": 30770
    },
    {
      "epoch": 1.5476669348350764,
      "grad_norm": 15.875,
      "learning_rate": 1.6904666130329847e-05,
      "loss": 0.8347,
      "step": 30780
    },
    {
      "epoch": 1.548169750603379,
      "grad_norm": 57.0,
      "learning_rate": 1.6903660498793243e-05,
      "loss": 1.1822,
      "step": 30790
    },
    {
      "epoch": 1.5486725663716814,
      "grad_norm": 35.0,
      "learning_rate": 1.690265486725664e-05,
      "loss": 0.9257,
      "step": 30800
    },
    {
      "epoch": 1.549175382139984,
      "grad_norm": 52.75,
      "learning_rate": 1.6901649235720035e-05,
      "loss": 0.9854,
      "step": 30810
    },
    {
      "epoch": 1.5496781979082863,
      "grad_norm": 41.0,
      "learning_rate": 1.6900643604183427e-05,
      "loss": 1.0632,
      "step": 30820
    },
    {
      "epoch": 1.550181013676589,
      "grad_norm": 9.375,
      "learning_rate": 1.6899637972646823e-05,
      "loss": 0.8412,
      "step": 30830
    },
    {
      "epoch": 1.5506838294448912,
      "grad_norm": 18.875,
      "learning_rate": 1.689863234111022e-05,
      "loss": 1.0999,
      "step": 30840
    },
    {
      "epoch": 1.551186645213194,
      "grad_norm": 10.5,
      "learning_rate": 1.6897626709573615e-05,
      "loss": 0.7441,
      "step": 30850
    },
    {
      "epoch": 1.5516894609814964,
      "grad_norm": 56.5,
      "learning_rate": 1.6896621078037007e-05,
      "loss": 1.054,
      "step": 30860
    },
    {
      "epoch": 1.5521922767497989,
      "grad_norm": 7.4375,
      "learning_rate": 1.6895615446500403e-05,
      "loss": 0.8734,
      "step": 30870
    },
    {
      "epoch": 1.5526950925181013,
      "grad_norm": 29.5,
      "learning_rate": 1.68946098149638e-05,
      "loss": 1.0145,
      "step": 30880
    },
    {
      "epoch": 1.5531979082864038,
      "grad_norm": 45.5,
      "learning_rate": 1.6893604183427195e-05,
      "loss": 1.1692,
      "step": 30890
    },
    {
      "epoch": 1.5537007240547065,
      "grad_norm": 9.8125,
      "learning_rate": 1.6892598551890588e-05,
      "loss": 1.2186,
      "step": 30900
    },
    {
      "epoch": 1.5542035398230087,
      "grad_norm": 15.4375,
      "learning_rate": 1.6891592920353983e-05,
      "loss": 0.8556,
      "step": 30910
    },
    {
      "epoch": 1.5547063555913114,
      "grad_norm": 20.375,
      "learning_rate": 1.689058728881738e-05,
      "loss": 0.8574,
      "step": 30920
    },
    {
      "epoch": 1.5552091713596137,
      "grad_norm": 11.3125,
      "learning_rate": 1.6889581657280775e-05,
      "loss": 1.012,
      "step": 30930
    },
    {
      "epoch": 1.5557119871279164,
      "grad_norm": 42.75,
      "learning_rate": 1.6888576025744168e-05,
      "loss": 1.1574,
      "step": 30940
    },
    {
      "epoch": 1.5562148028962188,
      "grad_norm": 35.25,
      "learning_rate": 1.6887570394207564e-05,
      "loss": 0.9172,
      "step": 30950
    },
    {
      "epoch": 1.5567176186645213,
      "grad_norm": 38.75,
      "learning_rate": 1.688656476267096e-05,
      "loss": 1.2656,
      "step": 30960
    },
    {
      "epoch": 1.5572204344328238,
      "grad_norm": 22.375,
      "learning_rate": 1.6885559131134355e-05,
      "loss": 1.2258,
      "step": 30970
    },
    {
      "epoch": 1.5577232502011262,
      "grad_norm": 30.5,
      "learning_rate": 1.6884553499597748e-05,
      "loss": 0.9932,
      "step": 30980
    },
    {
      "epoch": 1.558226065969429,
      "grad_norm": 16.25,
      "learning_rate": 1.6883547868061144e-05,
      "loss": 0.9863,
      "step": 30990
    },
    {
      "epoch": 1.5587288817377312,
      "grad_norm": 37.75,
      "learning_rate": 1.688254223652454e-05,
      "loss": 1.0073,
      "step": 31000
    },
    {
      "epoch": 1.5587288817377312,
      "eval_accuracy": 0.5121963311849281,
      "eval_loss": 1.025100827217102,
      "eval_runtime": 465.5257,
      "eval_samples_per_second": 86.655,
      "eval_steps_per_second": 86.655,
      "step": 31000
    },
    {
      "epoch": 1.5592316975060339,
      "grad_norm": 37.25,
      "learning_rate": 1.6881536604987932e-05,
      "loss": 1.144,
      "step": 31010
    },
    {
      "epoch": 1.5597345132743363,
      "grad_norm": 6.28125,
      "learning_rate": 1.6880530973451328e-05,
      "loss": 0.9005,
      "step": 31020
    },
    {
      "epoch": 1.5602373290426388,
      "grad_norm": 10.6875,
      "learning_rate": 1.6879525341914724e-05,
      "loss": 0.5832,
      "step": 31030
    },
    {
      "epoch": 1.5607401448109413,
      "grad_norm": 15.4375,
      "learning_rate": 1.687851971037812e-05,
      "loss": 0.8562,
      "step": 31040
    },
    {
      "epoch": 1.5612429605792437,
      "grad_norm": 12.375,
      "learning_rate": 1.6877514078841516e-05,
      "loss": 1.1508,
      "step": 31050
    },
    {
      "epoch": 1.5617457763475464,
      "grad_norm": 5.96875,
      "learning_rate": 1.6876508447304908e-05,
      "loss": 1.1421,
      "step": 31060
    },
    {
      "epoch": 1.5622485921158487,
      "grad_norm": 16.0,
      "learning_rate": 1.6875502815768304e-05,
      "loss": 0.8287,
      "step": 31070
    },
    {
      "epoch": 1.5627514078841513,
      "grad_norm": 7.875,
      "learning_rate": 1.68744971842317e-05,
      "loss": 1.3316,
      "step": 31080
    },
    {
      "epoch": 1.5632542236524536,
      "grad_norm": 14.375,
      "learning_rate": 1.6873491552695092e-05,
      "loss": 0.7115,
      "step": 31090
    },
    {
      "epoch": 1.5637570394207563,
      "grad_norm": 34.0,
      "learning_rate": 1.6872485921158488e-05,
      "loss": 0.8785,
      "step": 31100
    },
    {
      "epoch": 1.5642598551890587,
      "grad_norm": 12.6875,
      "learning_rate": 1.6871480289621884e-05,
      "loss": 1.0734,
      "step": 31110
    },
    {
      "epoch": 1.5647626709573612,
      "grad_norm": 12.0,
      "learning_rate": 1.687047465808528e-05,
      "loss": 0.7909,
      "step": 31120
    },
    {
      "epoch": 1.5652654867256637,
      "grad_norm": 15.625,
      "learning_rate": 1.6869469026548676e-05,
      "loss": 0.9091,
      "step": 31130
    },
    {
      "epoch": 1.5657683024939661,
      "grad_norm": 27.875,
      "learning_rate": 1.686846339501207e-05,
      "loss": 0.9691,
      "step": 31140
    },
    {
      "epoch": 1.5662711182622688,
      "grad_norm": 12.8125,
      "learning_rate": 1.6867457763475464e-05,
      "loss": 0.7055,
      "step": 31150
    },
    {
      "epoch": 1.566773934030571,
      "grad_norm": 25.125,
      "learning_rate": 1.686645213193886e-05,
      "loss": 1.3371,
      "step": 31160
    },
    {
      "epoch": 1.5672767497988738,
      "grad_norm": 52.5,
      "learning_rate": 1.6865446500402253e-05,
      "loss": 1.002,
      "step": 31170
    },
    {
      "epoch": 1.5677795655671762,
      "grad_norm": 13.1875,
      "learning_rate": 1.686444086886565e-05,
      "loss": 0.7929,
      "step": 31180
    },
    {
      "epoch": 1.5682823813354787,
      "grad_norm": 26.25,
      "learning_rate": 1.6863435237329044e-05,
      "loss": 1.1957,
      "step": 31190
    },
    {
      "epoch": 1.5687851971037812,
      "grad_norm": 30.5,
      "learning_rate": 1.686242960579244e-05,
      "loss": 0.9459,
      "step": 31200
    },
    {
      "epoch": 1.5692880128720836,
      "grad_norm": 20.25,
      "learning_rate": 1.6861423974255836e-05,
      "loss": 1.0148,
      "step": 31210
    },
    {
      "epoch": 1.5697908286403863,
      "grad_norm": 58.5,
      "learning_rate": 1.686041834271923e-05,
      "loss": 1.0629,
      "step": 31220
    },
    {
      "epoch": 1.5702936444086886,
      "grad_norm": 12.5625,
      "learning_rate": 1.6859412711182624e-05,
      "loss": 0.7868,
      "step": 31230
    },
    {
      "epoch": 1.5707964601769913,
      "grad_norm": 64.0,
      "learning_rate": 1.685840707964602e-05,
      "loss": 1.1365,
      "step": 31240
    },
    {
      "epoch": 1.5712992759452935,
      "grad_norm": 45.75,
      "learning_rate": 1.6857401448109413e-05,
      "loss": 1.1438,
      "step": 31250
    },
    {
      "epoch": 1.5718020917135962,
      "grad_norm": 8.4375,
      "learning_rate": 1.685639581657281e-05,
      "loss": 1.2121,
      "step": 31260
    },
    {
      "epoch": 1.5723049074818987,
      "grad_norm": 19.375,
      "learning_rate": 1.6855390185036205e-05,
      "loss": 0.9543,
      "step": 31270
    },
    {
      "epoch": 1.5728077232502011,
      "grad_norm": 22.0,
      "learning_rate": 1.6854384553499597e-05,
      "loss": 1.2719,
      "step": 31280
    },
    {
      "epoch": 1.5733105390185036,
      "grad_norm": 12.125,
      "learning_rate": 1.6853378921962996e-05,
      "loss": 0.9501,
      "step": 31290
    },
    {
      "epoch": 1.573813354786806,
      "grad_norm": 15.125,
      "learning_rate": 1.685237329042639e-05,
      "loss": 1.311,
      "step": 31300
    },
    {
      "epoch": 1.5743161705551088,
      "grad_norm": 65.0,
      "learning_rate": 1.6851367658889785e-05,
      "loss": 0.9785,
      "step": 31310
    },
    {
      "epoch": 1.574818986323411,
      "grad_norm": 83.0,
      "learning_rate": 1.685036202735318e-05,
      "loss": 1.1379,
      "step": 31320
    },
    {
      "epoch": 1.5753218020917137,
      "grad_norm": 18.875,
      "learning_rate": 1.6849356395816573e-05,
      "loss": 0.9162,
      "step": 31330
    },
    {
      "epoch": 1.575824617860016,
      "grad_norm": 15.1875,
      "learning_rate": 1.684835076427997e-05,
      "loss": 1.1401,
      "step": 31340
    },
    {
      "epoch": 1.5763274336283186,
      "grad_norm": 25.875,
      "learning_rate": 1.6847345132743365e-05,
      "loss": 0.977,
      "step": 31350
    },
    {
      "epoch": 1.576830249396621,
      "grad_norm": 4.5625,
      "learning_rate": 1.6846339501206757e-05,
      "loss": 0.8554,
      "step": 31360
    },
    {
      "epoch": 1.5773330651649236,
      "grad_norm": 28.25,
      "learning_rate": 1.6845333869670157e-05,
      "loss": 0.6816,
      "step": 31370
    },
    {
      "epoch": 1.577835880933226,
      "grad_norm": 7.8125,
      "learning_rate": 1.684432823813355e-05,
      "loss": 0.9491,
      "step": 31380
    },
    {
      "epoch": 1.5783386967015285,
      "grad_norm": 38.5,
      "learning_rate": 1.6843322606596945e-05,
      "loss": 1.2258,
      "step": 31390
    },
    {
      "epoch": 1.5788415124698312,
      "grad_norm": 10.625,
      "learning_rate": 1.684231697506034e-05,
      "loss": 1.2828,
      "step": 31400
    },
    {
      "epoch": 1.5793443282381334,
      "grad_norm": 14.1875,
      "learning_rate": 1.6841311343523733e-05,
      "loss": 0.9038,
      "step": 31410
    },
    {
      "epoch": 1.579847144006436,
      "grad_norm": 13.5,
      "learning_rate": 1.684030571198713e-05,
      "loss": 0.7747,
      "step": 31420
    },
    {
      "epoch": 1.5803499597747386,
      "grad_norm": 56.25,
      "learning_rate": 1.6839300080450525e-05,
      "loss": 0.9544,
      "step": 31430
    },
    {
      "epoch": 1.580852775543041,
      "grad_norm": 65.5,
      "learning_rate": 1.6838294448913918e-05,
      "loss": 1.2207,
      "step": 31440
    },
    {
      "epoch": 1.5813555913113435,
      "grad_norm": 31.0,
      "learning_rate": 1.6837288817377317e-05,
      "loss": 1.2127,
      "step": 31450
    },
    {
      "epoch": 1.581858407079646,
      "grad_norm": 18.25,
      "learning_rate": 1.683628318584071e-05,
      "loss": 1.0561,
      "step": 31460
    },
    {
      "epoch": 1.5823612228479487,
      "grad_norm": 27.625,
      "learning_rate": 1.6835277554304105e-05,
      "loss": 0.826,
      "step": 31470
    },
    {
      "epoch": 1.582864038616251,
      "grad_norm": 18.75,
      "learning_rate": 1.68342719227675e-05,
      "loss": 0.9848,
      "step": 31480
    },
    {
      "epoch": 1.5833668543845536,
      "grad_norm": 9.75,
      "learning_rate": 1.6833266291230894e-05,
      "loss": 1.0819,
      "step": 31490
    },
    {
      "epoch": 1.5838696701528558,
      "grad_norm": 26.25,
      "learning_rate": 1.683226065969429e-05,
      "loss": 0.8758,
      "step": 31500
    },
    {
      "epoch": 1.5838696701528558,
      "eval_accuracy": 0.5121219633118492,
      "eval_loss": 1.0259000062942505,
      "eval_runtime": 466.2684,
      "eval_samples_per_second": 86.517,
      "eval_steps_per_second": 86.517,
      "step": 31500
    },
    {
      "epoch": 1.5843724859211585,
      "grad_norm": 31.875,
      "learning_rate": 1.6831255028157685e-05,
      "loss": 1.302,
      "step": 31510
    },
    {
      "epoch": 1.584875301689461,
      "grad_norm": 52.5,
      "learning_rate": 1.6830249396621078e-05,
      "loss": 0.6147,
      "step": 31520
    },
    {
      "epoch": 1.5853781174577635,
      "grad_norm": 9.625,
      "learning_rate": 1.6829243765084474e-05,
      "loss": 0.9423,
      "step": 31530
    },
    {
      "epoch": 1.585880933226066,
      "grad_norm": 32.0,
      "learning_rate": 1.682823813354787e-05,
      "loss": 0.8199,
      "step": 31540
    },
    {
      "epoch": 1.5863837489943684,
      "grad_norm": 39.5,
      "learning_rate": 1.6827232502011262e-05,
      "loss": 1.0921,
      "step": 31550
    },
    {
      "epoch": 1.586886564762671,
      "grad_norm": 25.5,
      "learning_rate": 1.682622687047466e-05,
      "loss": 0.8707,
      "step": 31560
    },
    {
      "epoch": 1.5873893805309733,
      "grad_norm": 5.1875,
      "learning_rate": 1.6825221238938054e-05,
      "loss": 1.2554,
      "step": 31570
    },
    {
      "epoch": 1.587892196299276,
      "grad_norm": 32.75,
      "learning_rate": 1.682421560740145e-05,
      "loss": 1.0564,
      "step": 31580
    },
    {
      "epoch": 1.5883950120675785,
      "grad_norm": 59.25,
      "learning_rate": 1.6823209975864846e-05,
      "loss": 1.0239,
      "step": 31590
    },
    {
      "epoch": 1.588897827835881,
      "grad_norm": 9.9375,
      "learning_rate": 1.6822204344328238e-05,
      "loss": 0.9877,
      "step": 31600
    },
    {
      "epoch": 1.5894006436041834,
      "grad_norm": 68.5,
      "learning_rate": 1.6821198712791634e-05,
      "loss": 0.8101,
      "step": 31610
    },
    {
      "epoch": 1.589903459372486,
      "grad_norm": 13.0625,
      "learning_rate": 1.682019308125503e-05,
      "loss": 1.1594,
      "step": 31620
    },
    {
      "epoch": 1.5904062751407884,
      "grad_norm": 16.875,
      "learning_rate": 1.6819187449718422e-05,
      "loss": 0.9949,
      "step": 31630
    },
    {
      "epoch": 1.5909090909090908,
      "grad_norm": 28.75,
      "learning_rate": 1.681818181818182e-05,
      "loss": 0.7628,
      "step": 31640
    },
    {
      "epoch": 1.5914119066773935,
      "grad_norm": 22.5,
      "learning_rate": 1.6817176186645214e-05,
      "loss": 1.1559,
      "step": 31650
    },
    {
      "epoch": 1.5919147224456958,
      "grad_norm": 70.0,
      "learning_rate": 1.681617055510861e-05,
      "loss": 1.3046,
      "step": 31660
    },
    {
      "epoch": 1.5924175382139985,
      "grad_norm": 39.0,
      "learning_rate": 1.6815164923572006e-05,
      "loss": 1.0301,
      "step": 31670
    },
    {
      "epoch": 1.592920353982301,
      "grad_norm": 53.75,
      "learning_rate": 1.68141592920354e-05,
      "loss": 1.0824,
      "step": 31680
    },
    {
      "epoch": 1.5934231697506034,
      "grad_norm": 21.5,
      "learning_rate": 1.6813153660498794e-05,
      "loss": 0.9126,
      "step": 31690
    },
    {
      "epoch": 1.5939259855189059,
      "grad_norm": 9.0625,
      "learning_rate": 1.681214802896219e-05,
      "loss": 0.7917,
      "step": 31700
    },
    {
      "epoch": 1.5944288012872083,
      "grad_norm": 22.875,
      "learning_rate": 1.6811142397425583e-05,
      "loss": 0.992,
      "step": 31710
    },
    {
      "epoch": 1.594931617055511,
      "grad_norm": 83.5,
      "learning_rate": 1.6810136765888982e-05,
      "loss": 1.0011,
      "step": 31720
    },
    {
      "epoch": 1.5954344328238133,
      "grad_norm": 10.875,
      "learning_rate": 1.6809131134352374e-05,
      "loss": 0.9111,
      "step": 31730
    },
    {
      "epoch": 1.595937248592116,
      "grad_norm": 26.75,
      "learning_rate": 1.680812550281577e-05,
      "loss": 1.1833,
      "step": 31740
    },
    {
      "epoch": 1.5964400643604182,
      "grad_norm": 87.5,
      "learning_rate": 1.6807119871279166e-05,
      "loss": 1.0857,
      "step": 31750
    },
    {
      "epoch": 1.5969428801287209,
      "grad_norm": 10.4375,
      "learning_rate": 1.680611423974256e-05,
      "loss": 1.1092,
      "step": 31760
    },
    {
      "epoch": 1.5974456958970233,
      "grad_norm": 13.1875,
      "learning_rate": 1.6805108608205955e-05,
      "loss": 0.9503,
      "step": 31770
    },
    {
      "epoch": 1.5979485116653258,
      "grad_norm": 10.125,
      "learning_rate": 1.680410297666935e-05,
      "loss": 0.9397,
      "step": 31780
    },
    {
      "epoch": 1.5984513274336283,
      "grad_norm": 25.75,
      "learning_rate": 1.6803097345132743e-05,
      "loss": 1.0286,
      "step": 31790
    },
    {
      "epoch": 1.5989541432019307,
      "grad_norm": 7.0,
      "learning_rate": 1.680209171359614e-05,
      "loss": 0.9671,
      "step": 31800
    },
    {
      "epoch": 1.5994569589702334,
      "grad_norm": 16.25,
      "learning_rate": 1.6801086082059535e-05,
      "loss": 1.1065,
      "step": 31810
    },
    {
      "epoch": 1.5999597747385357,
      "grad_norm": 50.75,
      "learning_rate": 1.6800080450522927e-05,
      "loss": 1.2864,
      "step": 31820
    },
    {
      "epoch": 1.6004625905068384,
      "grad_norm": 14.125,
      "learning_rate": 1.6799074818986326e-05,
      "loss": 1.1793,
      "step": 31830
    },
    {
      "epoch": 1.6009654062751408,
      "grad_norm": 24.75,
      "learning_rate": 1.679806918744972e-05,
      "loss": 0.9746,
      "step": 31840
    },
    {
      "epoch": 1.6014682220434433,
      "grad_norm": 16.25,
      "learning_rate": 1.6797063555913115e-05,
      "loss": 0.6689,
      "step": 31850
    },
    {
      "epoch": 1.6019710378117458,
      "grad_norm": 4.875,
      "learning_rate": 1.679605792437651e-05,
      "loss": 0.9222,
      "step": 31860
    },
    {
      "epoch": 1.6024738535800482,
      "grad_norm": 8.4375,
      "learning_rate": 1.6795052292839903e-05,
      "loss": 1.0143,
      "step": 31870
    },
    {
      "epoch": 1.602976669348351,
      "grad_norm": 7.125,
      "learning_rate": 1.67940466613033e-05,
      "loss": 1.0008,
      "step": 31880
    },
    {
      "epoch": 1.6034794851166532,
      "grad_norm": 17.75,
      "learning_rate": 1.6793041029766695e-05,
      "loss": 1.0769,
      "step": 31890
    },
    {
      "epoch": 1.6039823008849559,
      "grad_norm": 36.25,
      "learning_rate": 1.6792035398230087e-05,
      "loss": 1.0066,
      "step": 31900
    },
    {
      "epoch": 1.604485116653258,
      "grad_norm": 13.0,
      "learning_rate": 1.6791029766693487e-05,
      "loss": 0.9391,
      "step": 31910
    },
    {
      "epoch": 1.6049879324215608,
      "grad_norm": 17.0,
      "learning_rate": 1.679002413515688e-05,
      "loss": 1.0733,
      "step": 31920
    },
    {
      "epoch": 1.6054907481898633,
      "grad_norm": 11.5625,
      "learning_rate": 1.6789018503620275e-05,
      "loss": 0.9589,
      "step": 31930
    },
    {
      "epoch": 1.6059935639581657,
      "grad_norm": 16.25,
      "learning_rate": 1.678801287208367e-05,
      "loss": 0.67,
      "step": 31940
    },
    {
      "epoch": 1.6064963797264682,
      "grad_norm": 13.875,
      "learning_rate": 1.6787007240547063e-05,
      "loss": 1.1844,
      "step": 31950
    },
    {
      "epoch": 1.6069991954947707,
      "grad_norm": 17.125,
      "learning_rate": 1.678600160901046e-05,
      "loss": 0.9152,
      "step": 31960
    },
    {
      "epoch": 1.6075020112630733,
      "grad_norm": 20.75,
      "learning_rate": 1.6784995977473855e-05,
      "loss": 0.8996,
      "step": 31970
    },
    {
      "epoch": 1.6080048270313756,
      "grad_norm": 17.125,
      "learning_rate": 1.678399034593725e-05,
      "loss": 1.3756,
      "step": 31980
    },
    {
      "epoch": 1.6085076427996783,
      "grad_norm": 14.1875,
      "learning_rate": 1.6782984714400647e-05,
      "loss": 0.8246,
      "step": 31990
    },
    {
      "epoch": 1.6090104585679805,
      "grad_norm": 15.375,
      "learning_rate": 1.678197908286404e-05,
      "loss": 1.1568,
      "step": 32000
    },
    {
      "epoch": 1.6090104585679805,
      "eval_accuracy": 0.5125681705503222,
      "eval_loss": 1.0259712934494019,
      "eval_runtime": 466.8872,
      "eval_samples_per_second": 86.402,
      "eval_steps_per_second": 86.402,
      "step": 32000
    }
  ],
  "logging_steps": 10,
  "max_steps": 198880,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "total_flos": 0.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
