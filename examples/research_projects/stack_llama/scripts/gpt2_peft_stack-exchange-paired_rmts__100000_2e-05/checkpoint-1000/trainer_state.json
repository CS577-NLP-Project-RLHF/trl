{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.16644474034620507,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016644474034620505,
      "grad_norm": 82.0,
      "learning_rate": 1.996671105193076e-05,
      "loss": 3.0074,
      "step": 10
    },
    {
      "epoch": 0.003328894806924101,
      "grad_norm": 205.0,
      "learning_rate": 1.9933422103861518e-05,
      "loss": 3.1453,
      "step": 20
    },
    {
      "epoch": 0.004993342210386152,
      "grad_norm": 17.625,
      "learning_rate": 1.990013315579228e-05,
      "loss": 2.3703,
      "step": 30
    },
    {
      "epoch": 0.006657789613848202,
      "grad_norm": 18.875,
      "learning_rate": 1.9866844207723038e-05,
      "loss": 2.0053,
      "step": 40
    },
    {
      "epoch": 0.008322237017310254,
      "grad_norm": 68.5,
      "learning_rate": 1.9833555259653796e-05,
      "loss": 1.8797,
      "step": 50
    },
    {
      "epoch": 0.009986684420772303,
      "grad_norm": 102.0,
      "learning_rate": 1.9800266311584554e-05,
      "loss": 2.2539,
      "step": 60
    },
    {
      "epoch": 0.011651131824234355,
      "grad_norm": 68.5,
      "learning_rate": 1.9766977363515315e-05,
      "loss": 2.3381,
      "step": 70
    },
    {
      "epoch": 0.013315579227696404,
      "grad_norm": 43.75,
      "learning_rate": 1.9733688415446073e-05,
      "loss": 2.4234,
      "step": 80
    },
    {
      "epoch": 0.014980026631158456,
      "grad_norm": 80.5,
      "learning_rate": 1.970039946737683e-05,
      "loss": 2.9699,
      "step": 90
    },
    {
      "epoch": 0.016644474034620507,
      "grad_norm": 110.5,
      "learning_rate": 1.966711051930759e-05,
      "loss": 2.0242,
      "step": 100
    },
    {
      "epoch": 0.018308921438082555,
      "grad_norm": 83.0,
      "learning_rate": 1.963382157123835e-05,
      "loss": 1.9811,
      "step": 110
    },
    {
      "epoch": 0.019973368841544607,
      "grad_norm": 14.375,
      "learning_rate": 1.960053262316911e-05,
      "loss": 1.4082,
      "step": 120
    },
    {
      "epoch": 0.021637816245006658,
      "grad_norm": 6.5625,
      "learning_rate": 1.956724367509987e-05,
      "loss": 2.027,
      "step": 130
    },
    {
      "epoch": 0.02330226364846871,
      "grad_norm": 38.25,
      "learning_rate": 1.953395472703063e-05,
      "loss": 1.9574,
      "step": 140
    },
    {
      "epoch": 0.024966711051930757,
      "grad_norm": 33.75,
      "learning_rate": 1.9500665778961387e-05,
      "loss": 1.8313,
      "step": 150
    },
    {
      "epoch": 0.02663115845539281,
      "grad_norm": 18.875,
      "learning_rate": 1.9467376830892145e-05,
      "loss": 1.3441,
      "step": 160
    },
    {
      "epoch": 0.02829560585885486,
      "grad_norm": 17.5,
      "learning_rate": 1.9434087882822907e-05,
      "loss": 1.6559,
      "step": 170
    },
    {
      "epoch": 0.02996005326231691,
      "grad_norm": 36.5,
      "learning_rate": 1.9400798934753665e-05,
      "loss": 1.5535,
      "step": 180
    },
    {
      "epoch": 0.03162450066577896,
      "grad_norm": 5.90625,
      "learning_rate": 1.9367509986684423e-05,
      "loss": 1.448,
      "step": 190
    },
    {
      "epoch": 0.033288948069241014,
      "grad_norm": 23.875,
      "learning_rate": 1.933422103861518e-05,
      "loss": 1.709,
      "step": 200
    },
    {
      "epoch": 0.03495339547270306,
      "grad_norm": 75.5,
      "learning_rate": 1.930093209054594e-05,
      "loss": 1.5148,
      "step": 210
    },
    {
      "epoch": 0.03661784287616511,
      "grad_norm": 65.5,
      "learning_rate": 1.92676431424767e-05,
      "loss": 1.5078,
      "step": 220
    },
    {
      "epoch": 0.038282290279627165,
      "grad_norm": 29.5,
      "learning_rate": 1.923435419440746e-05,
      "loss": 1.4211,
      "step": 230
    },
    {
      "epoch": 0.03994673768308921,
      "grad_norm": 54.25,
      "learning_rate": 1.9201065246338217e-05,
      "loss": 1.3559,
      "step": 240
    },
    {
      "epoch": 0.04161118508655127,
      "grad_norm": 13.125,
      "learning_rate": 1.9167776298268975e-05,
      "loss": 1.2094,
      "step": 250
    },
    {
      "epoch": 0.043275632490013316,
      "grad_norm": 29.625,
      "learning_rate": 1.9134487350199737e-05,
      "loss": 1.1453,
      "step": 260
    },
    {
      "epoch": 0.044940079893475364,
      "grad_norm": 22.25,
      "learning_rate": 1.9101198402130495e-05,
      "loss": 1.1934,
      "step": 270
    },
    {
      "epoch": 0.04660452729693742,
      "grad_norm": 33.5,
      "learning_rate": 1.9067909454061253e-05,
      "loss": 1.3309,
      "step": 280
    },
    {
      "epoch": 0.04826897470039947,
      "grad_norm": 55.25,
      "learning_rate": 1.903462050599201e-05,
      "loss": 1.5771,
      "step": 290
    },
    {
      "epoch": 0.049933422103861515,
      "grad_norm": 29.75,
      "learning_rate": 1.900133155792277e-05,
      "loss": 1.4488,
      "step": 300
    },
    {
      "epoch": 0.05159786950732357,
      "grad_norm": 16.25,
      "learning_rate": 1.896804260985353e-05,
      "loss": 1.1566,
      "step": 310
    },
    {
      "epoch": 0.05326231691078562,
      "grad_norm": 7.25,
      "learning_rate": 1.893475366178429e-05,
      "loss": 1.1781,
      "step": 320
    },
    {
      "epoch": 0.05492676431424767,
      "grad_norm": 14.75,
      "learning_rate": 1.8901464713715047e-05,
      "loss": 1.4129,
      "step": 330
    },
    {
      "epoch": 0.05659121171770972,
      "grad_norm": 28.75,
      "learning_rate": 1.8868175765645805e-05,
      "loss": 1.2781,
      "step": 340
    },
    {
      "epoch": 0.05825565912117177,
      "grad_norm": 8.875,
      "learning_rate": 1.8834886817576567e-05,
      "loss": 1.3512,
      "step": 350
    },
    {
      "epoch": 0.05992010652463382,
      "grad_norm": 13.25,
      "learning_rate": 1.8801597869507325e-05,
      "loss": 1.3848,
      "step": 360
    },
    {
      "epoch": 0.06158455392809587,
      "grad_norm": 34.25,
      "learning_rate": 1.8768308921438083e-05,
      "loss": 1.1969,
      "step": 370
    },
    {
      "epoch": 0.06324900133155792,
      "grad_norm": 47.25,
      "learning_rate": 1.873501997336884e-05,
      "loss": 1.3902,
      "step": 380
    },
    {
      "epoch": 0.06491344873501997,
      "grad_norm": 9.5,
      "learning_rate": 1.8701731025299603e-05,
      "loss": 0.9904,
      "step": 390
    },
    {
      "epoch": 0.06657789613848203,
      "grad_norm": 47.75,
      "learning_rate": 1.866844207723036e-05,
      "loss": 1.3535,
      "step": 400
    },
    {
      "epoch": 0.06824234354194407,
      "grad_norm": 36.25,
      "learning_rate": 1.8635153129161122e-05,
      "loss": 0.8746,
      "step": 410
    },
    {
      "epoch": 0.06990679094540612,
      "grad_norm": 19.125,
      "learning_rate": 1.860186418109188e-05,
      "loss": 1.4457,
      "step": 420
    },
    {
      "epoch": 0.07157123834886818,
      "grad_norm": 27.875,
      "learning_rate": 1.856857523302264e-05,
      "loss": 1.0061,
      "step": 430
    },
    {
      "epoch": 0.07323568575233022,
      "grad_norm": 24.625,
      "learning_rate": 1.8535286284953397e-05,
      "loss": 1.0208,
      "step": 440
    },
    {
      "epoch": 0.07490013315579228,
      "grad_norm": 29.5,
      "learning_rate": 1.8501997336884158e-05,
      "loss": 1.2641,
      "step": 450
    },
    {
      "epoch": 0.07656458055925433,
      "grad_norm": 26.375,
      "learning_rate": 1.8468708388814916e-05,
      "loss": 1.2277,
      "step": 460
    },
    {
      "epoch": 0.07822902796271637,
      "grad_norm": 42.25,
      "learning_rate": 1.8435419440745674e-05,
      "loss": 1.2291,
      "step": 470
    },
    {
      "epoch": 0.07989347536617843,
      "grad_norm": 51.25,
      "learning_rate": 1.8402130492676432e-05,
      "loss": 1.2758,
      "step": 480
    },
    {
      "epoch": 0.08155792276964048,
      "grad_norm": 53.75,
      "learning_rate": 1.836884154460719e-05,
      "loss": 1.5547,
      "step": 490
    },
    {
      "epoch": 0.08322237017310254,
      "grad_norm": 36.0,
      "learning_rate": 1.8335552596537952e-05,
      "loss": 1.292,
      "step": 500
    },
    {
      "epoch": 0.08322237017310254,
      "eval_accuracy": 0.49730379956860793,
      "eval_loss": 1.3427207469940186,
      "eval_runtime": 697.9048,
      "eval_samples_per_second": 34.543,
      "eval_steps_per_second": 17.272,
      "step": 500
    },
    {
      "epoch": 0.08488681757656458,
      "grad_norm": 22.75,
      "learning_rate": 1.830226364846871e-05,
      "loss": 1.2182,
      "step": 510
    },
    {
      "epoch": 0.08655126498002663,
      "grad_norm": 19.875,
      "learning_rate": 1.826897470039947e-05,
      "loss": 1.0531,
      "step": 520
    },
    {
      "epoch": 0.08821571238348869,
      "grad_norm": 21.0,
      "learning_rate": 1.8235685752330227e-05,
      "loss": 1.0973,
      "step": 530
    },
    {
      "epoch": 0.08988015978695073,
      "grad_norm": 43.25,
      "learning_rate": 1.8202396804260988e-05,
      "loss": 1.0156,
      "step": 540
    },
    {
      "epoch": 0.09154460719041278,
      "grad_norm": 18.375,
      "learning_rate": 1.8169107856191746e-05,
      "loss": 1.0514,
      "step": 550
    },
    {
      "epoch": 0.09320905459387484,
      "grad_norm": 31.0,
      "learning_rate": 1.8135818908122504e-05,
      "loss": 1.1303,
      "step": 560
    },
    {
      "epoch": 0.09487350199733688,
      "grad_norm": 36.25,
      "learning_rate": 1.8102529960053262e-05,
      "loss": 1.1049,
      "step": 570
    },
    {
      "epoch": 0.09653794940079893,
      "grad_norm": 8.5,
      "learning_rate": 1.806924101198402e-05,
      "loss": 1.0469,
      "step": 580
    },
    {
      "epoch": 0.09820239680426099,
      "grad_norm": 12.125,
      "learning_rate": 1.8035952063914782e-05,
      "loss": 0.9684,
      "step": 590
    },
    {
      "epoch": 0.09986684420772303,
      "grad_norm": 46.75,
      "learning_rate": 1.800266311584554e-05,
      "loss": 1.166,
      "step": 600
    },
    {
      "epoch": 0.10153129161118508,
      "grad_norm": 22.125,
      "learning_rate": 1.79693741677763e-05,
      "loss": 1.1551,
      "step": 610
    },
    {
      "epoch": 0.10319573901464714,
      "grad_norm": 9.0,
      "learning_rate": 1.7936085219707056e-05,
      "loss": 1.0684,
      "step": 620
    },
    {
      "epoch": 0.1048601864181092,
      "grad_norm": 22.75,
      "learning_rate": 1.7902796271637818e-05,
      "loss": 1.0961,
      "step": 630
    },
    {
      "epoch": 0.10652463382157124,
      "grad_norm": 21.125,
      "learning_rate": 1.7869507323568576e-05,
      "loss": 1.0809,
      "step": 640
    },
    {
      "epoch": 0.10818908122503329,
      "grad_norm": 20.375,
      "learning_rate": 1.7836218375499338e-05,
      "loss": 0.9951,
      "step": 650
    },
    {
      "epoch": 0.10985352862849534,
      "grad_norm": 14.0625,
      "learning_rate": 1.7802929427430096e-05,
      "loss": 0.9477,
      "step": 660
    },
    {
      "epoch": 0.11151797603195739,
      "grad_norm": 19.25,
      "learning_rate": 1.7769640479360854e-05,
      "loss": 1.1016,
      "step": 670
    },
    {
      "epoch": 0.11318242343541944,
      "grad_norm": 20.625,
      "learning_rate": 1.7736351531291612e-05,
      "loss": 1.2195,
      "step": 680
    },
    {
      "epoch": 0.1148468708388815,
      "grad_norm": 35.5,
      "learning_rate": 1.7703062583222374e-05,
      "loss": 1.1961,
      "step": 690
    },
    {
      "epoch": 0.11651131824234354,
      "grad_norm": 42.75,
      "learning_rate": 1.766977363515313e-05,
      "loss": 1.1164,
      "step": 700
    },
    {
      "epoch": 0.11817576564580559,
      "grad_norm": 27.875,
      "learning_rate": 1.763648468708389e-05,
      "loss": 0.9984,
      "step": 710
    },
    {
      "epoch": 0.11984021304926765,
      "grad_norm": 32.0,
      "learning_rate": 1.7603195739014648e-05,
      "loss": 1.0512,
      "step": 720
    },
    {
      "epoch": 0.12150466045272969,
      "grad_norm": 45.25,
      "learning_rate": 1.756990679094541e-05,
      "loss": 1.302,
      "step": 730
    },
    {
      "epoch": 0.12316910785619174,
      "grad_norm": 10.0,
      "learning_rate": 1.7536617842876168e-05,
      "loss": 1.0992,
      "step": 740
    },
    {
      "epoch": 0.1248335552596538,
      "grad_norm": 9.625,
      "learning_rate": 1.7503328894806926e-05,
      "loss": 0.9535,
      "step": 750
    },
    {
      "epoch": 0.12649800266311584,
      "grad_norm": 23.0,
      "learning_rate": 1.7470039946737684e-05,
      "loss": 1.1754,
      "step": 760
    },
    {
      "epoch": 0.1281624500665779,
      "grad_norm": 17.25,
      "learning_rate": 1.7436750998668442e-05,
      "loss": 1.1402,
      "step": 770
    },
    {
      "epoch": 0.12982689747003995,
      "grad_norm": 39.25,
      "learning_rate": 1.7403462050599203e-05,
      "loss": 1.0141,
      "step": 780
    },
    {
      "epoch": 0.131491344873502,
      "grad_norm": 11.0625,
      "learning_rate": 1.737017310252996e-05,
      "loss": 1.2168,
      "step": 790
    },
    {
      "epoch": 0.13315579227696406,
      "grad_norm": 40.75,
      "learning_rate": 1.733688415446072e-05,
      "loss": 1.027,
      "step": 800
    },
    {
      "epoch": 0.13482023968042608,
      "grad_norm": 34.0,
      "learning_rate": 1.7303595206391478e-05,
      "loss": 1.0939,
      "step": 810
    },
    {
      "epoch": 0.13648468708388814,
      "grad_norm": 22.875,
      "learning_rate": 1.727030625832224e-05,
      "loss": 0.9584,
      "step": 820
    },
    {
      "epoch": 0.1381491344873502,
      "grad_norm": 11.0625,
      "learning_rate": 1.7237017310252998e-05,
      "loss": 1.1285,
      "step": 830
    },
    {
      "epoch": 0.13981358189081225,
      "grad_norm": 11.5,
      "learning_rate": 1.7203728362183756e-05,
      "loss": 1.0547,
      "step": 840
    },
    {
      "epoch": 0.1414780292942743,
      "grad_norm": 8.8125,
      "learning_rate": 1.7170439414114514e-05,
      "loss": 1.0344,
      "step": 850
    },
    {
      "epoch": 0.14314247669773636,
      "grad_norm": 25.875,
      "learning_rate": 1.7137150466045275e-05,
      "loss": 1.2516,
      "step": 860
    },
    {
      "epoch": 0.14480692410119841,
      "grad_norm": 9.75,
      "learning_rate": 1.7103861517976033e-05,
      "loss": 1.1129,
      "step": 870
    },
    {
      "epoch": 0.14647137150466044,
      "grad_norm": 7.75,
      "learning_rate": 1.707057256990679e-05,
      "loss": 1.0324,
      "step": 880
    },
    {
      "epoch": 0.1481358189081225,
      "grad_norm": 6.25,
      "learning_rate": 1.703728362183755e-05,
      "loss": 1.0527,
      "step": 890
    },
    {
      "epoch": 0.14980026631158455,
      "grad_norm": 9.25,
      "learning_rate": 1.7003994673768308e-05,
      "loss": 0.9162,
      "step": 900
    },
    {
      "epoch": 0.1514647137150466,
      "grad_norm": 16.625,
      "learning_rate": 1.697070572569907e-05,
      "loss": 0.8867,
      "step": 910
    },
    {
      "epoch": 0.15312916111850866,
      "grad_norm": 13.4375,
      "learning_rate": 1.693741677762983e-05,
      "loss": 0.9637,
      "step": 920
    },
    {
      "epoch": 0.15479360852197072,
      "grad_norm": 8.125,
      "learning_rate": 1.690412782956059e-05,
      "loss": 1.0559,
      "step": 930
    },
    {
      "epoch": 0.15645805592543274,
      "grad_norm": 32.75,
      "learning_rate": 1.6870838881491347e-05,
      "loss": 0.8594,
      "step": 940
    },
    {
      "epoch": 0.1581225033288948,
      "grad_norm": 20.125,
      "learning_rate": 1.6837549933422105e-05,
      "loss": 1.0273,
      "step": 950
    },
    {
      "epoch": 0.15978695073235685,
      "grad_norm": 7.4375,
      "learning_rate": 1.6804260985352863e-05,
      "loss": 0.9008,
      "step": 960
    },
    {
      "epoch": 0.1614513981358189,
      "grad_norm": 8.75,
      "learning_rate": 1.6770972037283625e-05,
      "loss": 1.1035,
      "step": 970
    },
    {
      "epoch": 0.16311584553928096,
      "grad_norm": 59.25,
      "learning_rate": 1.6737683089214383e-05,
      "loss": 1.166,
      "step": 980
    },
    {
      "epoch": 0.16478029294274302,
      "grad_norm": 23.125,
      "learning_rate": 1.670439414114514e-05,
      "loss": 1.1871,
      "step": 990
    },
    {
      "epoch": 0.16644474034620507,
      "grad_norm": 16.625,
      "learning_rate": 1.66711051930759e-05,
      "loss": 0.9184,
      "step": 1000
    },
    {
      "epoch": 0.16644474034620507,
      "eval_accuracy": 0.5005392400862784,
      "eval_loss": 1.0660072565078735,
      "eval_runtime": 698.2983,
      "eval_samples_per_second": 34.524,
      "eval_steps_per_second": 17.262,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 6008,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "total_flos": 0.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
